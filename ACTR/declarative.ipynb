{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "21afc7b6-11f2-409f-a15f-559d8b49eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyactup as pau\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f858cca-3174-4bdd-b959-d238945dd959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Chunk 0001 {'a': 1} 1>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = pau.Memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65353567-5164-4446-9cb6-fda313c9884d",
   "metadata": {},
   "source": [
    "Now, we initialize the agent with a set of memories. They are just needed for initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f5439ae-7a7f-485f-a3aa-2be850ed364f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ec0f1b7-bb7b-4b58-8e53-740342b8859f",
   "metadata": {},
   "source": [
    "Now, we create a PSS Task object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3c60675-6110-4a9f-b6a6-6ae8f1205a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSS_Object():\n",
    "    \"\"\"A generic object for PSS task components\"\"\"\n",
    "    ACTIONS = (\"A\", \"C\", \"E\", \"F\", \"D\", \"B\")\n",
    "    NEG_ACTIONS = tuple(\"-\" + x for x in ACTIONS)\n",
    "    REWARD_TABLE = {\"A\" : 0.8, \"C\" : 0.7, \"E\" : 0.6,\n",
    "                    \"F\" : 0.4, \"D\" : 0.3, \"B\" : 0.2}\n",
    "\n",
    "    def is_action(self, action):\n",
    "        \"\"\"An action is valid only if it belongs to the list of possible actions\"\"\"\n",
    "        return action[-1] in self.ACTIONS\n",
    "    \n",
    "    def prob_reward(self, action):\n",
    "        \"\"\"Returns the probability of obtaining a reward given an action\"\"\"\n",
    "        if self.is_action(action):\n",
    "            return self.REWARD_TABLE[action]\n",
    "        \n",
    "    def get_reward(self, action):\n",
    "        \"\"\"Return a probabilistic reward associated with an action\"\"\"\n",
    "        i = random.random()\n",
    "        if i <= self.prob_reward(action):\n",
    "            return 1.0\n",
    "        else:\n",
    "            return -1.0\n",
    "    \n",
    "    def complement_action(self, action):\n",
    "        \"\"\"Returns the complement action (i.e., -A for A, and A for -A)\"\"\" \n",
    "        if self.is_action(action):\n",
    "            if action.startswith(\"-\"):\n",
    "                return action[-1]\n",
    "            else:\n",
    "                return \"-\" + action\n",
    "        \n",
    "class PSS_State(PSS_Object):\n",
    "    \"\"\"\n",
    "A state in the PSS object. A state is consists of two possible options\n",
    "to choose from, one on the left and one on the right.\n",
    "    \"\"\"\n",
    "    def __init__(self, options = (\"A\", \"B\")):\n",
    "        \"\"\"Initializes a state, with default options being (A, B)\"\"\"\n",
    "        if self.is_options(options):\n",
    "            self.options = options\n",
    "        else:\n",
    "            self.options = None\n",
    "            \n",
    "    @property\n",
    "    def left(self):\n",
    "        \"\"\"The option on the left\"\"\" \n",
    "        if (self.is_options(self.options)):\n",
    "            return self.options[0]\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    \n",
    "    @property\n",
    "    def right(self):\n",
    "        \"\"\"The option on the right\"\"\"\n",
    "        if (self.is_options(self.options)):\n",
    "            return self.options[1]\n",
    "        else:\n",
    "            return None\n",
    "           \n",
    "\n",
    "    def is_options(self, options):\n",
    "        \"\"\"Checks whether a given tuple is a set of options\"\"\"\n",
    "        if len(options) == 2 and not False in [x in self.ACTIONS for x in options]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Equality if the options are the same, independent of order\"\"\"\n",
    "        return (self.left == other.left and self.right == other.right) or \\\n",
    "               (self.left == other.right and self.right == other.left)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"Represented as a tuple '(O1, O2)'\"\"\"\n",
    "        return \"(%s,%s)\" % (self.left, self.right)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    \n",
    "class PSS_Decision(PSS_Object):\n",
    "    \"\"\"A decision made during the PSS task\"\"\"\n",
    "    def __init__(self, state = None, action = None, reward = 0.0):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "    \n",
    "    def is_state(self, state):\n",
    "        \"\"\"Checks if something is a valid state\"\"\"\n",
    "        return isinstance(state, PSS_State)\n",
    "    \n",
    "    @property\n",
    "    def successful(self):\n",
    "        \"\"\"Success if reward > 0.\"\"\"\n",
    "        if self.reward > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    @property\n",
    "    def optimal(self):\n",
    "        \"\"\"A an action was optimal if it corresponded to the highest prob option\"\"\"\n",
    "        s = self.state\n",
    "        apos = s.options.index(self.action)\n",
    "        probs = [self.prob_reward(x) for x in s.options]\n",
    "        ppos = probs.index(max(probs))\n",
    "        return apos == ppos\n",
    "    \n",
    "    def includes_option(self, option):\n",
    "        \"\"\"Checks if the decision included option 'option'\"\"\"\n",
    "        return option in self.state.options\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"The decision as a string\"\"\"\n",
    "        return \"<%s, %s, %0.1f>\" % (self.state, self.action, self.reward)\n",
    "\n",
    "\n",
    "\n",
    "class PSS_Task(PSS_Object):\n",
    "    \"\"\"An object implementing the PSS task\"\"\"\n",
    "    CRITERION = {\"AB\" : 0.65, \"CD\" : 0.60, \"EF\" : 0.50}\n",
    "    \n",
    "    TRAINING_BLOCK = (((\"A\", \"B\"),) * 10 +\n",
    "                      ((\"B\", \"A\"),) * 10 +\n",
    "                      ((\"C\", \"D\"),) * 10 +\n",
    "                      ((\"D\", \"C\"),) * 10 +\n",
    "                      ((\"E\", \"F\"),) * 10 +\n",
    "                      ((\"F\", \"E\"),) * 10)\n",
    "    \n",
    "    TEST_BLOCK = (((\"A\", \"B\"),) * 2 + ((\"B\", \"A\"),) * 2 +\n",
    "                  ((\"A\", \"C\"),) * 2 + ((\"C\", \"A\"),) * 2 +\n",
    "                  ((\"A\", \"D\"),) * 2 + ((\"D\", \"A\"),) * 2 +\n",
    "                  ((\"A\", \"E\"),) * 2 + ((\"E\", \"A\"),) * 2 +\n",
    "                  ((\"A\", \"F\"),) * 2 + ((\"F\", \"A\"),) * 2 +\n",
    "\n",
    "                  ((\"B\", \"C\"),) * 2 + ((\"C\", \"B\"),) * 2 +\n",
    "                  ((\"B\", \"D\"),) * 2 + ((\"D\", \"B\"),) * 2 +\n",
    "                  ((\"B\", \"E\"),) * 2 + ((\"E\", \"B\"),) * 2 +\n",
    "                  ((\"B\", \"F\"),) * 2 + ((\"F\", \"B\"),) * 2 +\n",
    "                  \n",
    "                  ((\"C\", \"D\"),) * 2 + ((\"D\", \"C\"),) * 2 +\n",
    "                  ((\"C\", \"E\"),) * 2 + ((\"E\", \"C\"),) * 2 +\n",
    "                  ((\"C\", \"F\"),) * 2 + ((\"F\", \"C\"),) * 2 +\n",
    "                  \n",
    "                  ((\"D\", \"E\"),) * 2 + ((\"E\", \"D\"),) * 2 +\n",
    "                  ((\"D\", \"F\"),) * 2 + ((\"F\", \"D\"),) * 2 +\n",
    "    \n",
    "                  ((\"E\", \"F\"),) * 2 + ((\"F\", \"E\"),) * 2)\n",
    "\n",
    "                  \n",
    "    \n",
    "    PHASES = (\"Training\", \"Test\")\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes a PSS task experiment\"\"\"\n",
    "        self.index = 0\n",
    "        self.phase = \"Training\"\n",
    "        \n",
    "        self.train = self.instantiate_block(self.TRAINING_BLOCK)        \n",
    "        self.test =  self.instantiate_block(self.TEST_BLOCK)\n",
    "        self.blocks = dict(zip(self.PHASES, [self.train, self.test]))                \n",
    "        self.history = dict(zip(self.PHASES, [[], []]))\n",
    "        \n",
    "        self.state = self.next_state()\n",
    "    \n",
    "    def instantiate_block(self, block):\n",
    "        \"\"\"Instantiates and randomizes a block of trials\"\"\"\n",
    "        trials = [PSS_State(x) for x in block]\n",
    "        random.shuffle(trials)\n",
    "        return deque(trials)\n",
    "    \n",
    "    def criterion_reached(self):\n",
    "        \"\"\"Reached criterion for successful learning\"\"\"\n",
    "        training = self.history['Training']\n",
    "        if len(training) < 60:\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            if len(training) > 60:\n",
    "                training = training[-60:]\n",
    "            ab = self.calculate_accuracy(training, \"A\")\n",
    "            cd = self.calculate_accuracy(training, \"C\")\n",
    "            ef = self.calculate_accuracy(training, \"E\")\n",
    "            \n",
    "            if ab >= self.CRITERION[\"AB\"] and cd >= self.CRITERION[\"CD\"] and ef >= self.CRITERION[\"EF\"]:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    \n",
    "    def next_state(self):\n",
    "        \"\"\"Next state (transitions are independent of actions)\"\"\"\n",
    "        state_next = None\n",
    "        current_block = self.blocks[self.phase]\n",
    "        if len(current_block) == 0:\n",
    "            if self.phase == \"Training\":\n",
    "                if self.criterion_reached() or len(self.history[\"Training\"]) >= 360:\n",
    "                    # Move to the Test phase and recalculate the current block.\n",
    "                    self.phase = \"Test\"\n",
    "                      \n",
    "                else:\n",
    "                    self.blocks[\"Training\"] = self.instantiate_block(self.TRAINING_BLOCK)\n",
    "                    \n",
    "                current_block = self.blocks[self.phase]\n",
    "                state_next = current_block.popleft()\n",
    "            \n",
    "            else: \n",
    "                state_next = None # End of the experiment\n",
    "        else:\n",
    "            state_next = current_block.popleft()\n",
    "        return state_next\n",
    "                    \n",
    "    \n",
    "    def execute_action(self, action):\n",
    "        \"\"\"Executes and action and returns the new state and a reward\"\"\"\n",
    "        if self.is_action(action):\n",
    "            if action.startswith(\"-\"):\n",
    "                # This handles the cases where an agent chooses NOT\n",
    "                # to pick a specific action (as in the BG models)\n",
    "                action = [x for x in self.state.options if x is not action[-1]][0]\n",
    "            \n",
    "            r = None\n",
    "            if self.phase == \"Training\":\n",
    "                r = self.get_reward(action)\n",
    "            \n",
    "            # Update history\n",
    "            d = PSS_Decision(self.state, action, reward = r)\n",
    "            self.history[self.phase].append(d)\n",
    "            \n",
    "            self.state = self.next_state()\n",
    "            return (self.state, r)\n",
    "    \n",
    "    def calculate_accuracy(self, decisions, option, exclude = \"None\"):\n",
    "        \"\"\"Calculates accuracy across all decisions that include option 'option' but not option 'exclude'\"\"\"\n",
    "        opt = [x.optimal for x in decisions if x.includes_option(option) and not x.includes_option(exclude)]\n",
    "        return np.mean(opt)\n",
    "        \n",
    "    def accuracies(self):\n",
    "        \"\"\"Returns the Choose / Avoid accuracies\"\"\"\n",
    "        test = self.history[\"Test\"]\n",
    "        if len(test) >= 60:\n",
    "            return (self.calculate_accuracy(test, option = 'A', exclude = 'B'),\n",
    "                    self.calculate_accuracy(test, option = 'B', exclude = 'A'))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d7384-79f6-4da2-bb5c-f292e6fe462a",
   "metadata": {},
   "source": [
    "Now we need to define a few functions to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3d8e99af-3bc1-47d8-b867-577e25e62f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMAgent():\n",
    "    \"\"\"A declarative memory agent\"\"\"\n",
    "    def __init__(self, nsamples=3):\n",
    "        \"\"\"Initializes an agent\"\"\"\n",
    "        self.memory = pau.Memory()\n",
    "        self.nsamples = nsamples\n",
    "        self.init_memory()\n",
    "        \n",
    "    def init_memory(self):\n",
    "        \"\"\"Initializes the agent with some fake memories (to make the first choice possible)\"\"\"\n",
    "        dm = self.memory\n",
    "        dm.learn(action=\"A\", reward=-1)\n",
    "        dm.learn(action=\"B\", reward=-1)\n",
    "        dm.learn(action=\"C\", reward=-1)\n",
    "        dm.learn(action=\"D\", reward=-1)\n",
    "        dm.learn(action=\"E\", reward=-1)\n",
    "        dm.learn(action=\"F\", reward=-1)\n",
    "    \n",
    "    def evaluate(self, option):\n",
    "        \"\"\"Evaluates an option by sampling from memory\"\"\"\n",
    "        res = []\n",
    "        dm = self.memory\n",
    "        for i in range(self.nsamples):\n",
    "            instance = dm.retrieve(action = option)\n",
    "            res.append(instance[\"reward\"])\n",
    "        return np.sum(res)\n",
    "    \n",
    "    def choose(self, state):\n",
    "        \"\"\"Choose an option\"\"\"               \n",
    "        val1, val2 = [self.evaluate(option) for option in [state.left, state.right]]\n",
    "        if val1 > val2:\n",
    "            return state.left\n",
    "        elif val1 < val2:\n",
    "            return state.right\n",
    "        else:\n",
    "            return random.choice([state.left, state.right])\n",
    "                       \n",
    "def PSS_loop(agent, task):\n",
    "    while task.state is not None:\n",
    "        state1 = task.state\n",
    "        choice = agent.choose(state1)\n",
    "        state2, reward1 = task.execute_action(choice)\n",
    "        if reward1 is not None:\n",
    "            agent.memory.learn(action = choice, reward = reward1)\n",
    "        agent.memory.advance(1)\n",
    "    \n",
    "def plot_results(results, param_values):\n",
    "    x = [1,2]   # X axis\n",
    "    jet = cm = plt.get_cmap('jet') \n",
    "    cNorm  = colors.Normalize(vmin = 0, vmax = (len(param_values) - 1))\n",
    "    scalarMap = cmx.ScalarMappable(norm = cNorm, cmap = jet)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for i in range(len(param_values)):\n",
    "        p = param_values[i]\n",
    "        colorVal = scalarMap.to_rgba(i)\n",
    "        ax.plot(x, results[p], \"o-\", color=colorVal)\n",
    "    \n",
    "    ax.axis([0.75, 2.25, 0.48, 0.95])\n",
    "    ax.set_ylabel(\"Mean Accuracy\")\n",
    "    ax.set_xticks([1,2])\n",
    "    ax.set_xticklabels([\"Choose\", \"Avoid\"])\n",
    "    #fig.colorbar(ax, param_values)\n",
    "    plt.legend([\"T = %.2f\" % x for x in param_values], loc=\"best\", ncol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "de619335-4ed9-4595-b926-604ae126b6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.625, 0.6875)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = DMAgent()\n",
    "task = PSS_Task()\n",
    "PSS_loop(agent, task)\n",
    "task.accuracies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093cdb2-074c-4a1f-a4c1-8aca860d3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for n in range(1, 11):\n",
    "    performance = []\n",
    "    for i in range( 250 ):\n",
    "        task = PSS_Task()\n",
    "        agent = DMAgent(nsamples = n)\n",
    "        PSS_loop(agent, task)\n",
    "        performance.append(task.accuracies())\n",
    "\n",
    "    results[n] = (np.mean([x[0] for x in performance]), np.mean([x[1] for x in performance]))\n",
    "\n",
    "plot_results(results, list(range(1, 11)))\n",
    "plt.savefig(\"figures/decision_by_sampling.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4c01a8-aed4-44b7-8bfd-81b899a77e93",
   "metadata": {},
   "source": [
    "## Instance-based Learning\n",
    "\n",
    "We can now generalize this approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
