{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21afc7b6-11f2-409f-a15f-559d8b49eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyactup as pau\n",
    "import random\n",
    "from copy import copy\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d56d321-7ead-434d-9eb2-b9d65d8d4817",
   "metadata": {},
   "source": [
    "# Basic effects of Declarative Memory \n",
    "\n",
    "Every memory system needs to account for at least two effects: _Recency_ and _Frequency_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15e26b-4789-4b50-ab51-3c8bc79fcd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "memory = pau.Memory()\n",
    "fact = {\"classname\" : \"PSYCH509\", \"instructor\" : \"Andrea Stocco\"}\n",
    "memory.learn(**fact)\n",
    "curve1 = []\n",
    "for i in range(100):\n",
    "    if i % 100 == 0:\n",
    "        memory.retrieve(**fact)\n",
    "        memory.learn(advance = 1, **fact)\n",
    "        #print(memory.chunks[0])\n",
    "    else:\n",
    "        memory.advance(1)\n",
    "    curve1.append(memory.retrieve(**fact)._base_activation)\n",
    "\n",
    "axs[0].plot(curve1)\n",
    "\n",
    "memory = pau.Memory()\n",
    "fact = {\"classname\" : \"PSYCH509\", \"instructor\" : \"Andrea Stocco\"}\n",
    "#memory.learn(**fact)\n",
    "curve2 = []\n",
    "for i in range(100):\n",
    "    if i < 80:\n",
    "        curve2.append(None)\n",
    "        memory.advance(1)\n",
    "    elif i == 80:\n",
    "        memory.learn(**fact)\n",
    "        memory.learn(advance = 0.001, **fact)\n",
    "    else:\n",
    "        memory.advance(1)\n",
    "        curve2.append(memory.retrieve(**fact)._base_activation)\n",
    "\n",
    "axs[0].plot(curve2)\n",
    "axs[0].set_title(\"Recency Effect\")\n",
    "\n",
    "## Frequency\n",
    "\n",
    "memory = pau.Memory()\n",
    "memory.learn(**fact)\n",
    "curve1 = []\n",
    "for i in range(100):\n",
    "    if i % 30 == 0:\n",
    "        memory.retrieve(**fact)\n",
    "        memory.learn(advance = 1, **fact)\n",
    "    else:\n",
    "        memory.advance(1)\n",
    "    curve1.append(memory.retrieve(**fact)._base_activation)\n",
    "\n",
    "\n",
    "memory = pau.Memory()\n",
    "memory.learn(**fact)\n",
    "curve2 = []\n",
    "for i in range(100):\n",
    "    if i % 20 ==0 and i < 20:\n",
    "        memory.retrieve(**fact)\n",
    "        memory.learn(advance = 1, **fact)\n",
    "    else:\n",
    "        memory.advance(1)\n",
    "    curve2.append(0.05 + memory.retrieve(**fact)._base_activation)\n",
    "\n",
    "axs[1].plot(curve2)\n",
    "axs[1].plot(curve1)\n",
    "\n",
    "axs[1].set_title(\"Frequency Effect\")\n",
    "\n",
    "plt.suptitle(\"Recency and Frequency effects\")\n",
    "plt.xlabel(\"Time ($s$)\")\n",
    "plt.ylabel(\"Activation\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/recency_frequency.png\")\n",
    "plt.show()\n",
    "#plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0f1b7-bb7b-4b58-8e53-740342b8859f",
   "metadata": {},
   "source": [
    "# Decision Making with Declarative Memory\n",
    "\n",
    "To test this, we will use Frank's PSS task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c60675-6110-4a9f-b6a6-6ae8f1205a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSS_Object():\n",
    "    \"\"\"A generic object for PSS task components\"\"\"\n",
    "    ACTIONS = (\"A\", \"C\", \"E\", \"F\", \"D\", \"B\")\n",
    "    NEG_ACTIONS = tuple(\"-\" + x for x in ACTIONS)\n",
    "    REWARD_TABLE = {\"A\" : 0.8, \"C\" : 0.7, \"E\" : 0.6,\n",
    "                    \"F\" : 0.4, \"D\" : 0.3, \"B\" : 0.2}\n",
    "\n",
    "    def is_action(self, action):\n",
    "        \"\"\"An action is valid only if it belongs to the list of possible actions\"\"\"\n",
    "        return action[-1] in self.ACTIONS\n",
    "    \n",
    "    def prob_reward(self, action):\n",
    "        \"\"\"Returns the probability of obtaining a reward given an action\"\"\"\n",
    "        if self.is_action(action):\n",
    "            return self.REWARD_TABLE[action]\n",
    "        \n",
    "    def get_reward(self, action):\n",
    "        \"\"\"Return a probabilistic reward associated with an action\"\"\"\n",
    "        i = random.random()\n",
    "        if i <= self.prob_reward(action):\n",
    "            return 1.0\n",
    "        else:\n",
    "            return -1.0\n",
    "    \n",
    "    def complement_action(self, action):\n",
    "        \"\"\"Returns the complement action (i.e., -A for A, and A for -A)\"\"\" \n",
    "        if self.is_action(action):\n",
    "            if action.startswith(\"-\"):\n",
    "                return action[-1]\n",
    "            else:\n",
    "                return \"-\" + action\n",
    "        \n",
    "class PSS_State(PSS_Object):\n",
    "    \"\"\"\n",
    "A state in the PSS object. A state is consists of two possible options\n",
    "to choose from, one on the left and one on the right.\n",
    "    \"\"\"\n",
    "    def __init__(self, options = (\"A\", \"B\")):\n",
    "        \"\"\"Initializes a state, with default options being (A, B)\"\"\"\n",
    "        if self.is_options(options):\n",
    "            self.options = options\n",
    "        else:\n",
    "            self.options = None\n",
    "            \n",
    "    @property\n",
    "    def left(self):\n",
    "        \"\"\"The option on the left\"\"\" \n",
    "        if (self.is_options(self.options)):\n",
    "            return self.options[0]\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    \n",
    "    @property\n",
    "    def right(self):\n",
    "        \"\"\"The option on the right\"\"\"\n",
    "        if (self.is_options(self.options)):\n",
    "            return self.options[1]\n",
    "        else:\n",
    "            return None\n",
    "           \n",
    "\n",
    "    def is_options(self, options):\n",
    "        \"\"\"Checks whether a given tuple is a set of options\"\"\"\n",
    "        if len(options) == 2 and not False in [x in self.ACTIONS for x in options]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Equality if the options are the same, independent of order\"\"\"\n",
    "        return (self.left == other.left and self.right == other.right) or \\\n",
    "               (self.left == other.right and self.right == other.left)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"Represented as a tuple '(O1, O2)'\"\"\"\n",
    "        return \"(%s,%s)\" % (self.left, self.right)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    \n",
    "class PSS_Decision(PSS_Object):\n",
    "    \"\"\"A decision made during the PSS task\"\"\"\n",
    "    def __init__(self, state = None, action = None, reward = 0.0):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "    \n",
    "    def is_state(self, state):\n",
    "        \"\"\"Checks if something is a valid state\"\"\"\n",
    "        return isinstance(state, PSS_State)\n",
    "    \n",
    "    @property\n",
    "    def successful(self):\n",
    "        \"\"\"Success if reward > 0.\"\"\"\n",
    "        if self.reward > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    @property\n",
    "    def optimal(self):\n",
    "        \"\"\"A an action was optimal if it corresponded to the highest prob option\"\"\"\n",
    "        s = self.state\n",
    "        apos = s.options.index(self.action)\n",
    "        probs = [self.prob_reward(x) for x in s.options]\n",
    "        ppos = probs.index(max(probs))\n",
    "        return apos == ppos\n",
    "    \n",
    "    def includes_option(self, option):\n",
    "        \"\"\"Checks if the decision included option 'option'\"\"\"\n",
    "        return option in self.state.options\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"The decision as a string\"\"\"\n",
    "        return \"<%s, %s, %0.1f>\" % (self.state, self.action, self.reward)\n",
    "\n",
    "\n",
    "\n",
    "class PSS_Task(PSS_Object):\n",
    "    \"\"\"An object implementing the PSS task\"\"\"\n",
    "    CRITERION = {\"AB\" : 0.65, \"CD\" : 0.60, \"EF\" : 0.50}\n",
    "    \n",
    "    TRAINING_BLOCK = (((\"A\", \"B\"),) * 10 +\n",
    "                      ((\"B\", \"A\"),) * 10 +\n",
    "                      ((\"C\", \"D\"),) * 10 +\n",
    "                      ((\"D\", \"C\"),) * 10 +\n",
    "                      ((\"E\", \"F\"),) * 10 +\n",
    "                      ((\"F\", \"E\"),) * 10)\n",
    "    \n",
    "    TEST_BLOCK = (((\"A\", \"B\"),) * 2 + ((\"B\", \"A\"),) * 2 +\n",
    "                  ((\"A\", \"C\"),) * 2 + ((\"C\", \"A\"),) * 2 +\n",
    "                  ((\"A\", \"D\"),) * 2 + ((\"D\", \"A\"),) * 2 +\n",
    "                  ((\"A\", \"E\"),) * 2 + ((\"E\", \"A\"),) * 2 +\n",
    "                  ((\"A\", \"F\"),) * 2 + ((\"F\", \"A\"),) * 2 +\n",
    "\n",
    "                  ((\"B\", \"C\"),) * 2 + ((\"C\", \"B\"),) * 2 +\n",
    "                  ((\"B\", \"D\"),) * 2 + ((\"D\", \"B\"),) * 2 +\n",
    "                  ((\"B\", \"E\"),) * 2 + ((\"E\", \"B\"),) * 2 +\n",
    "                  ((\"B\", \"F\"),) * 2 + ((\"F\", \"B\"),) * 2 +\n",
    "                  \n",
    "                  ((\"C\", \"D\"),) * 2 + ((\"D\", \"C\"),) * 2 +\n",
    "                  ((\"C\", \"E\"),) * 2 + ((\"E\", \"C\"),) * 2 +\n",
    "                  ((\"C\", \"F\"),) * 2 + ((\"F\", \"C\"),) * 2 +\n",
    "                  \n",
    "                  ((\"D\", \"E\"),) * 2 + ((\"E\", \"D\"),) * 2 +\n",
    "                  ((\"D\", \"F\"),) * 2 + ((\"F\", \"D\"),) * 2 +\n",
    "    \n",
    "                  ((\"E\", \"F\"),) * 2 + ((\"F\", \"E\"),) * 2)\n",
    "\n",
    "                  \n",
    "    \n",
    "    PHASES = (\"Training\", \"Test\")\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes a PSS task experiment\"\"\"\n",
    "        self.index = 0\n",
    "        self.phase = \"Training\"\n",
    "        \n",
    "        self.train = self.instantiate_block(self.TRAINING_BLOCK)        \n",
    "        self.test =  self.instantiate_block(self.TEST_BLOCK)\n",
    "        self.blocks = dict(zip(self.PHASES, [self.train, self.test]))                \n",
    "        self.history = dict(zip(self.PHASES, [[], []]))\n",
    "        \n",
    "        self.state = self.next_state()\n",
    "    \n",
    "    def instantiate_block(self, block):\n",
    "        \"\"\"Instantiates and randomizes a block of trials\"\"\"\n",
    "        trials = [PSS_State(x) for x in block]\n",
    "        random.shuffle(trials)\n",
    "        return deque(trials)\n",
    "    \n",
    "    def criterion_reached(self):\n",
    "        \"\"\"Reached criterion for successful learning\"\"\"\n",
    "        training = self.history['Training']\n",
    "        if len(training) < 60:\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            if len(training) > 60:\n",
    "                training = training[-60:]\n",
    "            ab = self.calculate_accuracy(training, \"A\")\n",
    "            cd = self.calculate_accuracy(training, \"C\")\n",
    "            ef = self.calculate_accuracy(training, \"E\")\n",
    "            \n",
    "            if ab >= self.CRITERION[\"AB\"] and cd >= self.CRITERION[\"CD\"] and ef >= self.CRITERION[\"EF\"]:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    \n",
    "    def next_state(self):\n",
    "        \"\"\"Next state (transitions are independent of actions)\"\"\"\n",
    "        state_next = None\n",
    "        current_block = self.blocks[self.phase]\n",
    "        if len(current_block) == 0:\n",
    "            if self.phase == \"Training\":\n",
    "                if self.criterion_reached() or len(self.history[\"Training\"]) >= 360:\n",
    "                    # Move to the Test phase and recalculate the current block.\n",
    "                    self.phase = \"Test\"\n",
    "                      \n",
    "                else:\n",
    "                    self.blocks[\"Training\"] = self.instantiate_block(self.TRAINING_BLOCK)\n",
    "                    \n",
    "                current_block = self.blocks[self.phase]\n",
    "                state_next = current_block.popleft()\n",
    "            \n",
    "            else: \n",
    "                state_next = None # End of the experiment\n",
    "        else:\n",
    "            state_next = current_block.popleft()\n",
    "        return state_next\n",
    "                    \n",
    "    \n",
    "    def execute_action(self, action):\n",
    "        \"\"\"Executes and action and returns the new state and a reward\"\"\"\n",
    "        if self.is_action(action):\n",
    "            if action.startswith(\"-\"):\n",
    "                # This handles the cases where an agent chooses NOT\n",
    "                # to pick a specific action (as in the BG models)\n",
    "                action = [x for x in self.state.options if x is not action[-1]][0]\n",
    "            \n",
    "            r = None\n",
    "            if self.phase == \"Training\":\n",
    "                r = self.get_reward(action)\n",
    "            \n",
    "            # Update history\n",
    "            d = PSS_Decision(self.state, action, reward = r)\n",
    "            self.history[self.phase].append(d)\n",
    "            \n",
    "            self.state = self.next_state()\n",
    "            return (self.state, r)\n",
    "    \n",
    "    def calculate_accuracy(self, decisions, option, exclude = \"None\"):\n",
    "        \"\"\"Calculates accuracy across all decisions that include option 'option' but not option 'exclude'\"\"\"\n",
    "        opt = [x.optimal for x in decisions if x.includes_option(option) and not x.includes_option(exclude)]\n",
    "        return np.mean(opt)\n",
    "        \n",
    "    def accuracies(self):\n",
    "        \"\"\"Returns the Choose / Avoid accuracies\"\"\"\n",
    "        test = self.history[\"Test\"]\n",
    "        if len(test) >= 60:\n",
    "            return (self.calculate_accuracy(test, option = 'A', exclude = 'B'),\n",
    "                    self.calculate_accuracy(test, option = 'B', exclude = 'A'))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d7384-79f6-4da2-bb5c-f292e6fe462a",
   "metadata": {},
   "source": [
    "Now we need to define a few functions to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e99af-3bc1-47d8-b867-577e25e62f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMAgent():\n",
    "    \"\"\"A declarative memory agent\"\"\"\n",
    "    def __init__(self, nsamples=3):\n",
    "        \"\"\"Initializes an agent\"\"\"\n",
    "        self.memory = pau.Memory()\n",
    "        self.nsamples = nsamples\n",
    "        self.init_memory()\n",
    "        \n",
    "    def init_memory(self):\n",
    "        \"\"\"Initializes the agent with some fake memories (to make the first choice possible)\"\"\"\n",
    "        dm = self.memory\n",
    "        dm.learn(action=\"A\", reward=-1)\n",
    "        dm.learn(action=\"B\", reward=-1)\n",
    "        dm.learn(action=\"C\", reward=-1)\n",
    "        dm.learn(action=\"D\", reward=-1)\n",
    "        dm.learn(action=\"E\", reward=-1)\n",
    "        dm.learn(action=\"F\", reward=-1)\n",
    "    \n",
    "    def evaluate(self, option):\n",
    "        \"\"\"Evaluates an option by sampling from memory\"\"\"\n",
    "        res = []\n",
    "        dm = self.memory\n",
    "        for i in range(self.nsamples):\n",
    "            instance = dm.retrieve(action = option)\n",
    "            res.append(instance[\"reward\"])\n",
    "        return np.sum(res)\n",
    "    \n",
    "    def choose(self, state):\n",
    "        \"\"\"Choose an option\"\"\"               \n",
    "        val1, val2 = [self.evaluate(option) for option in [state.left, state.right]]\n",
    "        if val1 > val2:\n",
    "            return state.left\n",
    "        elif val1 < val2:\n",
    "            return state.right\n",
    "        else:\n",
    "            return random.choice([state.left, state.right])\n",
    "                       \n",
    "def PSS_loop(agent, task):\n",
    "    while task.state is not None:\n",
    "        state1 = task.state\n",
    "        choice = agent.choose(state1)\n",
    "        state2, reward1 = task.execute_action(choice)\n",
    "        if reward1 is not None:\n",
    "            agent.memory.learn(action = choice, reward = reward1)\n",
    "        agent.memory.advance(1)\n",
    "    \n",
    "def plot_results(results, param_values):\n",
    "    x = [1,2]   # X axis\n",
    "    jet = cm = plt.get_cmap('jet') \n",
    "    cNorm  = colors.Normalize(vmin = 0, vmax = (len(param_values) - 1))\n",
    "    scalarMap = cmx.ScalarMappable(norm = cNorm, cmap = jet)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for i in range(len(param_values)):\n",
    "        p = param_values[i]\n",
    "        colorVal = scalarMap.to_rgba(i)\n",
    "        ax.plot(x, results[p], \"o-\", color=colorVal)\n",
    "    \n",
    "    ax.axis([0.75, 2.25, 0.48, 0.95])\n",
    "    ax.set_ylabel(\"Mean Accuracy\")\n",
    "    ax.set_xticks([1,2])\n",
    "    ax.set_xticklabels([\"Choose\", \"Avoid\"])\n",
    "    #fig.colorbar(ax, param_values)\n",
    "    plt.legend([\"N = %.2f\" % x for x in param_values], loc=\"best\", ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7336015-3706-4636-9deb-09bf804747e6",
   "metadata": {},
   "source": [
    "Let's do a simple test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de619335-4ed9-4595-b926-604ae126b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DMAgent()\n",
    "task = PSS_Task()\n",
    "PSS_loop(agent, task)\n",
    "task.accuracies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d53879-650a-4cb4-993e-b7c03a9912ce",
   "metadata": {},
   "source": [
    "And let's inspect the model's memories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82084ab8-abd4-4fc6-b42a-8c3e29ed7be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = []\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "labels=[]\n",
    "for action in \"ABCDEF\":\n",
    "    for reward in (1, -1):\n",
    "        labels.append(\"[%s, %d]\" % (action, reward))\n",
    "        chunk = agent.memory.retrieve(action = action, reward = reward)\n",
    "        if chunk is not None:\n",
    "            A.append(chunk._base_activation)\n",
    "        else:\n",
    "            A.append(0)\n",
    "ax.bar(range(len(labels)), height=A, color=[\"orange\", \"dodgerblue\"]*6)\n",
    "ax.set_title(\"Activation of Different Memories\")\n",
    "ax.set_ylabel(\"Activation\")\n",
    "ax.set_xlabel(\"Memories in DM\")\n",
    "ax.set_ylim(-3.5, 2)\n",
    "       \n",
    "for i in range(len(labels)):\n",
    "    ax.text(i, A[i] + 0.1, labels[i], ha=\"center\", va=\"bottom\", rotation=\"vertical\")\n",
    "plt.grid()\n",
    "plt.savefig(\"figures/pss_memory_activation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093cdb2-074c-4a1f-a4c1-8aca860d3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for n in range(1, 11):\n",
    "    performance = []\n",
    "    for i in range( 250 ):\n",
    "        task = PSS_Task()\n",
    "        agent = DMAgent(nsamples = n)\n",
    "        PSS_loop(agent, task)\n",
    "        performance.append(task.accuracies())\n",
    "\n",
    "    results[n] = (np.mean([x[0] for x in performance]), np.mean([x[1] for x in performance]))\n",
    "\n",
    "plot_results(results, list(range(1, 11)))\n",
    "plt.savefig(\"figures/decision_by_sampling.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4c01a8-aed4-44b7-8bfd-81b899a77e93",
   "metadata": {},
   "source": [
    "## Instance-based Learning\n",
    "\n",
    "We can now generalize this approach to complex tasks using the Instance-Based Learning (IBL) framework. In IBL, _instances_ are memorized as episodic memories of the form [_state_, _decision_, _utility_] (_SDU_).\n",
    "\n",
    "We will apply this framework to the RL problem of navigating a maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeaa971-9f79-4a41-8c85-ea0b27fe953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze():\n",
    "    \"\"\"A maze environment\"\"\"\n",
    "\n",
    "    ACTIONS = (\"up\", \"down\", \"left\", \"right\") # List of actions\n",
    "    INITIAL_STATE = (0, 0) # Always starts at the topleft corner\n",
    "    \n",
    "    def __init__(self, fname = \"data/grid.txt\"):\n",
    "        \"\"\"Inits a maze by loading the grid file\"\"\"\n",
    "        self.grid = np.loadtxt(fname)\n",
    "        self.state = self.INITIAL_STATE\n",
    "        self.end = False\n",
    "\n",
    "\n",
    "    def state_transition(self, state1, action1):\n",
    "        \"Defines the next state gien the \"\n",
    "        x, y = state1\n",
    "        \n",
    "        # If we have reached the cheese, we transition \n",
    "        # to the terminal state\n",
    "        if self.grid[x, y] > 0:\n",
    "            return None\n",
    "        \n",
    "        # Otherwise, we update the position\n",
    "        state2 = copy(state1)\n",
    "        \n",
    "        if action1 in self.ACTIONS:\n",
    "            if action1 == \"up\":\n",
    "                if x > 0:\n",
    "                    state2 = (x - 1, y)\n",
    "            \n",
    "            elif action1 == \"left\":\n",
    "                if y > 0:\n",
    "                    state2 = (x, y - 1)\n",
    "            \n",
    "            elif action1 == \"down\":\n",
    "                if x < (self.grid.shape[0] - 1):\n",
    "                    state2 = (x + 1, y)\n",
    "\n",
    "            elif action1 == \"right\":\n",
    "                if y < (self.grid.shape[1] - 1):\n",
    "                    state2 = (x, y + 1)\n",
    "                    \n",
    "        return state2\n",
    "                    \n",
    "    \n",
    "    def reward_transition(self, state1, action1, state2):\n",
    "        \"\"\"Reward is -1 for bouncing against the walls, and whatever is on the grid otherwise\"\"\"\n",
    "        #if state1 == state2:\n",
    "        #    return -1\n",
    "        if state2 == None:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.grid[state2[0], state2[1]]\n",
    "        \n",
    "    \n",
    "    # Quick way to combine State transitions and Reward transitions \n",
    "    def transition(self, action1):\n",
    "        \"\"\"Changes the state following an action\"\"\"\n",
    "        state1 = self.state\n",
    "        state2 = self.state_transition(state1, action1)\n",
    "        reward2 = self.reward_transition(state1, action1, state2)\n",
    "        \n",
    "        self.state = state2\n",
    "        return (state2, reward2) # Returns s_t+1, r_t+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa573f97-4c7a-497c-ac6b-94dc80cb40df",
   "metadata": {},
   "source": [
    "And here is the code for our IBL-based agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9159f0-8ef8-4d35-9d29-fd1e71aca3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBLAgent():\n",
    "    \"\"\"A instance-based learning agent\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes an agent\"\"\"\n",
    "        self.actions = (\"up\", \"left\", \"down\", \"right\")\n",
    "        self.memory = pau.Memory()\n",
    "        self.init_memory()\n",
    "        \n",
    "    def init_memory(self):\n",
    "        \"\"\"Initializes the agent with some preliminary SDUs (to make the first choice possible)\"\"\"\n",
    "        dm = self.memory\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                state = \"<%d-%d>\" % (i, j)\n",
    "                for action in self.actions:\n",
    "                    dm.learn(state = state, decision = action, utility = -1)\n",
    "                \n",
    "    \n",
    "    def evaluate(self, state, option):\n",
    "        \"\"\"Evaluates an option by sampling from memory\"\"\"\n",
    "        res = []\n",
    "        dm = self.memory\n",
    "        return dm.blend(\"utility\", state = state, decision = option)\n",
    "    \n",
    "    def choose(self, state):\n",
    "        \"\"\"Choose an option\"\"\"        \n",
    "        actions = list(self.actions)\n",
    "        random.shuffle(actions)\n",
    "        vals = [self.evaluate(state, x) for x in actions]\n",
    "        min_val = np.min(vals)\n",
    "        V = dict(zip(actions, vals))\n",
    "        best_actions = [x for x in actions if V[x] == min_val]\n",
    "        return random.choice(best_actions)\n",
    "    \n",
    "def maze_run(agent, maze):\n",
    "    while maze.state is not None:\n",
    "        state1 = \"<%d-%d>\" % maze.state\n",
    "        choice = agent.choose(state1)\n",
    "        state2, reward1 = maze.transition(choice)\n",
    "        \n",
    "        if state2 is not None:\n",
    "            state2 = \"<%d-%d>\" % state2\n",
    "        \n",
    "            if reward1 <= 0:\n",
    "                # Use Value of state2\n",
    "                utility = np.max([agent.memory.blend(\"utility\", state = state2, decision = x) for x in agent.actions])\n",
    "            else:\n",
    "                utility = reward1\n",
    "        \n",
    "            agent.memory.learn(state = state1, decision = choice, utility = utility)\n",
    "            agent.memory.advance(1)\n",
    "        \n",
    "def run_trials(agent, maze, n):\n",
    "    for i in range(n):\n",
    "        maze.state = Maze.INITIAL_STATE\n",
    "        maze_run(agent, maze)\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0f1bce-e71d-4e5d-991c-548f7a7a5400",
   "metadata": {},
   "source": [
    "Now, we can visualize what the agent has learned about the utlity of performing any given action in a particular position of the maze.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca8beb-f64e-43b1-888a-42cf496f895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = Maze()\n",
    "agent = IBLAgent()\n",
    "maze_run(agent, maze)\n",
    "run_trials(agent, maze, 8)\n",
    "\n",
    "def plot_ibl_tables(agent, **kwargs):\n",
    "    \"\"\"Visualizes the Q tables, one per action\"\"\"\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(5,5))\n",
    "    #i = 0\n",
    "    for i, a in enumerate(agent.actions):\n",
    "        # Create the corresponding state table\n",
    "        data = np.zeros((4,4))\n",
    "            \n",
    "        for j in range(4):\n",
    "            for k in range(4):\n",
    "                state = \"<%d-%d>\" % (j, k)\n",
    "                data[j, k] = agent.memory.blend(\"utility\", state = state, decision = a)  \n",
    "                \n",
    "        # Plot the heatmap\n",
    "        im = axs.flat[i].imshow(data, **kwargs, cmap=\"viridis\")\n",
    "\n",
    "        # We want to show all ticks...\n",
    "        axs.flat[i].set_xticks(np.arange(data.shape[1]))\n",
    "        axs.flat[i].set_yticks(np.arange(data.shape[0]))\n",
    "        axs.flat[i].set_xticklabels(range(1,5))\n",
    "        axs.flat[i].set_yticklabels(range(1,5))\n",
    "\n",
    "        # Let the horizontal axes labeling appear on top.\n",
    "        axs.flat[i].tick_params(top=False, bottom=True,\n",
    "                                labeltop=False, labelbottom=True)\n",
    "\n",
    "        # Turn spines off and create white grid.\n",
    "        for edge, spine in axs.flat[i].spines.items():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "        axs.flat[i].set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "        axs.flat[i].set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "        axs.flat[i].grid(which=\"minor\", color=\"grey\", linestyle='-', linewidth=2)\n",
    "        axs.flat[i].tick_params(which=\"minor\", bottom=False, left=False)\n",
    "        axs.flat[i].set_title(r\"$BV$s for '%s'\" % (a,))\n",
    "\n",
    "        #i += 1\n",
    "    \n",
    "    for ax in axs.flat:\n",
    "        ax.label_outer()\n",
    "\n",
    "    fig.subplots_adjust(right=0.85, hspace=0.2)\n",
    "    cbar_ax = fig.add_axes([0.88, 0.2, 0.03, 0.6])\n",
    "    fig.colorbar(im, cax=cbar_ax)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "plot_ibl_tables(agent)\n",
    "plt.savefig(\"figures/blended_values_tables.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c9b76e-1051-4656-8643-02d2bcf0eba4",
   "metadata": {},
   "source": [
    "Note the generality of IBL. Because of blending, it is absolutely possible to obtain blended values for _states_ as well as _actions_. In fact, it is possible to naturally represent states and actions in continuous form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c837f7-05b7-405f-9594-0f9c0d9d6530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ibl_state(agent, **kwargs):\n",
    "    \"\"\"Visualizes the V table\"\"\"\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(3,3))\n",
    "    data = np.zeros((4,4))\n",
    "            \n",
    "    for j in range(4):\n",
    "        for k in range(4):\n",
    "            state = \"<%d-%d>\" % (j, k)\n",
    "            data[j, k] = agent.memory.blend(\"utility\", state = state)  \n",
    "                \n",
    "    # Plot the heatmap\n",
    "    im = axs.imshow(data, **kwargs, cmap=\"viridis\")\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    axs.set_xticks(np.arange(data.shape[1]))\n",
    "    axs.set_yticks(np.arange(data.shape[0]))\n",
    "    axs.set_xticklabels(range(1,5))\n",
    "    axs.set_yticklabels(range(1,5))\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    axs.tick_params(top=False, bottom=True,\n",
    "                                labeltop=False, labelbottom=True)\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    for edge, spine in axs.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    axs.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    axs.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    axs.grid(which=\"minor\", color=\"grey\", linestyle='-', linewidth=2)\n",
    "    axs.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    axs.set_title(r\"$BV$s for states\")\n",
    "\n",
    "plot_ibl_state(agent)\n",
    "plt.savefig(\"figures/blended_values_states.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaaf73b-24a0-4759-ad21-2895b0873c49",
   "metadata": {},
   "source": [
    "# Distinguishing IBL and RL \n",
    "\n",
    "IBL and RL make almost overlapping predictions. One clever way to distinguish them in behavioral data was shown by Ann Collins in 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad77ca0b-d2ce-452e-9ed7-e70329f79bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli = (\"car\", \"house\", \"knife\", \"cup\", \"fork\", \"screwdriver\")\n",
    "responses = (\"a\", \"b\", \"c\")\n",
    "\n",
    "class CollinsTask():\n",
    "    def __init__(self, nstimuli = 3, num = 12):\n",
    "        self.numrep = num\n",
    "        self.init_stimuli(nstimuli)\n",
    "        self.numtrials = num * self.nstimuli\n",
    "        self.history = []\n",
    "        \n",
    "    def init_stimuli(self, nstimuli):\n",
    "        \"\"\"Fix the stimuli\"\"\"\n",
    "        self.stimuli = stimuli[:nstimuli]\n",
    "        self.nstimuli = nstimuli\n",
    "        assoc = {}\n",
    "        for stimulus in self.stimuli:\n",
    "            assoc[stimulus] = random.choice(responses)\n",
    "        self.associations = assoc\n",
    "        \n",
    "        trials = list(self.stimuli) * self.numrep\n",
    "        random.shuffle(trials)\n",
    "        self.trials = trials\n",
    "        \n",
    "        self.current = 0\n",
    "        self.state = self.trials[self.current]\n",
    "    \n",
    "    def transition(self, action):\n",
    "        \"\"\"A s1, a1 -> s2, r1 transition\"\"\"\n",
    "        stim = self.state \n",
    "        reward = 0\n",
    "        if action == self.associations[stim]:\n",
    "            reward = +1\n",
    "        \n",
    "        self.current += 1\n",
    "        if self.current < len(self.trials):\n",
    "            self.state = self.trials[self.current]\n",
    "        else:\n",
    "            self.state = None\n",
    "        state2 = self.state\n",
    "        self.history.append((stim, reward))\n",
    "        return (state2, reward)\n",
    "        \n",
    "    def curve(self):\n",
    "        lists =[]\n",
    "        for s in self.stimuli:\n",
    "            performance = [x[1] for x in self.history if x[0] == s]\n",
    "            lists.append(performance)\n",
    "        array = np.array(lists)\n",
    "        return np.mean(array, axis=0)\n",
    "    \n",
    "        \n",
    "class RLModel():\n",
    "    def __init__(self, epsilon = 0.1, alpha = 0.1):\n",
    "        self.Q = {}    ## Initial dictionary of (s, a) pairs. At the beginning, it's emtpy.\n",
    "        self.epsilon = epsilon     # Epsilon for e-greey policy\n",
    "        self.alpha = alpha         # Learning rate\n",
    "        self.actions = responses     # Set of possible actions (provide those of Maze.ACTIONS)\n",
    "        \n",
    "    def choose(self, state):\n",
    "        \"\"\"Selects an action with a epsilon-greedy policy\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(self.actions)\n",
    "        else:\n",
    "            q = [self.Q[(state, a)] if (state, a) in self.Q.keys() else 0.0 for a in self.actions]\n",
    "            maxQ = max(q)\n",
    "            count = q.count(maxQ)\n",
    "            if count > 1:\n",
    "                best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "                i = random.choice(best)\n",
    "            else:\n",
    "                i = q.index(maxQ)\n",
    "\n",
    "            action = self.actions[i]\n",
    "        return action\n",
    "    \n",
    "    def learn(self, state, action, reward):\n",
    "        a = self.alpha\n",
    "        \n",
    "        qval = 0.0\n",
    "        \n",
    "        if (state, action) in self.Q.keys():\n",
    "            q1 = self.Q[(state, action)]\n",
    "        \n",
    "        \n",
    "        rpe = reward - qval\n",
    "        qval += a * rpe\n",
    "        self.Q[(state, action)] = qval\n",
    "        \n",
    "    \n",
    "class IBLModel():\n",
    "    def __init__(self):\n",
    "        self.memory = pau.Memory(threshold=0.0)\n",
    "        self.actions = responses\n",
    "        \n",
    "    def choose(self, state):\n",
    "        res = [self.memory.retrieve(situation = state, action = x, utility = +1) for x in self.actions]\n",
    "        res = [0 if x is None else 1 for x in res]\n",
    "        count = res.count(1)\n",
    "        \n",
    "        if count > 1:\n",
    "            best = [i for i in range(len(res)) if res[i] == 1]\n",
    "            i = random.choice(best)\n",
    "            \n",
    "        elif count == 1:\n",
    "            i = res.index(1)\n",
    "        \n",
    "        else:\n",
    "            i = random.choice(range(len(self.actions)))\n",
    "        \n",
    "        return self.actions[i]\n",
    "    \n",
    "    def learn(self, state, action, reward):\n",
    "        state = \"%s\" % (state,)\n",
    "        self.memory.learn(advance = 0.5, situation = state, action = action, utility = reward)\n",
    "        \n",
    "\n",
    "def collins_loop(agent, task):\n",
    "    while task.state is not None:\n",
    "        state1 = task.state\n",
    "        action1 = agent.choose(state1)\n",
    "        state2, reward1 = task.transition(action1)\n",
    "        agent.learn(state1, action1, reward1)\n",
    "        \n",
    "def run_sims(atype, nstimuli, n):\n",
    "    partials = np.zeros((n, 12))\n",
    "    \n",
    "    for i in range(n):\n",
    "        if atype == \"IBL\":\n",
    "            agent = IBLModel()\n",
    "        else:\n",
    "            agent = RLModel()\n",
    "        task = CollinsTask(nstimuli = nstimuli)\n",
    "        collins_loop(agent, task)\n",
    "        partials[i,] = task.curve()\n",
    "        \n",
    "    return np.mean(partials, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8219d7b-fcae-48c1-92a8-33bec9adfa50",
   "metadata": {},
   "source": [
    "Let's see how both models perform the Collins task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef191dd9-739f-4c73-905f-e4fb8a920b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "NSIMS = 500\n",
    "\n",
    "rl3 = run_sims(\"RL\", 3, NSIMS)\n",
    "rl6 = run_sims(\"RL\", 6, NSIMS)\n",
    "ibl3 = run_sims(\"IBL\", 3, NSIMS)\n",
    "ibl6 = run_sims(\"IBL\", 6, NSIMS)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "axs[0].plot(rl3, \"-o\", label = \"$N$ = 3\")\n",
    "axs[0].plot(rl6, \"-o\", label = \"$N$ = 6\")\n",
    "axs[0].set_title(\"RL Agent\")\n",
    "#axs[0].legend()\n",
    "\n",
    "axs[1].plot(ibl3, \"-o\", label = \"$N$ = 3\")\n",
    "axs[1].plot(ibl6, \"-o\", label = \"$N$ = 6\")\n",
    "axs[1].set_title(\"IBL Agent\")\n",
    "#axs[0].legend()\n",
    "\n",
    "for ax in axs:\n",
    "    ax.legend()\n",
    "    \n",
    "    xticks = list(range(12))\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels([1 + i for i in xticks])\n",
    "    ax.set_xlabel(\"Stimulus Presentation\")\n",
    "    \n",
    "    ax.set_ylabel(\"Proportion Correct\")\n",
    "    \n",
    "                   \n",
    "fig.suptitle(\"Performance of RL and IBL agents in the Collins task\") \n",
    "fig.tight_layout()\n",
    "plt.savefig(\"figures/rl_vs_ibl.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
