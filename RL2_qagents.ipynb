{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Code, Part 2: Agents that act\n",
    "\n",
    "\n",
    "Here we are going to implement a RL agent interacting with a simple environment. In this case, our agent would be a simulated RL mouse, and the environment a 2D maze.\n",
    "\n",
    "## Defining the environment\n",
    "\n",
    "To define the environment, we need to define the set of possible states $S = {s_1, s_2 ... s_N}$, the transition function $P_{s,s'}^{a}$, and the reward transition function $R_{s,s'}^{a}$.\n",
    "\n",
    "In our case, the environment just consists of a 4x4 grid. Our hypothetical agent perceives only one cell at any time---the cell where it is.  Therefore, our states correspond to the sixteen position of the maze, which we can indicate with the coordinates $(0, 0), (0, 1)... (0, 3), (1, 0) ... (3, 3)$.\n",
    "\n",
    "An environment is characterized by two functions:\n",
    "\n",
    "* The state transition probability function $P(s, a, s')$, which the probability of transitioning to a possible state $s'$ when action $a$ is applied during state $s$; and\n",
    "\n",
    "* The reward transition probability function $R(s, a, s')$, which is the probability of receiving a reward $r$ when action $a$ is applied to state $s$ and the environment moves to state $s'\n",
    "\n",
    "In our simple cases, both $P(s,a,s')$ and $R(s,a,s')$ will be simplified to deterministic functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "class Maze():\n",
    "    \"\"\"A maze environment\"\"\"\n",
    "\n",
    "    ACTIONS = (\"up\", \"down\", \"left\", \"right\") # List of actions\n",
    "    INITIAL_STATE = (0, 0) # Always starts at the topleft corner\n",
    "    \n",
    "    def __init__(self, fname = \"grid.txt\"):\n",
    "        \"\"\"Inits a maze by loading the grid file\"\"\"\n",
    "        self.grid = np.loadtxt(fname)\n",
    "        self.state = self.INITIAL_STATE\n",
    "        self.end = False\n",
    "\n",
    "\n",
    "    def state_transition(self, state1, action1):\n",
    "        \"Defines the next state gien the \"\n",
    "        s = state1\n",
    "        state2 = copy(state1)\n",
    "        \n",
    "        if action1 in self.ACTIONS:\n",
    "            if action1 == \"up\":\n",
    "                if s[0] > 0:\n",
    "                    state2 = (s[0] - 1, s[1])\n",
    "            \n",
    "            elif action1 == \"left\":\n",
    "                if s[1] > 0:\n",
    "                    state2 = (s[0], s[1] - 1)\n",
    "            \n",
    "            elif action1 == \"down\":\n",
    "                if s[0] < (self.grid.shape[0] - 1):\n",
    "                    state2 = (s[0] + 1, s[1])\n",
    "\n",
    "            elif action1 == \"right\":\n",
    "                if s[1] < (self.grid.shape[1] - 1):\n",
    "                    state2 = (s[0], s[1] + 1)\n",
    "                    \n",
    "        return state2\n",
    "                    \n",
    "    \n",
    "    def reward_transition(self, state1, action1, state2):\n",
    "        \"\"\"Reward is -1 for bouncing against the walls, and whatever is on the grid otherwise\"\"\"\n",
    "        if state1 == state2:\n",
    "            return -1\n",
    "        else:\n",
    "            return self.grid[state2[0], state2[1]]\n",
    "        \n",
    "    \n",
    "    # Quick way to combine State transitions and Reward transitions \n",
    "    def transition(self, action1):\n",
    "        \"\"\"Changes the state following an action\"\"\"\n",
    "        state1 = self.state\n",
    "        state2 = self.state_transition(state1, action1)\n",
    "        reward2 = self.reward_transition(state1, action1, state2)\n",
    "        \n",
    "        self.state = state2\n",
    "        return (state2, reward2) # Returns s_t+1, r_t+1\n",
    "\n",
    "    \n",
    "    def print_state(self):\n",
    "        \"Prints a text representation of the maze (with the agent position)\"\n",
    "        bar = \"-\" * ( 4 * self.grid.shape[1] + 1)\n",
    "        for i in range(self.grid.shape[0]):\n",
    "            row = \"|\"\n",
    "            for j in range(self.grid.shape[1]):\n",
    "                cell = \" \"\n",
    "                if i == self.state[0] and j == self.state[1]:\n",
    "                    cell = \"*\"\n",
    "                row += (\" %s |\" % cell)\n",
    "            print(bar)\n",
    "            print(row)\n",
    "        print(bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the maze\n",
    "The maze is simple but functional. It is easy to create a maze, check the available actions, apply a few actions, and so on. Let's check that our environment works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Maze()\n",
    "m.print_state()\n",
    "state_after_bad_action = m.state_transition(m.state, \"up\")  # Illigal action: bounces!\n",
    "state_after_good_action = m.state_transition(m.state, \"down\") # Legal action: Goes down\n",
    "\n",
    "print(\"State after illegal action: %s\" % (state_after_bad_action,))\n",
    "print(\"Reward after illegal action: %s\" % (m.reward_transition(m.state, \"up\", state_after_bad_action)))\n",
    "\n",
    "print(\"State after legal action: %s\" % (state_after_good_action,))\n",
    "print(\"Reward after legal action: %s\" % (m.reward_transition(m.state, \"down\", state_after_good_action)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, we can easily navigate in our virtual maze by executing the appropriate actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.transition(\"down\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the ```Maze``` object returns two values at the end of each action execution, the new state $s_{t+1}$ and the associated reward $r_{t+1}$. If the ```down``` action was executed with the original maze layout of the ```grid.txt``` file, case, the two values are $s_{t+1} = $ ```(3, 0)``` and $r_{t+1} = $ ```0.0```. We can also execute more actions, and see what happens after a few movements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.transition(\"down\")\n",
    "m.transition(\"down\")\n",
    "m.transition(\"right\")\n",
    "m.print_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Q-Agent\n",
    "Now we can create our own very fantastic agents! As an example, we will create a $Q$-learning agent that interacts with the ``` Maze``` world. The $Q$-table should contain $16 \\times 4 = 64$ different state-action pairs; rather than filling it up right away, we will fill it up as we encounter new state-action pairs, assuming that any previously unencountered state has a value of $Q = 0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, actions=Maze.ACTIONS, epsilon=0.1, alpha=0.1, gamma=0.9):\n",
    "        \"\"\"Creates a Q-agent\"\"\"\n",
    "        self.Q = {}    ## Initial dictionary of (s, a) pairs. At the beginning, it's emtpy.\n",
    "\n",
    "        self.epsilon = epsilon     # Epsilon for e-greey policy\n",
    "        self.alpha = alpha         # Learning rate\n",
    "        self.gamma = gamma         # Temporal discounting\n",
    "        self.actions = actions     # Set of possible actions (provide those of Maze.ACTIONS)\n",
    "\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"Selects an action with a epsilon-greedy policy\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(self.actions)\n",
    "        else:\n",
    "            q = [self.Q[(state, a)] if (state, a) in self.Q.keys() else 0.0 for a in self.actions]\n",
    "            maxQ = max(q)\n",
    "            count = q.count(maxQ)\n",
    "            if count > 1:\n",
    "                best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "                i = random.choice(best)\n",
    "            else:\n",
    "                i = q.index(maxQ)\n",
    "\n",
    "            action = self.actions[i]\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def learnQ(self, state1, action1, reward2, state2, action2):\n",
    "        \"\"\"Updates the Q-values when given an (s,a) pair, the reward value and a new state\"\"\"\n",
    "        g = self.gamma\n",
    "        a = self.alpha\n",
    "        \n",
    "        q1 = 0.0\n",
    "        \n",
    "        if (state1, action1) in self.Q.keys():\n",
    "            q1 = self.Q[(state1, action1)]\n",
    "        \n",
    "        max_q2 = max([self.Q[(state2, a)] if (state2, a) in self.Q.keys() else 0.0 for a in self.actions])\n",
    "        \n",
    "        rpe = reward2 + g * max_q2 - q1\n",
    "        #print(\"s1 = %s, a1 = %s, s2 = %s, q1 = %.4f, r2 = %.4f, q2 = %.4f\" % (state1, action1, state2, q1, reward2, max_q2))\n",
    "        q1 += a * rpe\n",
    "        #print(\"new q1 = %.4f\" % (q1,))\n",
    "        self.Q[(state1, action1)] = q1\n",
    "\n",
    "            \n",
    "    def calculateV(self, task):\n",
    "        \"\"\"Returns a representation of the best Q values in every states\"\"\"\n",
    "        v = np.zeros(task.grid.shape)\n",
    "        for i in range(v.shape[0]):\n",
    "            for j in range(v.shape[1]):\n",
    "                maxq = max([self.getQ((i, j), a) for a in self.actions])\n",
    "                v[i,j] =maxq\n",
    "        return v\n",
    "    \n",
    "    def visualizeQ(self):\n",
    "        \"\"\"Visualizes the Q tables, one per action\"\"\"\n",
    "        for a in self.actions:\n",
    "            # Create the corresponding state table\n",
    "            mat = np.zeros((4,4))\n",
    "            states = [x for x in self.Q.keys() if x[1] == a]\n",
    "            \n",
    "            for s in states:\n",
    "                row, col = s[0]\n",
    "                mat[row, col] = self.Q[s]\n",
    "            \n",
    "            # Show the Q-table as a heatmap\n",
    "            plt.imshow(mat, interpolation = \"none\", vmin=-1, vmax=10, cmap='jet')\n",
    "            plt.title(\"Q-Table for action '%s'\" % a.upper())\n",
    "            plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Interaction between Environment and Agent\n",
    "Now, two functions to run multiple trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(environment, agent):\n",
    "    \"\"\"A trial ends when the agent gets a reward. The history is returned\"\"\"\n",
    "    state1 = environment.state\n",
    "    action1 = agent.policy(state1)\n",
    "    reward2 = 0.0\n",
    "    history = [state1]\n",
    "    while reward2 != 10:\n",
    "        state2, reward2 = environment.transition(action1)\n",
    "        action2 = agent.policy(state2)\n",
    "        \n",
    "        # Save the states visited\n",
    "        history.append(state2)\n",
    "        \n",
    "        # Update the Q-values for state1, action1\n",
    "        agent.learnQ(state1, action1, reward2, state2, action2)\n",
    "        \n",
    "        state1 = state2\n",
    "        action1 = action2\n",
    "        \n",
    "    return history\n",
    "\n",
    "    \n",
    "def run_trials(environment, agent, n, collect=True):\n",
    "    \"\"\"Runs N trials\"\"\"\n",
    "    history = []\n",
    "    for j in range(n):\n",
    "        h = run_trial(environment, agent)\n",
    "        history += h\n",
    "        environment.state = Maze.INITIAL_STATE\n",
    "    \n",
    "    return history\n",
    "        \n",
    "def visualize_history(history):\n",
    "    mat = np.zeros((4,4))            \n",
    "    for s in history:\n",
    "        row, col = s\n",
    "        mat[row, col] += 1\n",
    "    \n",
    "    mat /= np.sum(mat)\n",
    "    mat *= 5  # min number of moves\n",
    "    \n",
    "    plt.imshow(mat, interpolation = \"none\", vmin=0, vmax=1, cmap='jet')\n",
    "    plt.title(\"Preferred path\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the agent\n",
    "\n",
    "Here a few tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Maze()\n",
    "a = Agent()\n",
    "run_trials(m, a, 10000)\n",
    "a.visualizeQ()\n",
    "h = run_trials(m, a, 10)\n",
    "visualize_history(h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
