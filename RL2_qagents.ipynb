{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Code, Part 2: Agents that act\n",
    "\n",
    "\n",
    "Here we are going to implement a RL agent interacting with a simple environment. In this case, our agent would be a simulated RL mouse, and the environment a 2D maze.\n",
    "\n",
    "## Defining the environment\n",
    "\n",
    "To define the environment, we need to define the set of possible states $S = {s_1, s_2 ... s_N}$, the transition function $P_{s,s'}^{a}$, and the reward transition function $R_{s,s'}^{a}$.\n",
    "\n",
    "In our case, the environment just consists of a 4x4 grid. Our hypothetical agent perceives only one cell at any time---the cell where it is.  Therefore, our states correspond to the sixteen position of the maze, which we can indicate with the coordinates $(0, 0), (0, 1)... (0, 3), (1, 0) ... (3, 3)$.\n",
    "\n",
    "An environment is characterized by two functions:\n",
    "\n",
    "* The state transition probability function $P(s, a, s')$, which the probability of transitioning to a possible state $s'$ when action $a$ is applied during state $s$; and\n",
    "\n",
    "* The reward transition probability function $R(s, a, s')$, which is the probability of receiving a reward $r$ when action $a$ is applied to state $s$ and the environment moves to state $s'\n",
    "\n",
    "In our simple cases, both $P(s,a,s')$ and $R(s,a,s')$ will be simplified to deterministic functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "class Maze():\n",
    "    \"\"\"A maze environment\"\"\"\n",
    "\n",
    "    ACTIONS = (\"up\", \"down\", \"left\", \"right\") # List of actions\n",
    "    INITIAL_STATE = (0, 0) # Always starts at the topleft corner\n",
    "    \n",
    "    def __init__(self, fname = \"grid.txt\"):\n",
    "        \"\"\"Inits a maze by loading the grid file\"\"\"\n",
    "        self.grid = np.loadtxt(fname)\n",
    "        self.state = self.INITIAL_STATE\n",
    "        self.end = False\n",
    "\n",
    "\n",
    "    def state_transition(self, state, action):\n",
    "        s = state\n",
    "        new_s = copy(s)\n",
    "        \n",
    "        if action in self.ACTIONS:\n",
    "            if action == \"up\":\n",
    "                if s[0] > 0:\n",
    "                    new_s = (s[0] - 1, s[1])\n",
    "            \n",
    "            elif action == \"left\":\n",
    "                if s[1] > 0:\n",
    "                    new_s = (s[0], s[1] - 1)\n",
    "            \n",
    "            elif action == \"down\":\n",
    "                if s[0] < (self.grid.shape[0] - 1):\n",
    "                    new_s = (s[0] + 1, s[1])\n",
    "\n",
    "            else:\n",
    "                if s[1] < (self.grid.shape[1] -1):\n",
    "                    new_s = (s[0], s[1] + 1)\n",
    "                    \n",
    "        return new_s\n",
    "                    \n",
    "    \n",
    "    def reward_transition(self, state, action, new_state):\n",
    "        if state == new_state:\n",
    "            return -1\n",
    "        else:\n",
    "            return self.grid[new_state[0], new_state[1]]\n",
    "        \n",
    "    \n",
    "    # Quick way to combine State transitions and Reward transitions \n",
    "    def transition(self, action):\n",
    "        \"\"\"Changes the state following an action\"\"\"\n",
    "        s = self.state\n",
    "        new_s = self.state_transition(s, action)\n",
    "        new_r = self.reward_transition(s, action, new_s)\n",
    "        \n",
    "        self.state = new_s\n",
    "        return (new_s, new_r) # Returns s_t, r_t\n",
    "\n",
    "    def print_state(self):\n",
    "        \"Prints a text representation of the maze (with the agent position)\"\n",
    "        bar = \"-\" * ( 4 * self.grid.shape[1] + 1)\n",
    "        for i in range(self.grid.shape[0]):\n",
    "            row = \"|\"\n",
    "            for j in range(self.grid.shape[1]):\n",
    "                cell = \" \"\n",
    "                if i == self.state[0] and j == self.state[1]:\n",
    "                    cell = \"*\"\n",
    "                row += (\" %s |\" % cell)\n",
    "            print(bar)\n",
    "            print(row)\n",
    "        print(bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maze is simple but functional. It is easy to create a maze, check the available actions, apply a few actions, and so on. Let's check that our environment works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "| * |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "State after illegal action: (0, 0)\n",
      "Reward after illegal action: -1\n",
      "State after legal action: (1, 0)\n",
      "Reward after legal action: 0.0\n"
     ]
    }
   ],
   "source": [
    "m = Maze()\n",
    "m.print_state()\n",
    "state_after_bad_action = m.state_transition(m.state, \"up\")  # Illigal action: bounces!\n",
    "state_after_good_action = m.state_transition(m.state, \"down\") # Legal action: Goes down\n",
    "\n",
    "print(\"State after illegal action: %s\" % (state_after_bad_action,))\n",
    "print(\"Reward after illegal action: %s\" % (m.reward_transition(m.state, \"up\", state_after_bad_action)))\n",
    "\n",
    "print(\"State after legal action: %s\" % (state_after_good_action,))\n",
    "print(\"Reward after legal action: %s\" % (m.reward_transition(m.state, \"down\", state_after_good_action)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, we can easily navigate in our virtual maze by executing the appropriate actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 0), 0.0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.transition(\"down\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the ```Maze``` object returns two values at the end of each action execution, the new state $s_{t+1}$ and the associated reward $r_{t+1}$. If the ```down``` action was executed with the original maze layout of the ```grid.txt``` file, case, the two values are $s_{t+1} = $ ```(3, 0)``` and $r_{t+1} = $ ```0.0```. We can also execute more actions, and see what happens after a few movements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   | * |   |   |\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "m.transition(\"down\")\n",
    "m.transition(\"down\")\n",
    "m.transition(\"right\")\n",
    "m.print_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our own very fantastic agents! As an example, we will create a $Q$-learning agent that interacts with the ``` Maze``` world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, actions=Maze.ACTIONS, epsilon=0.1, alpha=0.2, gamma=0.9):\n",
    "        \"\"\"Creates a Q-agent\"\"\"\n",
    "        self.Q = {}    ## Initial dictionary of (s, a) pairs. At the beginning, it's emtpy.\n",
    "\n",
    "        self.epsilon = epsilon     # Epsilon for e-greey policy\n",
    "        self.alpha = alpha         # Learning rate\n",
    "        self.gamma = gamma         # Temporal discounting\n",
    "        self.actions = actions     # Set of possible actions (provide those of Maze.ACTIONS)\n",
    "\n",
    "    def getQ(self, state, action):\n",
    "        \"\"\"Returns the Q-value associated to a given state and action (or 0.0 if unknown)\"\"\"\n",
    "        return self.Q.get((state, action), 0.0)\n",
    "        \n",
    "    def updateQ(self, state, action, reward, value):\n",
    "        \"\"\"Updates the value Q(s, a) by a given value\"\"\"\n",
    "        oldv = self.Q.get((state, action), None)\n",
    "        if oldv is None:\n",
    "            self.q[(state, action)] = reward\n",
    "        else:\n",
    "            self.q[(state, action)] = oldv + self.alpha * (value - oldv)\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"Selects an action with a epsilon-greedy policy\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(self.actions)\n",
    "        else:\n",
    "            q = [self.getQ(state, a) for a in self.actions]\n",
    "            maxQ = max(q)\n",
    "            count = q.count(maxQ)\n",
    "            if count > 1:\n",
    "                best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "                i = random.choice(best)\n",
    "            else:\n",
    "                i = q.index(maxQ)\n",
    "\n",
    "            action = self.actions[i]\n",
    "        return action\n",
    "\n",
    "    def learn(self, state1, action1, reward, state2, action2):\n",
    "        \"\"\"Updates the Q-values when given an (s,a) pair, the reward value and a new state\"\"\"\n",
    "        g = self.gamma\n",
    "        a = self.alpha\n",
    "        \n",
    "        q1 = self.getQ(state1, action1)\n",
    "        max_q2 = max([self.getQ(state2, a) for a in self.actions])\n",
    "        \n",
    "        rpe = reward + g * max_q2 - q1 \n",
    "        self.Q[(state1, action1)] = q1 + a * rpe\n",
    "\n",
    "    def run(self, task, n):\n",
    "        \"\"\"Run through N moves in the maze\"\"\"\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            s = task.state\n",
    "            a = self.choose_action(s)\n",
    "            new_s, r = task.execute_action(a)\n",
    "            self.learn(s, a, r, new_s)\n",
    "            i += 1\n",
    "\n",
    "    def calculateV(self, task):\n",
    "        \"\"\"Returns a representation of the best Q values in every states\"\"\"\n",
    "        v = np.zeros(task.grid.shape)\n",
    "        for i in range(v.shape[0]):\n",
    "            for j in range(v.shape[1]):\n",
    "                maxq = max([self.getQ((i, j), a) for a in self.actions])\n",
    "                v[i,j] =maxq\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, two functions to run multiple trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(environment, agent):\n",
    "    \"A trial ends when the agent gets a reward\"\n",
    "    state1 = environment.state\n",
    "    reward1 = 0.0\n",
    "    action1 = agent.policy(state1)\n",
    "    \n",
    "    while reward1 != 10:\n",
    "        state2, reward2 = environment.transition(action1)\n",
    "        action2 = agent.policy(state2)\n",
    "        \n",
    "        # Update the Q-values for state1, action1\n",
    "        agent.learn(state1, action1, reward1, state2, action2)\n",
    "        \n",
    "        # Next trial\n",
    "        state1 = state2\n",
    "        action1 = action2\n",
    "        reward1 = reward2\n",
    "        \n",
    "        \n",
    "    agent.learn(state1, action1, reward1, None, None)\n",
    "\n",
    "def run_trials(environment, agent, n):\n",
    "    for j in range(n):\n",
    "        run_trial(environment, agent)\n",
    "        environment.state = Maze.INITIAL_STATE\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Maze()\n",
    "a = Agent()\n",
    "run_trials(m, a, 1)\n",
    "m.state\n",
    "a.Q\n",
    "run_trials(m, a, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
