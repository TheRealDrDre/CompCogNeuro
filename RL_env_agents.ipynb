{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning code\n",
    "Here we are going to implement a RL agent interacting with a simple environment. In this case, our agent would be a simulated RL mouse, and the environment a 2D maze.\n",
    "\n",
    "## Defining the environment\n",
    "\n",
    "To define the environment, we need to define the set of possible states $S = {s_1, s_2 ... s_N}$, the transition function $P_{s,s'}^{a}$, and the reward transition function $R_{s,s'}^{a}$.\n",
    "\n",
    "In our case, the environment just consists of a 4x4 grid. Our hypothetical agent perceives only one cell at any time---the cell where it is.  Therefore, our states correspond to the sixteen position of the maze, which we can indicate with the coordinates $(0, 0), (0, 1)... (0, 3), (1, 0) ... (3, 3)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Maze():\n",
    "    \"\"\"A maze environment\"\"\"\n",
    "\n",
    "    ACTIONS = (\"up\", \"down\", \"left\", \"right\") # List of actions\n",
    "    INITIAL_STATE = (0, 0) # Always starts at the topleft corner\n",
    "    \n",
    "    def __init__(self, fname = \"grid.txt\"):\n",
    "        \"\"\"Inits a maze by loading the grid file\"\"\"\n",
    "        self.grid = np.loadtxt(fname)\n",
    "        self.state = self.INITIAL_STATE\n",
    "\n",
    "        \n",
    "    def execute_action(self, action):\n",
    "        \"\"\"Executes one of four possible actions: up, down, right, and left\"\"\"\n",
    "        s = self.state\n",
    "        \n",
    "        # By default, it assumes you bounced into a wall\n",
    "        new_s = s\n",
    "        new_r = -1\n",
    "        \n",
    "        if action in self.ACTIONS:\n",
    "            if action == \"up\":\n",
    "                if s[0] > 0:\n",
    "                    new_s = (s[0] - 1, s[1])\n",
    "                    new_r = self.grid[new_s[0], new_s[1]]\n",
    "            \n",
    "            elif action == \"left\":\n",
    "                if s[1] > 0:\n",
    "                    new_s = (s[0], s[1] - 1)\n",
    "                    new_r = self.grid[new_s[0], new_s[1]]\n",
    "            \n",
    "            elif action == \"down\":\n",
    "                if s[0] < (self.grid.shape[0] - 1):\n",
    "                    new_s = (s[0] + 1, s[1])\n",
    "                    new_r = self.grid[new_s[0], new_s[1]]\n",
    "\n",
    "            else:\n",
    "                if s[1] < (self.grid.shape[1] -1):\n",
    "                    new_s = (s[0], s[1] + 1)\n",
    "                    new_r = self.grid[new_s[0], new_s[1]]\n",
    "\n",
    "        # If you find the cheese, you go back to square #1\n",
    "        # ---This keeps the environment looping---\n",
    "        if new_r == 10:\n",
    "            new_s =  self.INITIAL_STATE\n",
    "\n",
    "        # Updates the state and reward, and returns them as a tuple \n",
    "        self.state = new_s\n",
    "        return (new_s, new_r)\n",
    "\n",
    "    def print_state(self):\n",
    "        \"Prints a text representation of the maze (with the agent position)\"\n",
    "        bar = \"-\" * ( 4 * self.grid.shape[1] + 1)\n",
    "        for i in range(self.grid.shape[0]):\n",
    "            row = \"|\"\n",
    "            for j in range(self.grid.shape[1]):\n",
    "                cell = \" \"\n",
    "                if i == self.state[0] and j == self.state[1]:\n",
    "                    cell = \"*\"\n",
    "                row += (\" %s |\" % cell)\n",
    "            print(bar)\n",
    "            print(row)\n",
    "        print(bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maze is simple but functional. It is easy to create a maze, check the available actions, execute a few actions, and so on. For example, we can create a new ```Maze``` object and have the possible actions readily available: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('up', 'down', 'left', 'right')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Maze()\n",
    "m.ACTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the ```print_state()``` method to display a very low-tech representation of the maze, with teh agent's position indicated by the start symbol ```*```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "| * |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "m.print_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, we can easily navigate in our virtual maze by executing the appropriate actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 0), 0.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.execute_action(\"down\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the ```Maze``` object returns two values at the end of each action execution, the new state $s_{t+1}$ and the associated reward $r_{t+1}$. If the ```down``` action was executed with the original maze layout of the ```grid.txt``` file, case, the two values are $s_{t+1} = $ ```(3, 0)``` and $r_{t+1} = $ ```0.0```. We can also execute more actions, and see what happens after a few movements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   | * |   |   |\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "m.execute_action(\"down\")\n",
    "m.execute_action(\"down\")\n",
    "m.execute_action(\"right\")\n",
    "m.print_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our own very fantastic agents! As an example, we will create a $Q$-learning agent that interacts with the ``` Maze``` world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class QAgent():\n",
    "    def __init__(self, actions, epsilon=0.1, alpha=0.2, gamma=0.9):\n",
    "        \"\"\"Creates a Q-agent\"\"\"\n",
    "        self.q = {}    ## Initial dictionary of (s, a) pairs. At the beginning, it's emtpy.\n",
    "\n",
    "        self.epsilon = epsilon     # Epsilon for e-greey policy\n",
    "        self.alpha = alpha         # Learning rate\n",
    "        self.gamma = gamma         # Temporal discounting\n",
    "        self.actions = actions     # Set of possible actions (provide those of Maze.ACTIONS)\n",
    "\n",
    "    def getQ(self, state, action):\n",
    "        \"\"\"Returns the Q-value associated to a given state and action (or 0.0 if unknown)\"\"\"\n",
    "        return self.q.get((state, action), 0.0)\n",
    "        \n",
    "    def updateQ(self, state, action, reward, value):\n",
    "        \"\"\"Updates the value Q(s, a) by a given value\"\"\"\n",
    "        oldv = self.q.get((state, action), None)\n",
    "        if oldv is None:\n",
    "            self.q[(state, action)] = reward\n",
    "        else:\n",
    "            self.q[(state, action)] = oldv + self.alpha * (value - oldv)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Selects an action with a epsilong-greedy policy\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(self.actions)\n",
    "        else:\n",
    "            q = [self.getQ(state, a) for a in self.actions]\n",
    "            maxQ = max(q)\n",
    "            count = q.count(maxQ)\n",
    "            if count > 1:\n",
    "                best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "                i = random.choice(best)\n",
    "            else:\n",
    "                i = q.index(maxQ)\n",
    "\n",
    "            action = self.actions[i]\n",
    "        return action\n",
    "\n",
    "    def learn(self, state1, action1, reward, state2):\n",
    "        \"\"\"Updates the Q-values when given an (s,a) pair, the reward value and a new state\"\"\"\n",
    "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
    "        self.updateQ(state1, action1, reward, reward + self.gamma*maxqnew)\n",
    "\n",
    "    def run(self, task, n):\n",
    "        \"\"\"Run through N moves in the maze\"\"\"\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            s = task.state\n",
    "            a = self.choose_action(s)\n",
    "            new_s, r = task.execute_action(a)\n",
    "            self.learn(s, a, r, new_s)\n",
    "            i += 1\n",
    "\n",
    "    def calculateV(self, task):\n",
    "        \"\"\"Returns a representation of the best Q values in every states\"\"\"\n",
    "        v = np.zeros(task.grid.shape)\n",
    "        for i in range(v.shape[0]):\n",
    "            for j in range(v.shape[1]):\n",
    "                maxq = max([self.getQ((i, j), a) for a in self.actions])\n",
    "                v[i,j] =maxq\n",
    "        return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
