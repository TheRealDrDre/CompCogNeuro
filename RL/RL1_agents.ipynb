{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning, Part 1: Agents, States, and Rewards  \n",
    "\n",
    "Here we will show the simplest form of RL: An agent going through a simple environment with a determinstic succession of states.\n",
    "\n",
    "## The environment\n",
    "\n",
    "The environment represents a prototypical conditing paradigm. It is made of three states, which represent an initial  cue, a delay, and a reward. This captures a simple experiment such as the case in which a primate is given reward (juice) after a blue light is presented.\n",
    "\n",
    "An environment is defined by two functions, the state transition matrix $P(s,a,s')$ and a reward transition matrix $R(s,a,s')$. In this simple case, the agent does not act---it is simply observing the environment. So, we can define $P(s,s')$ and $R(s,s')$ without any reference to any action $a$. Furthermore, because the environment is deterministic, we can represent both $P(s,s')$ and $R(s,s')$ as _tables_.\n",
    "\n",
    "### Implementation in Python\n",
    "\n",
    "In the code below, the Environment's state and reward transitions are implemented as _dictionaries_, that is, special structures that associate one object (the _key_) with another (the _value_). This is a vary convenient format for a simple environments in which all events follow deterministically. THe `STATE_TRANSITIONS` dictionary links each state with the state that follows. The `REWARD_TRANSITIONS` dictionary links each state with its associated reward.  The null object `None` marks the end of a trial.\n",
    "\n",
    "When the environment undergoes a transition, the agent receives a new state-reward pair, which, in Python, will be represented as a tuple `(new_state, new_reward)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "class Environment:\n",
    "    \"\"\"A simple environment\"\"\"\n",
    "    STATE_TRANSITIONS = {\"cue\" : \"wait\",\n",
    "                         \"wait\" : \"juice\",\n",
    "                         \"juice\" : None}\n",
    "    \n",
    "    REWARD_TRANSITIONS = {\"cue\" : 0,\n",
    "                          \"wait\" : 0,\n",
    "                          \"juice\" : 1,\n",
    "                          None : 0}\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the environment\"\"\"\n",
    "        # An environment class always keeps track of the current state we are in.\n",
    "        # We always begin with the state 'cue' (the beginning of a trial)\n",
    "        self.state = \"cue\"\n",
    "    \n",
    "    \n",
    "    def transition(self):\n",
    "        \"\"\"Transitions, and returns the state-reward pair\"\"\"\n",
    "        state = self.state  # The current state\n",
    "        \n",
    "        new_state = Environment.STATE_TRANSITIONS[state]\n",
    "        new_reward = Environment.REWARD_TRANSITIONS[new_state]\n",
    "        \n",
    "        # Let's update the current state before \n",
    "        self.state = new_state\n",
    "        \n",
    "        # The agent receives the new state and reward. \n",
    "        return (new_state, new_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the environment\n",
    "\n",
    "Let's test the environment works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 1: 'cue'\n",
      "State 2: 'wait', Reward = 0\n",
      "State 3: 'juice', Reward = 1\n",
      "After state 3: 'None'\n"
     ]
    }
   ],
   "source": [
    "e = Environment()\n",
    "print(\"State 1: '%s'\" % (e.state,))\n",
    "\n",
    "# First transition: Cue -> Wait\n",
    "res = e.transition()\n",
    "print(\"State 2: '%s', Reward = %s\" % res)\n",
    "\n",
    "# Second transition: Wait -> Juice\n",
    "res = e.transition()\n",
    "print(\"State 3: '%s', Reward = %s\" % res)\n",
    "\n",
    "# Final transition: Juice -> 'None'\n",
    "res = e.transition()\n",
    "print(\"After state 3: '%s'\" % (e.state,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "Now, let's create a simple agent that just observes states and estimates their values by creating a $V$-table. Because the number of states is so small, the $V$-table can be initialized right away, with all the states being zero. \n",
    "\n",
    "The $V$-Agent learns the value of states using TD-learning. Every time the agent observes a new state, it updates the associated value of the previous state using the TD-learning equation:\n",
    "\n",
    "$V(S_t) = V(S_t) + \\alpha [ r_{t} + \\gamma V(S_{t+1}) - V(S_t)]$\n",
    "\n",
    "### Implementation in Python\n",
    "\n",
    "The agent is an object with two internal parameters, `alpha` and `gamma` (which record to the $\\alpha$ and $\\gamma$ values of the TD-learning equation), and a dictionary that records the $V$-values associated with every state (The $V$ table). The agent has a single method, `learn`, tha implements the TD-learning equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDAgent():\n",
    "    \"\"\"An agent that passively observes states\"\"\"\n",
    "    def __init__(self, gamma=0.9, alpha=0.1):\n",
    "        \"Initializes an agent with default parameters and zero values in the V table\"\n",
    "        self.V = {\"cue\" : 0,\n",
    "                  \"wait\" : 0,\n",
    "                  \"juice\" : 0,\n",
    "                  None : 0}\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        \n",
    "    # This method implements TD-learning\n",
    "    #\n",
    "    def TD_learn(self, state1, reward1, state2):\n",
    "        \"\"\"Learns using TD-learning\"\"\"\n",
    "        V1 = self.V[state1]\n",
    "        V2 = self.V[state2]\n",
    "        g = self.gamma\n",
    "        a = self.alpha\n",
    "        \n",
    "        # Calculate RPE and new estimate of V1\n",
    "        rpe = reward1 + g * V2 - V1\n",
    "        V1 = V1 + a * rpe\n",
    "        \n",
    "        # Update the internal value of V1\n",
    "        self.V[state1] = V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions between agent and environment\n",
    "Now, we need to define how to run a trial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_loop(environment, agent):\n",
    "    \"The interaction cycle between an agent and its environment\"\n",
    "    while environment.state is not None:\n",
    "        state = environment.state\n",
    "        reward = environment.REWARD_TRANSITIONS[state]\n",
    "        \n",
    "        # This is the moment in which we learn!\n",
    "        transition = environment.transition()\n",
    "        new_state = None\n",
    "        if transition is not None:\n",
    "            new_state = transition[0]\n",
    "        \n",
    "        agent.TD_learn(state, reward, new_state)\n",
    "\n",
    "        \n",
    "def run_trials(environment, agent, n):\n",
    "    \"Run multiple simulations within an environment\"\n",
    "    for j in range(n):\n",
    "        environment.state = \"cue\"\n",
    "        rl_loop(environment, agent)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the system\n",
    "\n",
    "To test the system, we need to create a function that visualizes a $V$-table. The following function takes the an agent's dictionary of state-value associations and visualizes them ina bargraph. For simplicity, the state `None` is not shown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple function to visualize the agent's V-table\n",
    "def plot_v_table(Vtable, axes, title=\"V table\"):\n",
    "    \"\"\"Visualizes the V-table\"\"\"\n",
    "    states = [\"cue\", \"wait\", \"juice\"]\n",
    "    values = [Vtable[x] for x in states]\n",
    "\n",
    "    axes.axis([-0.5, 2.5, 0, 1.0])\n",
    "    axes.set_xticks([0, 1, 2])\n",
    "    axes.set_xticklabels(states)\n",
    "    axes.bar([0, 1, 2], values)\n",
    "    axes.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can test the agent's learning by visualizing its internal $V$-table at different momements during learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x252 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "states = [\"cue\", \"wait\", \"juice\", None]\n",
    "\n",
    "a = TDAgent()\n",
    "e = Environment()\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(7,3.5), sharey=True)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "axs[0].set_ylabel(r\"Value $V$\")\n",
    "axs[1].set_xlabel(\"States\")    \n",
    "\n",
    "run_trials(e, a, 1)\n",
    "plot_v_table(a.V, axs[0], r\"$V$-table ($t=1$)\")\n",
    "\n",
    "run_trials(e, a, 9)\n",
    "plot_v_table(a.V, axs[1], r\"$V$-table ($t=10$)\")\n",
    "\n",
    "run_trials(e, a, 90)\n",
    "plot_v_table(a.V, axs[2], r\"$V$-table ($t=100$)\")\n",
    "\n",
    "plt.savefig(\"figures/vtable.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
