{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a6aae1-8344-4ebf-8b81-df29b3af442c",
   "metadata": {},
   "source": [
    "# Reinforcement Learning, Part 5: Continuous States and Actions\n",
    "\n",
    "So far, our agents have been designed to have a simple memory, entirely made of tables. Most real-world problems, however, require dealing with continuous quantities. Mazes do not come divided in cells, and navigation does not happen in single steps along the four cardinal directions.\n",
    "\n",
    "To deal with this, we need to make a jump from $V$- and $Q$-tables to approximatig $V$- and $Q$-_functions_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3bd24db-79ef-4853-b2a2-8fc86745a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ad81f3-99c1-4139-a950-35f4c9f8b719",
   "metadata": {},
   "source": [
    "## A continuous maze\n",
    "\n",
    "We will now create a continuous environment, a maze that is not made of predefined cells but of a continuous 2D space, bounded in both axes in the range (0, 1).\n",
    "\n",
    "The reward (the cheese) is placed in a specific position, (0.75, 0.75). The reward is obtained whenever the agent falls within a distance of < 0.1 from that point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "56c25c1d-095d-4b31-9fda-848e1e9f8578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(v1, v2):\n",
    "    \"\"\"2D Euclidean distance\"\"\"\n",
    "    if v1 is None or v2 is None:\n",
    "        return 0.0\n",
    "    else:\n",
    "        x1, y1 = v1\n",
    "        x2, y2 = v2\n",
    "        return math.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f53defc-bafb-4336-a2aa-989fc0c28235",
   "metadata": {},
   "source": [
    "Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "083c2e86-8649-4638-a76e-e42194447f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze():\n",
    "    \"\"\"A maze environment\"\"\"\n",
    "\n",
    "    INITIAL_STATE = (0, 0) # Always starts at the topleft corner\n",
    "    CHEESE_POS = (0.75, 0.75)\n",
    "    CHEESE_RAD = 0.1\n",
    "    \n",
    "    def is_reward(self, state):\n",
    "        \"\"\"Determins if we are close enough to the cheese\"\"\"\n",
    "        a = np.array(state)\n",
    "        b = np.array(Maze.CHEESE_POS)\n",
    "        if math.sqrt(np.sum((a-b)**2)) < Maze.CHEESE_RAD:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        \n",
    "    def reward(self, state):\n",
    "        if state is None:\n",
    "            return 0\n",
    "        elif self.is_reward(state):\n",
    "            return 10\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "    def __init__(self):\n",
    "        \"\"\"Inits a maze by loading the grid file\"\"\"\n",
    "        self.state = self.INITIAL_STATE\n",
    "        self.end = False\n",
    "\n",
    "\n",
    "    def state_transition(self, state1, action1):\n",
    "        \"Defines the next state given the current action\"\n",
    "        # If we have reached the cheese, we transition \n",
    "        # to the terminal state\n",
    "        if self.is_reward(state1):\n",
    "            return None\n",
    "        \n",
    "        x1, y1 = state1\n",
    "        dx, dy = action1\n",
    "        \n",
    "        x2 = min(max(0, x1 + dx), 1)\n",
    "        y2 = min(max(0, y1 + dy), 1)\n",
    "                            \n",
    "        return (x2, y2)\n",
    "                    \n",
    "    \n",
    "    def reward_transition(self, state1, action1, state2):\n",
    "        \"\"\"Reward is -1 for bouncing against the walls, and whatever is on the grid otherwise\"\"\"\n",
    "        #if state1 == state2:\n",
    "        #    return -1\n",
    "        if state2 == None:\n",
    "            return 0\n",
    "        else:\n",
    "            if self.is_reward(state2):\n",
    "                return 10\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "    \n",
    "    # Quick way to combine State transitions and Reward transitions \n",
    "    def transition(self, action1):\n",
    "        \"\"\"Changes the state following an action\"\"\"\n",
    "        state1 = self.state\n",
    "        state2 = self.state_transition(state1, action1)\n",
    "        reward2 = self.reward_transition(state1, action1, state2)\n",
    "        \n",
    "        self.state = state2\n",
    "        return (state2, reward2) # Returns s_t+1, r_t+1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ccaae-b0c3-4952-bddf-e1f8cd622511",
   "metadata": {},
   "source": [
    "## State aggregation \n",
    "The easiest method to handle continuous states is called _state aggregation_, and is basically a discretization of the continuous space.\n",
    "Now, the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "16376e15-e267-4c71-8b16-ec4a73d72f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class ContinuousTDAgent():\n",
    "    \"\"\"An agent that keeps track of the value of states\"\"\"\n",
    "    def __init__(self, alpha=0.1, gamma=0.9, step=0.1, res=10):\n",
    "        \"\"\"Creates a V-agent\"\"\"\n",
    "        self.step = step\n",
    "        #self.W = [0.0 for x in CENTERS]\n",
    "        self.alpha = alpha         # Learning rate\n",
    "        self.gamma = gamma         # Temporal discounting\n",
    "        self.values = np.zeros((res, res))\n",
    "        self.res = res\n",
    "        \n",
    "    def policy(self, state):\n",
    "        \"\"\"Random policy to explore the maze\"\"\"\n",
    "        angle = random.uniform(0, math.pi * 2)\n",
    "        s = self.step\n",
    "        return (math.sin(angle) * s, math.cos(angle) * s)\n",
    "        \n",
    "    \n",
    "    def td_learning(self, state1, reward1, state2):\n",
    "        \"\"\"Updates the Q-values when given an (s,a) pair, the reward value and a new state\"\"\"\n",
    "        g = self.gamma\n",
    "        a = self.alpha\n",
    "\n",
    "        x1, y1 = state1\n",
    "        n = self.res - 1\n",
    "        vx1 = min(math.floor(self.res * x1), n)\n",
    "        vy1 = min(math.floor(self.res * y1), n)\n",
    "        \n",
    "        v1 = self.values[vx1, vy1]\n",
    "        \n",
    "        v2 = 0\n",
    "        \n",
    "        if state2 is not None:\n",
    "            x2, y2 = state2\n",
    "            vx2 = min(math.floor(self.res * x2), n)\n",
    "            vy2 = min(math.floor(self.res * y2), n)\n",
    "        \n",
    "            v2 = self.values[vx2, vy2]\n",
    "                \n",
    "        rpe = reward1 + g * v2 - v1\n",
    "        self.values[vx1, vy1] += a * rpe\n",
    "\n",
    "    \n",
    "def plot_v_table(agent, ax=None, title=r\"$V$-table\", **kwargs):\n",
    "    \"Plots a matrix-like representation of the V-table of an agent\"\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    data = np.zeros((4,4))            \n",
    "            \n",
    "    for s in agent.V.keys():\n",
    "        x, y = s\n",
    "        data[x, y] = agent.V[s]\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs, cmap=\"viridis\")\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    ax.set_xticklabels(range(1,5))\n",
    "    ax.set_yticklabels(range(1,5))\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=False, bottom=True,\n",
    "                   labeltop=False, labelbottom=True)\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    for edge, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"grey\", linestyle='-', linewidth=2)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    ax.set_title(title)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f2b2cd97-7e1e-4458-88ec-b27638059ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_loop(environment, agent):\n",
    "    \"\"\"A trial ends when the agent gets a reward. The history is returned\"\"\"\n",
    "    state1 = environment.state\n",
    "    reward1 = environment.reward(state1)\n",
    "    state2 = \"Start\"\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    while state2 != None:\n",
    "        action = agent.policy(state1)\n",
    "        state2, reward2 = environment.transition(action)\n",
    "        history.append(state2)\n",
    "        # Update the V-values for state1\n",
    "        agent.td_learning(state1, reward1, state2)\n",
    "        \n",
    "        state1 = state2\n",
    "        reward1 = reward2\n",
    "\n",
    "    return history\n",
    "\n",
    "    \n",
    "def run_trials(environment, agent, n, collect=True):\n",
    "    \"\"\"Runs N trials\"\"\"\n",
    "    history = []\n",
    "    for j in range(n):\n",
    "        h = rl_loop(environment, agent)\n",
    "        history.append(h)\n",
    "        environment.state = Maze.INITIAL_STATE\n",
    "    \n",
    "    return history    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7a350ba3-552a-48b8-a89a-4e852e865816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f71d23c3430>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = Maze()\n",
    "a = ContinuousTDAgent(alpha=0.1, res=20)\n",
    "\n",
    "# Because the agent moves at random, the following instruction can take a variable amount of time to complete\n",
    "h=run_trials(m, a, 1000)\n",
    "a.values\n",
    "plt.imshow(a.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a967c-9226-4a85-8eff-a181ea8498c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t[1])\n",
    "xs = [j[0] for j in h[:-1]]\n",
    "ys = [j[1] for j in h[:-1]]\n",
    "plt.plot(xs, ys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
