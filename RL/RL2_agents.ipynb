{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL: Part 2: Agents That Act in an Environment\n",
    "\n",
    "\n",
    "Here we are going to implement a RL agent interacting with a simple environment. In this case, our agent would be a simulated RL mouse, and the environment a 2D maze.\n",
    "\n",
    "## Defining the Environment\n",
    "\n",
    "To define the environment, we need to define the set of possible states $S = {s_1, s_2 ... s_N}$, the transition function $P_{s,s'}^{a}$, and the reward transition function $R_{s,s'}^{a}$.\n",
    "\n",
    "In our case, the environment just consists of a 4x4 grid. Our hypothetical agent perceives only one cell at any time---the cell where it is.  Therefore, our states correspond to the sixteen position of the maze, which we can indicate with the coordinates $(0, 0), (0, 1) ... (0, 3), (1, 0) ... (3, 3)$.\n",
    "\n",
    "An environment is characterized by two functions:\n",
    "\n",
    "* The state transition probability function $P(s, a, s')$, which the probability of transitioning to a possible state $s'$ when action $a$ is applied during state $s$; and\n",
    "\n",
    "* The reward transition probability function $R(s, a, s')$, which is the probability of receiving a reward $r$ when action $a$ is applied to state $s$ and the environment moves to state $s'$.\n",
    "\n",
    "In our simple cases, both $P(s,a,s')$ and $R(s,a,s')$ will be simplified to deterministic functions.\n",
    "\n",
    "A run of the maze ends when the simulated \"rat\" agent finds the cheese reward. To simulate this fact, we will add a \"termination\" state, indicated as `None`. The transition function will automatically move the agent to a `None` state whatever action is taken after the cheese is found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "class Maze():\n",
    "    \"\"\"A maze environment\"\"\"\n",
    "\n",
    "    ACTIONS = (\"up\", \"down\", \"left\", \"right\") # List of actions\n",
    "    INITIAL_STATE = (0, 0) # Always starts at the topleft corner\n",
    "    \n",
    "    def __init__(self, fname = \"grid.txt\"):\n",
    "        \"\"\"Inits a maze by loading the grid file\"\"\"\n",
    "        self.grid = np.loadtxt(fname)\n",
    "        self.state = self.INITIAL_STATE\n",
    "        self.end = False\n",
    "\n",
    "\n",
    "    def state_transition(self, state1, action1):\n",
    "        \"Defines the next state gien the \"\n",
    "        x, y = state1\n",
    "        \n",
    "        # If we have reached the cheese, we transition \n",
    "        # to the terminal state\n",
    "        if self.grid[x, y] > 0:\n",
    "            return None\n",
    "        \n",
    "        # Otherwise, we update the position\n",
    "        state2 = copy(state1)\n",
    "        \n",
    "        if action1 in self.ACTIONS:\n",
    "            if action1 == \"up\":\n",
    "                if x > 0:\n",
    "                    state2 = (x - 1, y)\n",
    "            \n",
    "            elif action1 == \"left\":\n",
    "                if y > 0:\n",
    "                    state2 = (x, y - 1)\n",
    "            \n",
    "            elif action1 == \"down\":\n",
    "                if x < (self.grid.shape[0] - 1):\n",
    "                    state2 = (x + 1, y)\n",
    "\n",
    "            elif action1 == \"right\":\n",
    "                if y < (self.grid.shape[1] - 1):\n",
    "                    state2 = (x, y + 1)\n",
    "                    \n",
    "        return state2\n",
    "                    \n",
    "    \n",
    "    def reward_transition(self, state1, action1, state2):\n",
    "        \"\"\"Reward is -1 for bouncing against the walls, and whatever is on the grid otherwise\"\"\"\n",
    "        if state1 == state2:\n",
    "            return -1\n",
    "        elif state2 == None:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.grid[state2[0], state2[1]]\n",
    "        \n",
    "    \n",
    "    # Quick way to combine State transitions and Reward transitions \n",
    "    def transition(self, action1):\n",
    "        \"\"\"Changes the state following an action\"\"\"\n",
    "        state1 = self.state\n",
    "        state2 = self.state_transition(state1, action1)\n",
    "        reward2 = self.reward_transition(state1, action1, state2)\n",
    "        \n",
    "        self.state = state2\n",
    "        return (state2, reward2) # Returns s_t+1, r_t+1\n",
    "\n",
    "    \n",
    "    def print_state(self):\n",
    "        \"Prints a text representation of the maze (with the agent position)\"\n",
    "        bar = \"-\" * ( 4 * self.grid.shape[1] + 1)\n",
    "        for i in range(self.grid.shape[0]):\n",
    "            row = \"|\"\n",
    "            for j in range(self.grid.shape[1]):\n",
    "                cell = \" \"\n",
    "                if i == self.state[0] and j == self.state[1]:\n",
    "                    cell = \"*\"\n",
    "                row += (\" %s |\" % cell)\n",
    "            print(bar)\n",
    "            print(row)\n",
    "        print(bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the maze\n",
    "The maze is simple but functional. It is easy to create a maze, check the available actions, apply a few actions, and so on. Let's check that our environment works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "| * |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "State after illegal action: (0, 0)\n",
      "Reward after illegal action: -1\n",
      "State after legal action: (1, 0)\n",
      "Reward after legal action: 0.0\n"
     ]
    }
   ],
   "source": [
    "m = Maze()\n",
    "m.print_state()\n",
    "state_after_bad_action = m.state_transition(m.state, \"up\")  # Illigal action: bounces!\n",
    "state_after_good_action = m.state_transition(m.state, \"down\") # Legal action: Goes down\n",
    "\n",
    "print(\"State after illegal action: %s\" % (state_after_bad_action,))\n",
    "print(\"Reward after illegal action: %s\" % (m.reward_transition(m.state, \"up\", state_after_bad_action)))\n",
    "\n",
    "print(\"State after legal action: %s\" % (state_after_good_action,))\n",
    "print(\"Reward after legal action: %s\" % (m.reward_transition(m.state, \"down\", state_after_good_action)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, we can easily navigate in our virtual maze by executing the appropriate actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 0), 0.0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.transition(\"down\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the ```Maze``` object returns two values at the end of each action execution, the new state $s_{t+1}$ and the associated reward $r_{t+1}$. If the ```down``` action was executed with the original maze layout of the ```grid.txt``` file, case, the two values are $s_{t+1} = $ ```(3, 0)``` and $r_{t+1} = $ ```0.0```. We can also execute more actions, and see what happens after a few movements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   | * |   |   |\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "m.transition(\"down\")\n",
    "m.transition(\"down\")\n",
    "m.transition(\"right\")\n",
    "m.print_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a $V$-Agent\n",
    "\n",
    "Now we can create our own very fantastic agents! As an example, we will create a $V$-learning agent that interacts with the ``` Maze``` world.\n",
    "\n",
    "The agent contains two parts:\n",
    "\n",
    "1. The agent's _memory_ is composed of a single $V$-table, which contains  $4 \\times 4 = 16$ states, one for every possible position in the maze. \n",
    "2. The agent's _policy_ is a simple policy that selects actions _at random_. This guarantees that the agent will explore the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# -- NAMING CONVENTIONS --\n",
    "# s_{t}   = state1\n",
    "# a_{t}   = action1\n",
    "# s_{t+1} = state2\n",
    "# r_{t+1} = reward2\n",
    "# a_{t+1} = action2\n",
    "\n",
    "\n",
    "class VAgent():\n",
    "    \"\"\"An agent that keeps track of the value of states\"\"\"\n",
    "    def __init__(self, actions=Maze.ACTIONS, alpha=0.1, gamma=0.9):\n",
    "        \"\"\"Creates a V-agent\"\"\"\n",
    "        self.V = {}                # Initial dictionary of (s, a) pairs. At the beginning, it's emtpy.\n",
    "        self.alpha = alpha         # Learning rate\n",
    "        self.gamma = gamma         # Temporal discounting\n",
    "        self.actions = actions     # Set of possible actions (provide those of Maze.ACTIONS)\n",
    "\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"Random policy to explore the maze\"\"\"\n",
    "        return random.choice(self.actions)\n",
    "        \n",
    "    \n",
    "    def learnV(self, state1, reward1, state2):\n",
    "        \"\"\"Updates the Q-values when given an (s,a) pair, the reward value and a new state\"\"\"\n",
    "        g = self.gamma\n",
    "        a = self.alpha\n",
    "        \n",
    "        v1 = 0.0\n",
    "        \n",
    "        if state1 in self.V.keys():\n",
    "            v1 = self.V[state1]\n",
    "        \n",
    "        v2 = 0.0\n",
    "        \n",
    "        if state2 in self.V.keys():\n",
    "            v2 = self.V[state2]\n",
    "        \n",
    "        rpe = reward1 + g * v2 - v1\n",
    "        v1 += a * rpe\n",
    "\n",
    "        self.V[state1] = v1\n",
    "\n",
    "            \n",
    "    def visualizeV(self, axes):\n",
    "        \"\"\"Visualizes the V table\"\"\"\n",
    "        # Create the corresponding state table\n",
    "        mat = np.zeros((4,4))\n",
    "        states = [x for x in self.V.keys()]\n",
    "            \n",
    "        for s in states:\n",
    "            row, col = s\n",
    "            mat[row, col] = self.V[s]\n",
    "            \n",
    "        # Show the Q-table as a heatmap\n",
    "        im=axes.matshow(mat, vmin=-1, vmax=10, cmap='viridis')\n",
    "        return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Between $V$-Agent and Environment\n",
    "\n",
    "We need to additional functions to make sure the agent and the environment interact with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(environment, agent):\n",
    "    \"\"\"A trial ends when the agent gets a reward. The history is returned\"\"\"\n",
    "    state1 = environment.state\n",
    "    reward1 = environment.grid[state1[0], state1[1]]\n",
    "    state2 = \"Start\"\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    while state2 != None:\n",
    "        action = agent.policy(state1)\n",
    "        state2, reward2 = environment.transition(action)\n",
    "        history.append(state2)\n",
    "        \n",
    "        # Update the V-values for state1\n",
    "        agent.learnV(state1, reward1, state2)\n",
    "        \n",
    "        state1 = state2\n",
    "        reward1 = reward2\n",
    "\n",
    "    return history\n",
    "\n",
    "    \n",
    "def run_trials(environment, agent, n, collect=True):\n",
    "    \"\"\"Runs N trials\"\"\"\n",
    "    history = []\n",
    "    for j in range(n):\n",
    "        h = run_trial(environment, agent)\n",
    "        history += h\n",
    "        environment.state = Maze.INITIAL_STATE\n",
    "    \n",
    "    return history    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the agent\n",
    "\n",
    "Now, we can finally test the agent. Note that the cell for the \"reward\" state is (where the \"food\" is) remains at zero; this is because the trial actually ends when the agent arrives in the final cell. Because TD-learning methods propagate back in time, another state (\"End\", for example) would be needed to actually assign the correct value to that cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = Maze()\n",
    "a = VAgent(alpha=0.1)\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(7,3))\n",
    "\n",
    "# Because the agent moves at random, the following instruction can take a variable amount of time to complete\n",
    "run_trials(m, a, 1)  \n",
    "a.visualizeV(axs[0])\n",
    "axs[0].set_title(r\"$V$-Table ($t$=1)\")\n",
    "\n",
    "run_trials(m, a, 10)\n",
    "a.visualizeV(axs[1])\n",
    "axs[1].set_title(r\"$V$-Table ($t$=10)\")\n",
    "\n",
    "run_trials(m, a, 1000)\n",
    "im = a.visualizeV(axs[2])\n",
    "axs[2].set_title(r\"$V$-Table ($t$=1000)\")\n",
    "\n",
    "#fig.colorbar(im, cax=cbar_ax)\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.88, 0.2, 0.03, 0.6])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "plt.savefig(\"vtable_maze.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a $Q$-Agent\n",
    "Now we will create a $Q$-learning agent that interacts with the ``` Maze``` world. The $Q$-table should contain $16 \\times 4 = 64$ different state-action pairs; rather than filling it up right away, we will fill it up as we encounter new state-action pairs, assuming that any previously unencountered state has a value of $Q = 0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class QAgent():\n",
    "    def __init__(self, actions=Maze.ACTIONS, epsilon=0.1, alpha=0.1, gamma=0.9):\n",
    "        \"\"\"Creates a Q-agent\"\"\"\n",
    "        self.Q = {}    ## Initial dictionary of (s, a) pairs. At the beginning, it's emtpy.\n",
    "\n",
    "        self.epsilon = epsilon     # Epsilon for e-greey policy\n",
    "        self.alpha = alpha         # Learning rate\n",
    "        self.gamma = gamma         # Temporal discounting\n",
    "        self.actions = actions     # Set of possible actions (provide those of Maze.ACTIONS)\n",
    "\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"Selects an action with a epsilon-greedy policy\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(self.actions)\n",
    "        else:\n",
    "            q = [self.Q[(state, a)] if (state, a) in self.Q.keys() else 0.0 for a in self.actions]\n",
    "            maxQ = max(q)\n",
    "            count = q.count(maxQ)\n",
    "            if count > 1:\n",
    "                best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "                i = random.choice(best)\n",
    "            else:\n",
    "                i = q.index(maxQ)\n",
    "\n",
    "            action = self.actions[i]\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def learnQ(self, state1, action1, reward1, state2, action2):\n",
    "        \"\"\"Updates the Q-values when given an (s,a) pair, the reward value and a new state\"\"\"\n",
    "        g = self.gamma\n",
    "        a = self.alpha\n",
    "        \n",
    "        q1 = 0.0\n",
    "        \n",
    "        if (state1, action1) in self.Q.keys():\n",
    "            q1 = self.Q[(state1, action1)]\n",
    "        \n",
    "        max_q2 = max([self.Q[(state2, a)] if (state2, a) in self.Q.keys() else 0.0 for a in self.actions])\n",
    "        \n",
    "        rpe = reward1 + g * max_q2 - q1\n",
    "        #print(\"s1 = %s, a1 = %s, s2 = %s, q1 = %.4f, r2 = %.4f, q2 = %.4f\" % (state1, action1, state2, q1, reward2, max_q2))\n",
    "        q1 += a * rpe\n",
    "        #print(\"new q1 = %.4f\" % (q1,))\n",
    "        self.Q[(state1, action1)] = q1\n",
    "\n",
    "            \n",
    "    def visualizeQ(self):\n",
    "        \"\"\"Visualizes the Q tables, one per action\"\"\"\n",
    "        fig, axs = plt.subplots(2,2, figsize=(5,5))\n",
    "        i = 0\n",
    "        for a in self.actions:\n",
    "            # Create the corresponding state table\n",
    "            mat = np.zeros((4,4))\n",
    "            states = [x for x in self.Q.keys() if x[1] == a]\n",
    "            \n",
    "            for s in states:\n",
    "                row, col = s[0]\n",
    "                mat[row, col] = self.Q[s]\n",
    "            \n",
    "            # Show the Q-table as a heatmap\n",
    "            axs.flat[i].matshow(mat, interpolation = \"none\", vmin=-1, vmax=10, cmap='inferno')\n",
    "            axs.flat[i].set_title(r\"$Q$-Table for '%s'\" % a.upper())\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions between $Q$-Agent and Environment\n",
    "\n",
    "For a $Q$-Agent to work properly, we need to redefine the `run_trial` function to handle to more complex expression that is needed for $Q$-learning and SARSA.  Because the agent actually _learns_ how to navigate the maze, we can also visualize the agent's preferred paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(environment, agent):\n",
    "    \"\"\"A trial ends when the agent gets a reward. The history is returned\"\"\"\n",
    "    state1  = environment.state\n",
    "    reward1 = environment.grid[state1[0], state1[1]]\n",
    "    \n",
    "    action1 = agent.policy(state1)\n",
    "    \n",
    "    history = []\n",
    "    state2  = \"\"\n",
    "    \n",
    "    while state2 != None:\n",
    "        # See the next state \n",
    "        state2, reward2 = environment.transition(action1)\n",
    "        \n",
    "        # Decide the next action\n",
    "        action2 = agent.policy(state2)\n",
    "        \n",
    "        # Save the states visited\n",
    "        history.append(state1)\n",
    "        \n",
    "        # Update the Q-values for state1, action1\n",
    "        agent.learnQ(state1, action1, reward1, state2, action2)\n",
    "        \n",
    "        state1 = state2\n",
    "        reward1 = reward2\n",
    "        action1 = action2\n",
    "        \n",
    "    return history\n",
    "\n",
    "\n",
    "def visualize_history(history):\n",
    "    \"Visualizes the number of times each cell in the Maze has been transversed\"\n",
    "    mat = np.zeros((4,4))            \n",
    "    for s in history:\n",
    "        row, col = s\n",
    "        mat[row, col] += 1\n",
    "    \n",
    "    mat /= np.sum(mat)\n",
    "    mat *= 5  # min number of moves\n",
    "    \n",
    "    plt.imshow(mat, interpolation = \"none\", vmin=0, vmax=1, cmap='inferno')\n",
    "    plt.title(\"Preferred path\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the agent\n",
    "\n",
    "Here a few tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = Maze()\n",
    "a = QAgent(epsilon = 0.6)\n",
    "run_trials(m, a, 300)\n",
    "a.visualizeQ()\n",
    "plt.show()\n",
    "h = run_trials(m, a, 100)\n",
    "visualize_history(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are the $Q$ values changing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials_for_altair(environment, agent, n, collect=True):\n",
    "    \"\"\"Runs N trials\"\"\"\n",
    "    state_action = {} \n",
    "    # init all state_actions\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for direction in ['down', 'right', 'up', 'left']:\n",
    "                state_action[str(((i, j), direction))] = [0] \n",
    "\n",
    "    for j in range(n):\n",
    "        run_trial(environment, agent)\n",
    "        all_keys = set(state_action.keys())\n",
    "        # keys with new values\n",
    "        for key, val in agent.Q.items():\n",
    "            state_action[str(key)].append(val)\n",
    "            all_keys.remove(str(key))\n",
    "        # keys without new values\n",
    "        for key in all_keys:\n",
    "            state_action[str(key)].append(state_action[str(key)][-1])\n",
    "        environment.state = Maze.INITIAL_STATE\n",
    "        \n",
    "    import pandas as pd\n",
    "    location = []\n",
    "    run = []\n",
    "    q_value = []\n",
    "    for loc in state_action.keys():\n",
    "        for i in range(len(state_action[loc])):\n",
    "            location.append(loc)\n",
    "            run.append(i)\n",
    "            q_value.append(state_action[loc][i])\n",
    "    df = pd.DataFrame({\"location\":location, \"run\":run, \"q_value\":q_value})\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-1f7d71123f76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_trials_for_altair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Agent' is not defined"
     ]
    }
   ],
   "source": [
    "m = Maze()\n",
    "a = Agent()\n",
    "df = run_trials_for_altair(m, a, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "slider = alt.binding_range(min=1, max=100, step=1)\n",
    "select_run = alt.selection_single(name=\"iteration\", fields=['run'], bind=slider)\n",
    "alt.data_transformers.enable('default', max_rows=None)\n",
    "alt.Chart(df).mark_bar().encode(\n",
    "    x='location:N',\n",
    "    y=alt.Y('q_value:Q', scale=alt.Scale(domain=(0, 11))),\n",
    ").add_selection(\n",
    "    select_run\n",
    ").transform_filter(\n",
    "    select_run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
