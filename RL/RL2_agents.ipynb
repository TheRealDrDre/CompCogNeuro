{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL: Part 2: Agents That Act in an Environment\n",
    "\n",
    "\n",
    "Here we are going to implement a RL agent interacting with a simple environment. In this case, our agent would be a simulated RL mouse, and the environment a 2D maze.\n",
    "\n",
    "## Defining the Environment\n",
    "\n",
    "To define the environment, we need to define the set of possible states $S = {s_1, s_2 ... s_N}$, the transition function $P_{s,s'}^{a}$, and the reward transition function $R_{s,s'}^{a}$.\n",
    "\n",
    "In our case, the environment just consists of a 4x4 grid. Our hypothetical agent perceives only one cell at any time---the cell where it is.  Therefore, our states correspond to the sixteen position of the maze, which we can indicate with the coordinates $(0, 0), (0, 1) ... (0, 3), (1, 0) ... (3, 3)$.\n",
    "\n",
    "An environment is characterized by two functions:\n",
    "\n",
    "* The state transition probability function $P(s, a, s')$, which the probability of transitioning to a possible state $s'$ when action $a$ is applied during state $s$; and\n",
    "\n",
    "* The reward transition probability function $R(s, a, s')$, which is the probability of receiving a reward $r$ when action $a$ is applied to state $s$ and the environment moves to state $s'$.\n",
    "\n",
    "In our simple cases, both $P(s,a,s')$ and $R(s,a,s')$ will be simplified to deterministic functions.\n",
    "\n",
    "A run of the maze ends when the simulated \"rat\" agent finds the cheese reward. To simulate this fact, we will add a \"termination\" state, indicated as `None`. The transition function will automatically move the agent to a `None` state whatever action is taken after the cheese is found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "class Maze():\n",
    "    \"\"\"A maze environment\"\"\"\n",
    "\n",
    "    ACTIONS = (\"up\", \"down\", \"left\", \"right\") # List of actions\n",
    "    INITIAL_STATE = (0, 0) # Always starts at the topleft corner\n",
    "    \n",
    "    def __init__(self, fname = \"grid.txt\"):\n",
    "        \"\"\"Inits a maze by loading the grid file\"\"\"\n",
    "        self.grid = np.loadtxt(fname)\n",
    "        self.state = self.INITIAL_STATE\n",
    "        self.end = False\n",
    "\n",
    "\n",
    "    def state_transition(self, state1, action1):\n",
    "        \"Defines the next state gien the \"\n",
    "        x, y = state1\n",
    "        \n",
    "        # If we have reached the cheese, we transition \n",
    "        # to the terminal state\n",
    "        if self.grid[x, y] > 0:\n",
    "            return None\n",
    "        \n",
    "        # Otherwise, we update the position\n",
    "        state2 = copy(state1)\n",
    "        \n",
    "        if action1 in self.ACTIONS:\n",
    "            if action1 == \"up\":\n",
    "                if x > 0:\n",
    "                    state2 = (x - 1, y)\n",
    "            \n",
    "            elif action1 == \"left\":\n",
    "                if y > 0:\n",
    "                    state2 = (x, y - 1)\n",
    "            \n",
    "            elif action1 == \"down\":\n",
    "                if x < (self.grid.shape[0] - 1):\n",
    "                    state2 = (x + 1, y)\n",
    "\n",
    "            elif action1 == \"right\":\n",
    "                if y < (self.grid.shape[1] - 1):\n",
    "                    state2 = (x, y + 1)\n",
    "                    \n",
    "        return state2\n",
    "                    \n",
    "    \n",
    "    def reward_transition(self, state1, action1, state2):\n",
    "        \"\"\"Reward is -1 for bouncing against the walls, and whatever is on the grid otherwise\"\"\"\n",
    "        if state1 == state2:\n",
    "            return -1\n",
    "        elif state2 == None:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.grid[state2[0], state2[1]]\n",
    "        \n",
    "    \n",
    "    # Quick way to combine State transitions and Reward transitions \n",
    "    def transition(self, action1):\n",
    "        \"\"\"Changes the state following an action\"\"\"\n",
    "        state1 = self.state\n",
    "        state2 = self.state_transition(state1, action1)\n",
    "        reward2 = self.reward_transition(state1, action1, state2)\n",
    "        \n",
    "        self.state = state2\n",
    "        return (state2, reward2) # Returns s_t+1, r_t+1\n",
    "\n",
    "    \n",
    "    def print_state(self):\n",
    "        \"Prints a text representation of the maze (with the agent position)\"\n",
    "        bar = \"-\" * ( 4 * self.grid.shape[1] + 1)\n",
    "        for i in range(self.grid.shape[0]):\n",
    "            row = \"|\"\n",
    "            for j in range(self.grid.shape[1]):\n",
    "                cell = \" \"\n",
    "                if i == self.state[0] and j == self.state[1]:\n",
    "                    cell = \"*\"\n",
    "                row += (\" %s |\" % cell)\n",
    "            print(bar)\n",
    "            print(row)\n",
    "        print(bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the maze\n",
    "The maze is simple but functional. It is easy to create a maze, check the available actions, apply a few actions, and so on. Let's check that our environment works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "| * |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "State after illegal action: (0, 0)\n",
      "Reward after illegal action: -1\n",
      "State after legal action: (1, 0)\n",
      "Reward after legal action: 0.0\n"
     ]
    }
   ],
   "source": [
    "m = Maze()\n",
    "m.print_state()\n",
    "state_after_bad_action = m.state_transition(m.state, \"up\")  # Illigal action: bounces!\n",
    "state_after_good_action = m.state_transition(m.state, \"down\") # Legal action: Goes down\n",
    "\n",
    "print(\"State after illegal action: %s\" % (state_after_bad_action,))\n",
    "print(\"Reward after illegal action: %s\" % (m.reward_transition(m.state, \"up\", state_after_bad_action)))\n",
    "\n",
    "print(\"State after legal action: %s\" % (state_after_good_action,))\n",
    "print(\"Reward after legal action: %s\" % (m.reward_transition(m.state, \"down\", state_after_good_action)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, we can easily navigate in our virtual maze by executing the appropriate actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 0), 0.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.transition(\"down\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the ```Maze``` object returns two values at the end of each action execution, the new state $s_{t+1}$ and the associated reward $r_{t+1}$. If the ```down``` action was executed with the original maze layout of the ```grid.txt``` file, case, the two values are $s_{t+1} = $ ```(3, 0)``` and $r_{t+1} = $ ```0.0```. We can also execute more actions, and see what happens after a few movements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   |   |   |   |\n",
      "-----------------\n",
      "|   | * |   |   |\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "m.transition(\"down\")\n",
    "m.transition(\"down\")\n",
    "m.transition(\"right\")\n",
    "m.print_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a $V$-Agent\n",
    "\n",
    "Now we can create our own very fantastic agents! As an example, we will create a $V$-learning agent that interacts with the ``` Maze``` world.\n",
    "\n",
    "The agent contains two parts:\n",
    "\n",
    "1. The agent's _memory_ is composed of a single $V$-table, which contains  $4 \\times 4 = 16$ states, one for every possible position in the maze. \n",
    "2. The agent's _policy_ is a simple policy that selects actions _at random_. This guarantees that the agent will explore the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# -- NAMING CONVENTIONS --\n",
    "# s_{t}   = state1\n",
    "# a_{t}   = action1\n",
    "# s_{t+1} = state2\n",
    "# r_{t+1} = reward2\n",
    "# a_{t+1} = action2\n",
    "\n",
    "\n",
    "class VAgent():\n",
    "    \"\"\"An agent that keeps track of the value of states\"\"\"\n",
    "    def __init__(self, actions=Maze.ACTIONS, alpha=0.1, gamma=0.9):\n",
    "        \"\"\"Creates a V-agent\"\"\"\n",
    "        self.V = {}                # Initial dictionary of (s, a) pairs. At the beginning, it's emtpy.\n",
    "        self.alpha = alpha         # Learning rate\n",
    "        self.gamma = gamma         # Temporal discounting\n",
    "        self.actions = actions     # Set of possible actions (provide those of Maze.ACTIONS)\n",
    "\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"Random policy to explore the maze\"\"\"\n",
    "        return random.choice(self.actions)\n",
    "        \n",
    "    \n",
    "    def td_learning(self, state1, reward1, state2):\n",
    "        \"\"\"Updates the Q-values when given an (s,a) pair, the reward value and a new state\"\"\"\n",
    "        g = self.gamma\n",
    "        a = self.alpha\n",
    "        \n",
    "        v1 = 0.0\n",
    "        \n",
    "        if state1 in self.V.keys():\n",
    "            v1 = self.V[state1]\n",
    "        \n",
    "        v2 = 0.0\n",
    "        \n",
    "        if state2 in self.V.keys():\n",
    "            v2 = self.V[state2]\n",
    "        \n",
    "        rpe = reward1 + g * v2 - v1\n",
    "        v1 += a * rpe\n",
    "\n",
    "        self.V[state1] = v1\n",
    "\n",
    "            \n",
    "    def visualizeV(self, axes):\n",
    "        \"\"\"Visualizes the V table\"\"\"\n",
    "        # Create the corresponding state table\n",
    "        mat = np.zeros((4,4))\n",
    "        states = [x for x in self.V.keys()]\n",
    "            \n",
    "        for s in states:\n",
    "            row, col = s\n",
    "            mat[row, col] = self.V[s]\n",
    "            \n",
    "        # Show the Q-table as a heatmap\n",
    "        im=axes.matshow(mat, vmin=-1, vmax=10, cmap='viridis')\n",
    "        return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Between $V$-Agent and Environment\n",
    "\n",
    "We need to additional functions to make sure the agent and the environment interact with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(environment, agent):\n",
    "    \"\"\"A trial ends when the agent gets a reward. The history is returned\"\"\"\n",
    "    state1 = environment.state\n",
    "    reward1 = environment.grid[state1[0], state1[1]]\n",
    "    state2 = \"Start\"\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    while state2 != None:\n",
    "        action = agent.policy(state1)\n",
    "        state2, reward2 = environment.transition(action)\n",
    "        history.append(state2)\n",
    "        \n",
    "        # Update the V-values for state1\n",
    "        agent.td_learning(state1, reward1, state2)\n",
    "        \n",
    "        state1 = state2\n",
    "        reward1 = reward2\n",
    "\n",
    "    return history\n",
    "\n",
    "    \n",
    "def run_trials(environment, agent, n, collect=True):\n",
    "    \"\"\"Runs N trials\"\"\"\n",
    "    history = []\n",
    "    for j in range(n):\n",
    "        h = run_trial(environment, agent)\n",
    "        history += h\n",
    "        environment.state = Maze.INITIAL_STATE\n",
    "    \n",
    "    return history    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the agent\n",
    "\n",
    "Now, we can finally test the agent. Note that the cell for the \"reward\" state is (where the \"food\" is) remains at zero; this is because the trial actually ends when the agent arrives in the final cell. Because TD-learning methods propagate back in time, another state (\"End\", for example) would be needed to actually assign the correct value to that cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = Maze()\n",
    "a = VAgent(alpha=0.1)\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(7,3))\n",
    "\n",
    "# Because the agent moves at random, the following instruction can take a variable amount of time to complete\n",
    "run_trials(m, a, 1)  \n",
    "a.visualizeV(axs[0])\n",
    "axs[0].set_title(r\"$V$-Table ($t$=1)\")\n",
    "\n",
    "run_trials(m, a, 10)\n",
    "a.visualizeV(axs[1])\n",
    "axs[1].set_title(r\"$V$-Table ($t$=10)\")\n",
    "\n",
    "run_trials(m, a, 1000)\n",
    "im = a.visualizeV(axs[2])\n",
    "axs[2].set_title(r\"$V$-Table ($t$=1000)\")\n",
    "\n",
    "#fig.colorbar(im, cax=cbar_ax)\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.88, 0.2, 0.03, 0.6])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "plt.savefig(\"vtable_maze.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a $Q$-Agent\n",
    "Now we will create a $Q$-learning agent that interacts with the ``` Maze``` world. The $Q$-table should contain $16 \\times 4 = 64$ different state-action pairs; rather than filling it up right away, we will fill it up as we encounter new state-action pairs, assuming that any previously unencountered state has a value of $Q = 0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class QAgent():\n",
    "    def __init__(self, actions=Maze.ACTIONS, epsilon=0.1, alpha=0.1, gamma=0.9):\n",
    "        \"\"\"Creates a Q-agent\"\"\"\n",
    "        self.Q = {}    ## Initial dictionary of (s, a) pairs. At the beginning, it's emtpy.\n",
    "\n",
    "        self.epsilon = epsilon     # Epsilon for e-greey policy\n",
    "        self.alpha = alpha         # Learning rate\n",
    "        self.gamma = gamma         # Temporal discounting\n",
    "        self.actions = actions     # Set of possible actions (provide those of Maze.ACTIONS)\n",
    "\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"Selects an action with a epsilon-greedy policy\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(self.actions)\n",
    "        else:\n",
    "            q = [self.Q[(state, a)] if (state, a) in self.Q.keys() else 0.0 for a in self.actions]\n",
    "            maxQ = max(q)\n",
    "            count = q.count(maxQ)\n",
    "            if count > 1:\n",
    "                best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "                i = random.choice(best)\n",
    "            else:\n",
    "                i = q.index(maxQ)\n",
    "\n",
    "            action = self.actions[i]\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def q_learning(self, state1, action1, reward1, state2):\n",
    "        \"\"\"Updates the Q-values when given an (s,a) pair, the reward value and a new state\"\"\"\n",
    "        g = self.gamma\n",
    "        a = self.alpha\n",
    "        \n",
    "        q1 = 0.0\n",
    "        \n",
    "        if (state1, action1) in self.Q.keys():\n",
    "            q1 = self.Q[(state1, action1)]\n",
    "        \n",
    "        max_q2 = max([self.Q[(state2, a)] if (state2, a) in self.Q.keys() else 0.0 for a in self.actions])\n",
    "        \n",
    "        rpe = reward1 + g * max_q2 - q1\n",
    "        #print(\"s1 = %s, a1 = %s, s2 = %s, q1 = %.4f, r2 = %.4f, q2 = %.4f\" % (state1, action1, state2, q1, reward2, max_q2))\n",
    "        q1 += a * rpe\n",
    "        #print(\"new q1 = %.4f\" % (q1,))\n",
    "        self.Q[(state1, action1)] = q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions between $Q$-Agent and Environment\n",
    "\n",
    "For a $Q$-Agent to work properly, we need to redefine the `run_trial` function to handle to more complex expression that is needed for $Q$-learning and SARSA.  Because the agent actually _learns_ how to navigate the maze, we can also visualize the agent's preferred paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(environment, agent):\n",
    "    \"\"\"A trial ends when the agent gets a reward. The history is returned\"\"\"\n",
    "    state1  = environment.state\n",
    "    reward1 = environment.grid[state1[0], state1[1]]\n",
    "    \n",
    "    action1 = agent.policy(state1)\n",
    "    \n",
    "    history = []\n",
    "    state2  = \"\"\n",
    "    \n",
    "    while state2 != None:\n",
    "        # See the next state \n",
    "        state2, reward2 = environment.transition(action1)\n",
    "        \n",
    "        \n",
    "        # Save the states visited\n",
    "        history.append(state1)\n",
    "        \n",
    "        # Update the Q-values for state1, action1\n",
    "        agent.q_learning(state1, action1, reward1, state2)\n",
    "        \n",
    "        # Decide the next action and complete the loop\n",
    "        action2 = agent.policy(state2)\n",
    "        \n",
    "        state1 = state2\n",
    "        reward1 = reward2\n",
    "        action1 = action2\n",
    "        \n",
    "    return history\n",
    "\n",
    "\n",
    "def visualizeQ(agent):\n",
    "    \"\"\"Visualizes the Q tables, one per action\"\"\"\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(5,5))\n",
    "    i = 0\n",
    "    for a in agent.actions:\n",
    "        # Create the corresponding state table\n",
    "        mat = np.zeros((4,4))\n",
    "        states = [x for x in agent.Q.keys() if x[1] == a]\n",
    "            \n",
    "        for s in states:\n",
    "            x, y = s[0]\n",
    "            mat[x, y] = agent.Q[s]\n",
    "            \n",
    "        # Show the Q-table as a heatmap\n",
    "        im = axs.flat[i].matshow(mat, interpolation = \"none\", vmin=-1, vmax=10, \n",
    "                                 cmap='viridis')\n",
    "        axs.flat[i].grid(color='w', linestyle='-', linewidth=2, which=\"minor\")\n",
    "        axs.flat[i].set_title(r\"$Q$ values for '%s'\" % a.upper(), pad=20)\n",
    "        axs.flat[i].set_frame_on(False)\n",
    "        i += 1\n",
    "        \n",
    "    fig.subplots_adjust(right=0.85, hspace=0.5)\n",
    "    cbar_ax = fig.add_axes([0.88, 0.2, 0.03, 0.6])\n",
    "    fig.colorbar(im, cax=cbar_ax)\n",
    "    #fig.tight_layout()\n",
    "            \n",
    "\n",
    "def visualize_history(history):\n",
    "    \"Visualizes the number of times each cell in the Maze has been transversed\"\n",
    "    mat = np.zeros((4,4))            \n",
    "    for s in history:\n",
    "        row, col = s\n",
    "        mat[row, col] += 1\n",
    "    \n",
    "    mat /= np.sum(mat)\n",
    "    mat *= 5  # min number of moves\n",
    "    \n",
    "    plt.matshow(mat, interpolation = \"none\", vmin=0, vmax=1, \n",
    "                cmap='inferno')\n",
    "    \n",
    "    plt.grid(color='w', linestyle='-', linewidth=1, which=\"minor\")\n",
    "    plt.title(\"Preferred path\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the agent\n",
    "\n",
    "Here a few tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUUAAAFDCAYAAACtLRxAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deZhddZ3n8feHENYEooKapZqgYNoII9gxorhggDGgCPTTIghuo0ZbUVBmFJ0eV8aHtm2mwW0mCsIMm2xNI7JIQxBRBAIGzAJtWKQCwbAYSSACqfrOH+cUObmpqnvurXPPOffW5/U856m7nHO+v3vrW9/6ne13FBGYmVliq6obYGZWJy6KZmYZLopmZhkuimZmGS6KZmYZLopmZhkuimZmGS6KZmYZPVsUJT0o6aCSY86StETSOkmfKTO2jcy5YK0ovShK2lHSKZLuSxNmuaSPl92ODvk8sCgiJkfEGWNdWfrHPDN9HJL2aHj/q5LObZh/g6T1kv4o6WxJkxrXVRfOhfwacmHo97xO0lpJv5b0CUlbNSzzIUm/k/SMpEcl/UDSlMz7X5R0dcMyvx/htaMzsddI2jHz/kcl3ThcW7tRqUVR0ouAm4HdgQOBnYCPAd+Q9JEy29IhuwHLWl1I0tYFtuGwiJgEvA6YA/xDgesujHNheC3kwmERMTmNcyrwBeDMzHpOAv4R+G/AzsB+6bzXSdomne0m4E2SJqTLTAUmAvs2vLZHOu+QCcAJrX62blF2T/F04I/AcRHxYCR+BZwGfKpxZklfkHRJw2unSzojfXxyQy/jyJECN/a00l7UKenjaZIulfSYpAcaN3fSdjycxrlX0oHDrP8G4O3Ad9Oe2qskvVrSjel/82WS3p2Z/8F0vXcDTxdcGImIh4Grgb2KXG+BnAub5m87FyLizxFxBfBe4IOS9pK0E/A14NMRcU1EPB8RDwJHATOB49LFbycpgvukz98CLALubXjtvoh4JBP2n4D/mu119pSIKGUi+S81ALxumPfeAzw5wjLPAJPT5xOA1cB+meWmkRT39wJPA1PT9x4EDsqsK4A9Ms/PBk5Jl70D+DKwDfAK4H7gHel8s4B+YFr6fCbwyhE+443AR9PHE4GVwJfS9c4D1gGzMu1bAvQB2+f4/jZrf/raV4FzM89f+MzpepcB3yjrd+xcKC0XNvs8mdcfAv4emA9sBLYeZp5zgAsyzxcBn00ffxf4L8D/bHjtrMbYwGXAKelrHwVurDqviprK7CkeBPRHxJ3DvDcdWNX4YkT8AbgTGPqvPw94JiJ+k75/cUQ8EhGDEfET4PfA3Bbb9Xpg14j4ekQ8FxH3Az8Ejk7fHwC2BWZLmhhJr+a+HOvdD5gEnJqu9wbgSuCYzDxnRER/RGxosc2juVzSWpJN018A3yxw3UVxLnQmFx4BXgzsAjweERuHmWd1+v6QXwBvTR+/BfhlOmVf+8Uw6/ky8GlJu46hvbVUZlHclWGSPXUEcMMI753PpuR5X/ocAEkfUHKEb21aCPZi8194HrsB04bWka7nS8DLACJiJXAiSa9sjaQLJU3Lsd5pJH/4g5nX/kDyRz+kv4V2DpD0OLImAs83vHZEREyJiN0i4pMFF9yiOBfGlgsjmQ48CTwO7DLCZvjU9P0hNwFvlvRikn8Ivwd+TbKv8cUk3+NNjSuJiKUkhf3kAtpdK2UWxQeA3bTlEbKDSf5D//MIy10MHCBpBkkv4fx0ud1I/osfD7wkIqYASwGNsJ5ngB0yz1+e/uwHHkgLydA0OSIOHZoxIs6PiDeT/NEEyQ7sZh4B+ho+718BD2eetzKY5UMkm2tZu5P8cXUb58LYcmELkl5PUhRvBm4BngX+tmGeScAhwPWZl28hORDzMeBXABHxVNrmjwGPRMQDI4T9SjrP9BHe70plFsWfpT9PkbSDpG0lHQdcALwnIob9TxkRj5Hsn/kxScKuSN/akSSRHgOQ9GFGP6iwBHifpAmS5gNvS1+/DViX7ujePn1/rzTJhs43mydpW+AvwAZgcNgIm7uV5I/v85ImSjoAOAy4MMeyw/kJ8A+SZkjaSsl5d4cBlzRZro6cC2PLhRdI2knSu9J1nRsRv4uIP5McaPmOpPlpzJnARSQ99P83tHy6JbEY+BzJZvOQm9PXtuglZpZdSZKXPXUeZmlFMSLWk5x6sTfJztoNJF/62yLiqiaLn0+yH+qFzaWIWE7So7iF5Cjm3qT/6UZwAkkirgWOBS5P1zMAvIvkaNsDJJsWPyL57wnJPqRT09cfBV4KfDHH530ujXdIuuz3gQ9ExD3Nlh3B10k2a24G/gR8Czg23YzpKs6FMecCwE8lrSPp3f53kqP2H87E/BbJpv+3gadICnM/cGBEPNuwrl+kn+XmzGu/TF8bsSimvk7yT6lnKKKa2xFIeg9wBvCaiHiykkZYLTgXrE4KPTeuFRFxsaRdGGFHro0fzgWrk8p6imZmddSzA0KYmbXDRdHMLMNF0cwsw0XRzCzDRdHMLKMWRTE96/5eSSsldexaSklnKRkgs+MnPEvqk7QoHcZqmaSOjT8naTtJt0m6K431tU7FqpOy8iaNVUrulJk3abxxmTujqmJonuxEMgTUfSTDNG0D3AXM7lCst5IMvrq0hM81lXRoLGAy8B8d/FwCJqWPJ5JcvbBf1b/bXsmbMnOnzLwZr7nTbKpDT3EusDIi7o/kcqgLgcM7ESgibiIZRaTjImJ1pENjRcQ6YAUdunA+EuvTpxPTqddPQC0tb6C83Ckzb9IY4zF3RlWHojidzYdNWkWPjbqRXoy/L8l/4U7FmCBpCbAGuC4iOharJpw3xcUZb7kzqjoUxZ6WDtd0KXBiJEMydUREDETEPsAMYK6kut6GwHIoK2/AudOoDkXxYZJh2IfMYPNx5rqWpIkkiX1eRFxWRsyIWEsyxPz8MuJVyHlTsHGUO6OqQ1G8HdhT0u5K7jJ2NHBFxW0aM0kiubvaiog4rcOxdlV6EyFJ2wMHA2MZlqobOG+KiTcec2dUlRfFSO4jcTxwLclO5YsiouVbQ+Yh6QKSMfdmSVqlzt5Kc3/g/cA8JcPkL5F0aLOF2jQVWKTkbnC3k+wXurJDsWqhzLyBUnOnzLyBcZg7zXiUHDOzjMp7imZmdeKiaGaW4aJoZpbhomhmluGiaGaWUZuiKGlBL8YqO17Zn60OevX77dVYdVebogiU+UspOwF6+bPVQa9+v70aq9bqVBTNzCrXkZO3dzvn1JZXuv7G25h0wNyWY02asqHlZf507R286B1/0/Jy03f+c8vLAKz66e+YcdjeLS+31VF/aXmZ/g3L6dt+dsvLXbPmf6vlhQo266v/q61kXHvHLUz5mze2tMz2c55oJxRPXHMnL5n/upaWOe4Vt7cV6zcXrWK/o2a0vNz1beRa/1N307fTf2p5uavv+3bleVO02vQU2ymI7WqnII5FOwWxXe0UxG7XakEci1YL4li0UxDb1U5B7FW1KYpmZnXgomhmluGiaGaW4aJoZpbhomhmluGiaGaW4aJoZpbhomhmlpGrKEqaL+leSSslndzpRllvcN5YN2paFCVNAL4HHALMBo6RNP4um7CWOG+sW+XpKc4FVkbE/RHxHHAhcHhnm2U9wHljXSlPUZwO9Geer0pfMxuN88a6UmEHWiQtkLRY0uL1N95W1Gqtx2XzZu0dt1TdHLNcRfFhoC/zfEb62mYiYmFEzImIOWWOeGO11XLelDnajdlI8hTF24E9Je0uaRvgaOCKzjbLeoDzxrrS1s1miIiNko4HrgUmAGdFxLKOt8y6mvPGulXToggQEVcBV3W4LdZjnDfWjXxFi5lZhouimVmGi6KZWYaLoplZhouimVmGi6KZWYaLopl1PUlnSVojaWnmtRdLuk7S79OfL8qzLhdFM+sFZwPzG147Gbg+IvYErk+fN5Xr5O1Wvfrk/uYzFeTZvfqaz1SQKV9/tLRYAHd+p7zPVgevOOsPpcU68KgVpcV6ww4rS4sFcP1fXlVqvDqIiJskzWx4+XDggPTxOcCNwBearcs9RTPrVS+LiNXp40eBl+VZqCM9RTOzZt7x9h3jiScHcs17x93PLgP+knlpYUQszBsrIkJS5JnXRdHMKvH4kwPceu2MXPNOnHrfXyJiTosh/ihpakSsljQVWJNnIW8+m1lFgoEYzDW16Qrgg+njDwL/lmch9xTNrBIBDJJri7YpSReQHFTZRdIq4CvAqcBFkj4C/AE4Ks+6XBTNrDKDtN0L3ExEHDPCWwe2ui4XRTOrRBA83/6mcce4KJpZJQIYKGjzuUguimZWmaL2KRap6dHn4a4pNMvDuWOjCWAgItdUpjyn5JzNltcUmuVxNs4dG8VgzqlMee7mN9w1hWZNOXdsNBHBcyX3AvPwPkUzq0RynmL9FHZFi6QFkhZLWtz/jG/va/lsljfrf1d1c6xUYiDnVKbCimJELIyIORExp2+H1xS1Wutxm+XNpL2rbo6VKIDByDeVyZvPZlaZsnuBeeQ5JecC4BZglqRV6XWEZk05d2w0ATwfW+WaypTn6PNI1xSajcq5Y6NJrmipX0/Rm89mVolADNRw9EIXRTOrzGC4p2hmBnjz2cysgRgo+SBKHi6KZlaJAJ5nQtXN2IKLoplVIsI9RTOzzQx6n6KZWSI50DJOeoraqrwP+sxLJ5YW68Sp15UWC+Abxx5ZXrD3lheqDq7+xFtLi/W5i+4vLRYAE+q3n2543nw2M3tBcplf/Qq4i6KZVcJXtJiZNRj05rOZWWJcHWgxM2smEAO+9tnMbJNB9xTNzBIR8tFnM7MhAT5P0cwsq44HWvLco6VP0iJJyyUtk3RCGQ2z7ua8sWYCMRj5pjwkfTbNtaWSLpC0XTvtylOmNwInRcRsYD/gU5JmtxPMxhXnjTU1wFa5pmYkTQc+A8yJiL2ACcDR7bQpz42rVgOr08frJK0ApgPL2wlo44PzxpoJCj/QsjWwvaTngR2AR9pdSW6SZgL7Are2E8zGJ+eNDSco7oqWiHhY0reBh4ANwM8j4uftrCt3iyRNAi4FToyIp4Z5f4GkxZIW9z+9tJ22WA9qKW/W/678BlqlBlCuCdhlKE/SaUF2PZJeBBwO7A5MA3aUdFw7bcpVFCVNJEns8yLisuHmiYiFETEnIub07bhXO22xHtNy3kzau9wGWqUixGBslWsCHh/Kk3Ra2LC6g4AHIuKxiHgeuAx4Uzvtarr5LEnAmcCKiDitnSA2/jhvLI8Cz1N8CNhP0g4km88HAovbWVGeFu0PvB+YJ2lJOh3aTjAbV5w3NqqhAy15pqbrirgVuAS4E/gdSW1r7E3mkufo881QwxspWK05b6yZ5EBLcSkSEV8BvjLW9fiKFjOrTB2vaHFRNLNKDF3RUjcuimZWGQ8dZmaWisCDzJqZDQnExkGPp2hm9oKBGp6g4KJoZpUo+pScorgomllF5FucmpllDXrzuXhr3lBerG/MO7K8YOPRwEBpoba579HSYr3zTe8uLRbQNX/VEfC8D7SYmSV88raZWQNvPpuZpXz02cysgY8+m5kNaeH2pWVyUTSzSgSw0T1FM7OE9ymamTXoyqIoaTvgJmDbdP5L0mG/zUbkvLFmuvk8xWeBeRGxPr1l5c2Sro6I33S4bdbdnDfWVFeepxgRAaxPn05Mp+hko6z7OW+smQjYOFi/Ay25WiRpgqQlwBrguvR2gmajct5YM4PpaTnNpjLlKooRMRAR+wAzgLmS9mqcR9ICSYslLe5/emnR7bQu5Lyx0QztU+zKojgkItYCi4D5w7y3MCLmRMScvh23yH0bx5w3NpII5ZrK1LQoStpV0pT08fbAwcA9nW6YdTfnjeUxiHJNZcpz9HkqcI6kCSRF9KKIuLKzzbIe4LyxUUXAQA0PtOQ5+nw3sG8JbbEe4ryx5rr3PEUzs44oe39hHi6KZlYJX/tsZpYVyX7FuqnfXk4zGzeKPPosaYqkSyTdI2mFpDe20yb3FM2sEoGKPvp8OnBNRPydpG2AHdpZiYuimVWmqM1nSTsDbwU+lKw3ngOea2dd3nw2s8oUeEXL7sBjwI8l/VbSjyTt2E6bXBTNrBIRLRXFXYaukU+nBQ2r2xp4HfCDiNgXeBo4uZ12efPZzCrTwik5j0fEnFHeXwWsyozEdAl1KopXP/ydlk8+krQgIhZ2oj2FxfpMyfFqHqto7eQN9O7326uxsgYHizlPMSIeldQvaVZE3AscCCxvZ1112nxu7A73Sqyy45X92eqgV7/fXo0FJEefCx4l59PAeZLuBvYBvtlOu7z5bGaVKfLc7YhYAoy2iZ2Li6KZVSN87XMzZe7PKHvfSS9/tjro1e+3V2NtUsPL/BR1vPjQzHredq+cHn2n/n2ueVce9T/uaHL0uTB16ima2TgSFHf0uUguimZWjQC8T9HMbJM67r1zUTSz6rgompkNKf/2pXm4KJpZNQLCB1rMzDK8+WxmluWeopnZJu4pmplluCiamaV8oMXMrIF7imZmGTU8T7FOI2+3RdKDkg4qOeYsSUskrZPU5k0KrEzOk3pS5JvK1LGiKGlHSadIui9NiuWSPt6peCX7PLAoIiZHxBljXVn6Bzsz83jYP970vQ2S1mem747y3rSG54MN8xzbGL9szpP8hsmTod/lo5LOljRpmPkPyjw/WtKtkp6WtCZ9/ElJGm7+9LUPSbo5fVxsLkULU4k6UhQlvQi4meRerAcCOwEfA74h6SOdiFmy3YBlrS4kqYjdFYdFxKTMdPwo7z2SfQ481DDPeQW0p23Ok+G1kCeHpb/XfYB9gS+Oss6TgNOBfwJeDrwM+ASwP7BNnmDF55KSzec8U4k61VM8HfgjcFxEPBiJXwGnAZ9qnFnSFyRd0vDa6ZLOSB+f3NCTOHKkwJJC0h6Z52dLOiV9PE3SpZIek/RA4yZN2o6H0zj3SjpwmPXfALwd+G76H/JVkl4t6UZJayUtk/TuzPwPpuu9G3i6oMLYK5wnm+ZvO08i4lHgWpLiONxn3Rn4OvDJiLgkItal3/VvI+LYiHg2b6zCDeacSlR4UZS0G3As8KXYcljv+4CZwyx2IXCopMnpOiYARwHnZ5Z7C7Az8DXgXElTW2zXVsBPgbuA6SQ9kxMlvSN9fxZwPPD6iJgMvAN4sHE9ETEP+CVwfPof84F0vT8HXsqmO4rNyix2DPBOYEpEbBxmnTMjYotYZakivvOkuDyRNAM4BFg5wsd6I7At8G9NPv6YtZxL42Tz+SCgPyLuHOa96SQ3rd5MRPwBuBMY+s8+D3gmIn6Tvn9xuik4GBE/AX4PzG2xXa8Hdo2Ir0fEcxFxP/BD4Oj0/QGSxJktaWLac7kvx3r3AyYBp6brvQG4kiTBh5wREf0RsaHFNg/n8rSnMTR9bIT3Li8gVic5T8aeJ5dLWgf0A2uAr4ww3y4kN5N/odBK+nWaJxskvbVhnS/kF/D9nG1p3dAgs+Ng83lXhkno1BHADSO8dz6bEuR9bPrvj6QPKDmKN/SL2ovkF92K3YBpDb/wL5HsWyEiVgInAl8F1ki6UNK0HOudRvLHne3k/4HkD3tIf4ttHc0RETElM/1whPeOKDBmJzhPxp4nR6S91QOAv2bkz/oEsEt2kzwi3hQRU9L3snVgs/wCPtlim1oyXo4+PwDslm6GvEDSwST/hf95hOUuBg5INwWOJE32dDPrhySbLC9Jf1FLGflK8meAHTLPX57+7AceaCgokyPi0KEZI+L8iHgzyR9GAP+Y4/M+AvQ1fN6/Ah7OPK/hKaqVc54UlCcR8QvgbODbI8xyC/AscHg76++ocbL5/LP05ymSdpC0raTjgAuA90TEsP8NI+Ix4EbgxyRJuSJ9a0eSr+UxAEkfJukBjGQJ8D5JEyTNB96Wvn4bsC7dmb19+v5ekl6frneWpHmStgX+Amwg3y7eW0n+wD4vaaKkA4DDSPZ/tWuipO0yUy8enHGejD1Psv4FOFjSaxvfiIi1JPtYvy/p7yRNlrSVpH1IvrfKjIueYkSsJ9k5vTfJDugNwOeAt0XEVU0WP59kX9MLm0QRsZyk13ALyZHKvYFfjbKOE0iSbS3JjvzL0/UMAO8iOUL3APA48COSnfKQ7Cc6NX39UZKd4SOe4pBp33NpvEPSZb8PfCAi7mm27CiuIvnehqavZt77qTY/X+xfxxCnMs6TQvIku/7HgP8LfHmE979F8v1+nuT7+SPwf4AvAL8uog1tqeE+xY7f91nSe4AzgNdExJMdDWZdy3ky/mzb1xfTT/psrnkf+OxJvXPf54i4WNIuJJsyN3U6nnUn58k4VcO97aXsq4qIH5QRx7qb82T8KXt/YR69uAPfzLqFi6KZWUIBKvkSvjy6fugwM+tiBR99Tk+h+q2kK9ttkouimVWn+JO3TwBWNJ1rFC6KZlaZIk/eTq9yeifJeaVtq0VRlDRfyRBMKyWd3ME4ZykZXHNpp2JkYvVJWqRkCKtlkk7oYKztJN0m6a401tc6FatOysqbNFYpuVNm3qTxqs2dYnuK/0JycvqY9lRWXhSVDP/0PZIz/WcDx0ia3aFwZwPzO7TuRhuBkyJiNskIKZ/q4Od6FpgXEa8luRJjvqT9OhSrFkrOGygvd8rMG6gyd9IDLXkmkgEtFmemBdlVSXoXsCYi7hhrs+pw9HkusDIdoglJF5JcuL686EARcZNKGnY/IlYDq9PH6yStIBkRpROfK4D16dOJ6VTDkx0KVVreQHm5U2bepDGqzZ38kR5vckXL/sC7JR0KbAfsJOnciDiu1SZV3lMk+YVnL/5fxebDKXW99I9pX5JBAToVY4KkJSTj6l0XER2LVRPOm+LiVJY7Re1TjIgvRsSMiJhJMvblDe0URKhHUexpSm4mdClwYkQ81ak4ETEQEfsAM4C5kkYbIcZqrqy8AedOozoUxYeBvszzGWw+xlzXkjSRJLHPi4jLyoiZDhO1iPL2nVbFeVOwSnKnA+MpRsSNEfGudptUh6J4O7CnpN0lbUPS9b2i4jaNmSQBZwIrIuK0DsfaVdKU9PH2wMFAIUNS1Zjzpph41eVOawdaSlN5UUzvG3E8yd3IVgAXRUTLt4XMQ9IFJOPtzZK0Sp29jeb+wPuBeUqGyF+S7gTuhKnAIiV3grudZL9Q22f0d4My8wZKzZ0y8waqzp0ajrzd8fEUzcyGs/20vpj50c/lmveeb3yud8ZTNDMbUQ37ZC6KZlaNCu6/koeLoplVx0XRzGyTOo6n6KJoZtWpYU+x8lNyhjRe4N0rscqOV/Znq4Ne/X57NdYL8p6O0+33fR6DMn8pZSdAL3+2OujV77dXY72gyPEUi+LNZzOrTg03nzty8vbu53+z5ZWuu+E2Js+b23Is5b99wwueuv42djqw9VjbbPt868GAtT9fzJT/3Pp5p3OmP9TyMr+//B72POKvW17u3Dec2cY3Waw3X/f5tpLx0Z/dxcvf+dqWlnnlzo+3E4r/+Nd7edWRs1pa5lcP7t5WrKf+fTE7HdR63uxxwqMtL9P/zDL6dnhNy8tds/p7befN9i/viz2OzXfy9tLTxuHJ2+0UxHa1UxDHop2C2K52CmK3a7UgjkWrBXEs2imI7WqnII5ZBfsL86hNUTSz8UXpVDcuimZWHfcUzcw28WV+ZmZZLopmZqnwZX5mZptzT9HMbBPvUzQzy6phUcx17bOk+ZLulbRS0smdbpT1BueNNVPHa5+bFkVJE4DvAYcAs4FjJM3udMOsuzlvrKkABnNOJcrTU5wLrIyI+yPiOeBC4PDONst6gPPGRiW6tKcITAf6M89Xpa+ZjcZ5Y8318niKkhZIWixp8bobbitqtdbjsnnz6M/uqro5VjJF5JrKlKcoPgz0ZZ7PSF/bTEQsjIg5ETGnzBFvrLZazpsyR7uxGujikbdvB/aUtLukbYCjgSs62yzrAc4ba0qD+aYyNT1PMSI2SjoeuBaYAJwVEcs63jLras4by6NrT96OiKuAqzrcFusxzhtrqoZFsU43rjKz8STn6Th5epOS+iQtkrRc0jJJJ7TbLF/mZ2bVKa6nuBE4KSLulDQZuEPSdRGxvNUVuSiaWSWGTt4uQkSsBlanj9dJWkFyXqyLopl1Dw0Wv1NR0kxgX+DWdpZ3UTSzarR2DuIukhZnni+MiIWNM0maBFwKnBgRT7XTLBdFM6tMC+cgPt7svs+SJpIUxPMi4rJ22+SiaGbVKWjrWZKAM4EVEXHaWNblU3LMrDIFjpKzP/B+YJ6kJel0aDtt6khPcdY3n+7Eaoe3caC0UH86vbRQADzxtzuUF2yLq5LLt/7ZbUqL9ciJu5cWa+sDJ5UWC4CXTCk3XruiuAMtEXEzyQHtMfPms5lVp4ZXtLgomlklijxPsUguimZWjYhkqhkXRTOrjHuKZmZZLopmZqkADdSvKroomll16lcTXRTNrDrep2hmllXDo89NL/OTdJakNZKWltEg6x3OHWumwMv8CpPn2uezgfkdbof1prNx7tgIlF7ml2cqU9OiGBE3AU+W0BbrMc4da2ow51Qi71M0s8qoG/cp5iVpgaTFkhb3P/nbolZrPS6bN09cc2fVzbEyRQtTiQorihGxMCLmRMScvhfvW9Rqrcdl8+Yl819XdXOsVLHp+udmU4m8+WxmlSn7IEoeeU7JuQC4BZglaZWkj3S+WdYLnDs2qkju0ZJnKlPTnmJEHFNGQ6z3OHesqRoeaPHms5lVp3410UXRzKpTx1NyXBTNrDouimZmCUV4PEUzs824p2hmluGiaGaWCkof7CEPF0Uzq8y4OfqsPz3VidUOb+vy6vr5sy8sLRbA8du+r9R4VZv68T+XFutLv7qqtFizJm4oLRbAB885qtR47QsYrF9X0T1FM6tG4H2KZmabqV9H0UXRzKpTx32KhY2naGbWsgLHU5Q0X9K9klZKOrndJrkomlk1AhiMfFMTkiYA3wMOAWYDx0ia3U6zXBTNrCLp0ec8U3NzgZURcX9EPAdcCBzeTqtcFM2sOsVtPk8H+jPPV6WvtcwHWsysGkObz/nsImlx5vnCiFhYfKNcFM2sMgGR+5ycxyNizijvPwz0ZZ7PSF9rWZ57tPRJWiRpuaRlkk5oJ5CNL84by2xoFuEAAANASURBVKW4zefbgT0l7S5pG+Bo4Ip2mpSnp7gROCki7pQ0GbhD0nURsbydgDZuOG9sdAEMFHP2dkRslHQ8cC0wATgrIpa1s648N65aDaxOH6+TtIJkB6aT20bkvLFcCjx5OyKuAsZ8UXtL+xQlzQT2BW4da2AbP5w3Nrzyb3SfR+5TciRNAi4FToyILYbBkbRA0mJJi/ufXlpkG62LOW9sREGR5ykWJldRlDSRJLHPi4jLhpsnIhZGxJyImNO3415FttG6lPPGmirwMr+iNN18liTgTGBFRJzW+SZZL3DeWHNR2IGWIuXpKe4PvB+YJ2lJOh3a4XZZ93Pe2OgCIgZzTWXKc/T5ZkAltMV6iPPGcsl/RUtpfEWLmVWnhkefXRTNrBrhe7SYmW3OPUUzsyFBDAxU3YgtuCiaWTVaGzqsNC6KZladkk+3ycNF0cwqEUC4p2hmloqWBpktjYuimVXGB1o6YePG0kId/9ZjSosFjLvrQeJFO5UW65v7l3jF4YQJ5cUC2LrkeG1ax5+u/fe4ZJecsz/e0cZkdH9RNLOuFBHzq27DcHyLUzOzDBdFM7MMF0UzswwXRTOzDBdFM7MMF0UzswwXRTOzjKZFUdJ2km6TdJekZZK+VkbDrLs5b6xb5Tl5+1lgXkSsT29ZebOkqyPiNx1um3U35411pTw3rgpgffp0YjrVb2gLqxXnjXWrXPsUJU2QtARYA1wXEbd2tlnWC5w31o1yFcWIGIiIfYAZwFxJezXOI2mBpMWSFvc/vbTodloXajlvnvxt+Y00a9DS0eeIWAssAra4kDsiFkbEnIiY07fjFrlv41juvHnxvuU3zqxBnqPPu0qakj7eHjgYuKfTDbPu5ryxbpXn6PNU4BxJE0iK6EURcWVnm2U9wHljXSnP0ee7AW/XWEucN9atfEWLmVmGi6KZWYaLoplZhouimVmGi6KZWYaLoplZhouimVmGi6KZWYaLoplZhpJh76onaUFELOy1WGXHK/uz1UGvfr+9Gqvu6tRTXNCjscqOV/Znq4Ne/X57NVat1akomplVzkXRzCyjTkWxzP0ZZe876eXPVge9+v32aqxaq82BFjOzOqhTT9HMrHL/H46nmDTnNfuZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = Maze()\n",
    "a = QAgent(epsilon = 0.1)\n",
    "run_trials(m, a, 1000)\n",
    "visualizeQ(a)\n",
    "plt.savefig(\"qtable.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAEOCAYAAACpc6r8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAMhklEQVR4nO3cf4zkdX3H8efrfgAnHOWHSJDjvKItCTUt1ovY0EZKtb0CtSbtH5JoMdFeSaxCQiKSNimNkrZJq9hAf5yAtMFKKWrTXrUtBgyxFZRfNsBpQYQKPX4pBCgWBd/9Y75Hlwt3OywzO7v7fj6SSWZ2PvPd9/eW5853vzNDqgpJvaya9QCSFp/hSw0ZvtSQ4UsNGb7UkOFLDRn+MpTkmCS3Jnkiyftn8P03Jakkaxbp+1WS1yzG9+rC8BdJknuSfC/Jk0keTHJZkgMWuLkPANdW1fqq+tNJzjlrSb6Y5D2znmOlM/zF9StVdQDw08Bm4Hd3XzDms+irgNsXMsALbX+xnrm1dBj+DFTV/cDngdfCc4ey701yJ3Dn8LVTh8P5x5L8e5KfHL5+DfDzwIXD0cOPJ9k3yR8n+a/haOIvkqwb1p+Y5L4k5yR5APhEkvOSXJXk8iSPA+9K8iNJLkmyM8n9ST6cZPWwjdXD9h9Jcjdwyt72bzi6OTfJHUkeTfKJJPsN9x2cZHuSh4f7tifZMNx3PvBzc/btwjmbfXOSO4d/j4uSZEI/jp6qyssiXIB7gDcP149i9Iz9oeF2AVcDhwDrgNcBDwHHA6uB04fH7zus/yLwnjnb/ijwD8Pj1wP/CPzBcN+JwDPAHwH7Dts/D/gB8DZGv/zXAZ8F/hLYH3gF8BXgt4ZtnAF8fZj7EODaYeY1e9nX2+as/zfgw8N9hwK/BrxsmPXvgL+f89jn7ducf5/twEHARuBhYMusf6bL+TLzAbpchhieBB4D7gX+DFg33FfASXPW/vmuXwpzvvYN4E3D9efiAAL8D/DqOWt/BvjWcP1E4PvAfnPuPw+4bs7tw4Gnd80zfO00RucRAK4Bzphz3y+OEf7c9ScD39zD2uOAR+fc3lP4Pzvn9pXAB2f9M13OF/+2W1xvq6ov7OG+b8+5/irg9CTvm/O1fYBXvsDjDmP07HnTnKPfMDpS2OXhqvrfeb7fWmDnnG2smrPmlbutv3cP+7Cn7d+7a/YkL2N0hLIFOHi4f32S1VX17F6298Cc608BCz0xKjD8JWTuxyS/DZxfVeeP8bhHgO8BP1GjcwfzbXtP3+9p4OVV9cwLrN3J6LB9l41jzLX7+v8erp8NHAMcX1UPJDkOuIXRL6s9zaoJ8+Te0vRx4Iwkx2dk/ySnJFm/+8Kq+uGw/qNJXgGQ5MgkvzTuN6uqncC/An+S5MAkq5K8OsmbhiVXAu9PsiHJwcAHx9jse4f1hwC/A/zt8PX1jH5RPTbc93u7Pe5B4OhxZ9fCGP4SVFU3Ar8JXAg8CtwFvGsvDzlnWHP9cJb+C4yeVV+M32D058Qdw/e8CjhiuO/jwL8AXwNuBj4zxvb+htEvk7uBbwIfHr5+AaOTiY8A1wP/vNvjPgb8+nDGf0W9R2EpyXCyZDIbS7Yw+sGtBi6uqj+c2MZnLMmlwKnAQ1X12lnPM2lJjgL+mtGJvgK2VdXHFritexidoNvT+YxFN7yceB2jVzbWAFdV1e5HG8va8PLrjcD9VXXq3tZO7Bl/+KYXAb8MHAucluTYSW1/CbiM0QmpleoZ4OyqOhZ4I6ND9ZX083ua0SsnP8XolYQtSd4445km7UxgxzgLJ3mo/wbgrqq6u6q+D1wB/OoEtz9TVXUd8N1ZzzEtVbWzqm4erj/B6D+gI2c71eTUyJPDzbXDZcWcSBzeBHUKcPE46ycZ/pE8/yWc+1hB/+F0kmQTozcR3bCQx1fVpqV0mL/L8A7EWxm9OerqqlrQ/i1RFzD6DMcPx1nsyT09z/DBoU8DZ1XV47OeZ5Kq6tmqOg7YALwhyYo4V5Nk17mnm8Z9zCTDv5/nv3a7YfialokkaxlF/8mqGufM/bJUVY8xetvxSjlncwLw1uGk6hXASUku39sDJhn+V4EfS/KjSfYB3s7o/eNaBoYPvVwC7Kiqj8x6nklLcliSg4br64C3MPr8wbJXVedW1Yaq2sSou2uq6h17e8zEwh/e8fXbjF7v3QFcWVUL+ujoUpTkU8CXgWOGT7u9e9YzTdgJwDsZPVvcOlxOnvVQE3QEcG2S/2D0JHV1VW2f8UwzM9HX8SUtD57ckxoyfKkhw5caMnypobHCT7IlyTeS3JVknI9kSlrC5g1/IR++SbJ1MuMtPSt538D9W+7G3b9xnvEX8uGblfyPu5L3Ddy/5W5i4fvhG2mFmdj/c284xNgKsO++a16/efPRK/KdQRs3Hsqa1fuvyH0DWJV9SLJi9w9Y8fs3jnHCH+vDN1W1DdgGsHnz0XXDVz80kQGXosMPuGDWI0zVd566ZdYjaMH29j8q/n/jHOr74RtphZn3Gb+qnkmy68M3q4FLV9KHb6SOxvobv6o+B3xuyrNIWiS+c09qyPClhgxfasjwpYYMX2rI8KWGDF9qyPClhgxfasjwpYYMX2rI8KWGDF9qyPClhgxfasjwpYYMX2rI8KWGDF9qyPClhgxfasjwpYYMX2rI8KWGDF9qyPClhgxfasjwpYYMX2rI8KWGDF9qyPClhgxfasjwpYYMX2rI8KWGDF9qyPClhgxfasjwpYYMX2rI8KWGDF9qyPClhgxfasjwpYYMX2po3vCTXJrkoSS3LcZAkqZvnGf8y4AtU55D0iKaN/yqug747iLMImmRrJnUhpJsBbYCbNx46KQ2uyQ9+ORZsx5hqg4/4IJZjzBV33nqllmPMHMTO7lXVduqanNVbT7ssAMntVlJU+BZfakhw5caGuflvE8BXwaOSXJfkndPfyxJ0zTvyb2qOm0xBpG0eDzUlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGpo3/CRHJbk2yR1Jbk9y5mIMJml61oyx5hng7Kq6Ocl64KYkV1fVHVOeTdKUzPuMX1U7q+rm4foTwA7gyGkPJml6xnnGf06STcDrgBte4L6twNbnNrzq9Jc42tL1g386fNYjTNW3/vMXZj3CVB244ZZZjzBzY4ef5ADg08BZVfX47vdX1TZg27C2JjahpIkb66x+krWMov9kVX1muiNJmrZxzuoHuATYUVUfmf5IkqZtnGf8E4B3AicluXW4nDzluSRN0bx/41fVl4AswiySFonv3JMaMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxqaN/wk+yX5SpKvJbk9ye8vxmCSpmfNGGueBk6qqieTrAW+lOTzVXX9lGeTNCXzhl9VBTw53Fw7XGqaQ0maroy6nmdRshq4CXgNcFFVnfMCa7YCW4ebr4fVk5xT0liepaoy36qxwn9ucXIQ8FngfVV1217WleFLszBe+C/qrH5VPQZcC2xZ6FiSZm+cs/qHDc/0JFkHvAX4+rQHkzQ945zVPwL4q+Hv/FXAlVW1fbpjSZqmF/U3/tgb9W98aUam8De+pJXB8KWGDF9qyPClhgxfasjwpYYMX2rI8KWGDF9qyPClhgxfasjwpYYMX2rI8KWGDF9qyPClhgxfasjwpYYMX2rI8KWGDF9qyPClhgxfasjwpYYMX2rI8KWGDF9qyPClhgxfasjwpYYMX2rI8KWGDF9qyPClhgxfasjwpYYMX2rI8KWGDF9qyPClhgxfasjwpYYMX2rI8KWGDF9qyPClhgxfamjs8JOsTnJLku3THEjS9L2YZ/wzgR3TGkTS4hkr/CQbgFOAi6c7jqTFsGbMdRcAHwDW72lBkq3A1uHm0/DsbS9xtqXq5cAjsx5iity/5e2YcRbNG36SU4GHquqmJCfuaV1VbQO2DY+5sao2jznosrKS9w3cv+UuyY3jrBvnUP8E4K1J7gGuAE5KcvlLmE3SjM0bflWdW1UbqmoT8Hbgmqp6x9QnkzQ103odf9uUtrsUrOR9A/dvuRtr/1JV0x5E0hLjO/ekhgxfasjwpYYMX2rI8KWGDF9qyPClhv4PP91JCfhMzZcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = run_trials(m, a, 10)\n",
    "visualize_history(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are the $Q$ values changing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials_for_altair(environment, agent, n, collect=True):\n",
    "    \"\"\"Runs N trials\"\"\"\n",
    "    state_action = {} \n",
    "    # init all state_actions\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for direction in ['down', 'right', 'up', 'left']:\n",
    "                state_action[str(((i, j), direction))] = [0] \n",
    "\n",
    "    for j in range(n):\n",
    "        run_trial(environment, agent)\n",
    "        all_keys = set(state_action.keys())\n",
    "        # keys with new values\n",
    "        for key, val in agent.Q.items():\n",
    "            state_action[str(key)].append(val)\n",
    "            all_keys.remove(str(key))\n",
    "        # keys without new values\n",
    "        for key in all_keys:\n",
    "            state_action[str(key)].append(state_action[str(key)][-1])\n",
    "        environment.state = Maze.INITIAL_STATE\n",
    "        \n",
    "    import pandas as pd\n",
    "    location = []\n",
    "    run = []\n",
    "    q_value = []\n",
    "    for loc in state_action.keys():\n",
    "        for i in range(len(state_action[loc])):\n",
    "            location.append(loc)\n",
    "            run.append(i)\n",
    "            q_value.append(state_action[loc][i])\n",
    "    df = pd.DataFrame({\"location\":location, \"run\":run, \"q_value\":q_value})\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-1f7d71123f76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_trials_for_altair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Agent' is not defined"
     ]
    }
   ],
   "source": [
    "m = Maze()\n",
    "a = Agent()\n",
    "df = run_trials_for_altair(m, a, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "slider = alt.binding_range(min=1, max=100, step=1)\n",
    "select_run = alt.selection_single(name=\"iteration\", fields=['run'], bind=slider)\n",
    "alt.data_transformers.enable('default', max_rows=None)\n",
    "alt.Chart(df).mark_bar().encode(\n",
    "    x='location:N',\n",
    "    y=alt.Y('q_value:Q', scale=alt.Scale(domain=(0, 11))),\n",
    ").add_selection(\n",
    "    select_run\n",
    ").transform_filter(\n",
    "    select_run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
