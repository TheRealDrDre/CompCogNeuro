{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning, Part 1: Agents, States, and Rewards  \n",
    "\n",
    "Here we will show the simplest form of RL: An agent going through a simple environment with a determinstic succession of states.\n",
    "\n",
    "## The environment\n",
    "\n",
    "The environment represents a prototypical conditing paradigm. It is made of three states, which represent an initial  cue, a delay, and a reward. This captures a simple experiment such as the case in which a primate is given reward (juice) after a blue light is presented.\n",
    "\n",
    "An environment is defined by two functions, the state transition matrix $P(s,a,s')$ and a reward transition matrix $R(s,a,s')$. In this simple case, the agent does not act---it is simply observing the environment. So, we can define $P(s,s')$ and $R(s,s')$ without any reference to any action $a$. Furthermore, because the environment is deterministic, we can represent both $P(s,s')$ and $R(s,s')$ as matrices.\n",
    "\n",
    "The environment transitions are given to the agent in the form of a state-reward pair, which, in Python, will be represented as a tuple `(new_state, new_reward)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "class Environment:\n",
    "    \"\"\"A simple environment\"\"\"\n",
    "    STATE_TRANSITIONS = {\"cue\" : \"wait\",\n",
    "                         \"wait\" : \"juice\"}\n",
    "    \n",
    "    REWARD_TRANSITIONS = {\"cue\" : 0,\n",
    "                         \"wait\" : 0,\n",
    "                         \"juice\" : 1,\n",
    "                         None : 0}\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the environment\"\"\"\n",
    "        self.state = \"cue\"\n",
    "    \n",
    "    \n",
    "    def transition(self):\n",
    "        \"\"\"Transitions, and returns the state-reward pair\"\"\"\n",
    "        result = None\n",
    "        state = self.state\n",
    "        if state in Environment.STATE_TRANSITIONS.keys():\n",
    "            new_state = Environment.STATE_TRANSITIONS[state]\n",
    "            self.state = new_state\n",
    "            result = (new_state, Environment.REWARD_TRANSITIONS[new_state])\n",
    "        else:\n",
    "            self.state = None\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the environment\n",
    "\n",
    "Let's test the environment works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Environment()\n",
    "print(\"State 1: '%s'\" % (e.state,))\n",
    "res = e.transition()\n",
    "print(\"State 2: '%s', Reward = %s\" % res)\n",
    "res = e.transition()\n",
    "print(\"State 3: '%s', Reward = %s\" % res)\n",
    "res = e.transition()\n",
    "print(\"After state 3: '%s'\" % (e.state,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a V-Agent\n",
    "\n",
    "Now, let's create a simple agent that just observes states and estimates their values by creating a $V$-table. Because the number of states is so small, the $V$-table can be initialized right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAgent():\n",
    "    \"\"\"An agent that passively observes states\"\"\"\n",
    "    def __init__(self):\n",
    "        self.V = {\"cue\" : 0,\n",
    "                 \"wait\" : 0,\n",
    "                 \"juice\" : 0,\n",
    "                 None : 0}\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.1\n",
    "        \n",
    "    def learnV(self, state1, reward1, state2):\n",
    "        \"\"\"Learns from rewards\"\"\"\n",
    "        V1 = self.V[state1]\n",
    "        V2 = self.V[state2]\n",
    "        g = self.gamma\n",
    "        a = self.alpha\n",
    "        \n",
    "        rpe = reward1 + g * V2 - V1\n",
    "        V1 = V1 + a * rpe\n",
    "        \n",
    "        self.V[state1] = V1\n",
    "        \n",
    "    \n",
    "    def visualizeV(self, title=\"V table\"):\n",
    "        \"\"\"Visualizes the V-table\"\"\"\n",
    "        states = [\"cue\", \"wait\", \"juice\", None]\n",
    "        values = [self.V[x] for x in states]\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.axis([-0.5, 3.5, 0, 1.0])\n",
    "        ax.set_xticks([0, 1, 2, 3])\n",
    "        ax.set_xticklabels([str(x) for x in states])\n",
    "        ax.set_ylabel(\"Value V\")\n",
    "        ax.set_xlabel(\"States\")\n",
    "        ax.bar([0, 1, 2, 3], values, color='black')\n",
    "        ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions between agent and environment\n",
    "Now, we need to define how to run a trial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(environment, agent):\n",
    "    while environment.state is not None:\n",
    "        state = environment.state\n",
    "        reward = environment.REWARD_TRANSITIONS[state]\n",
    "        \n",
    "        # This is the moment in which we learn!\n",
    "        transition = environment.transition()\n",
    "        new_state = None\n",
    "        if transition is not None:\n",
    "            new_state = transition[0]\n",
    "        \n",
    "        agent.learnV(state, reward, new_state)\n",
    "        \n",
    "def run_trials(environment, agent, n):\n",
    "    for j in range(n):\n",
    "        environment.state = \"cue\"\n",
    "        run_trial(environment, agent)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"cue\", \"wait\", \"juice\", None]\n",
    "a = VAgent()\n",
    "e = Environment()\n",
    "run_trials(e, a, 1)\n",
    "a.visualizeV(\"V table (n=1)\")\n",
    "run_trials(e, a, 9)\n",
    "a.visualizeV(\"V table (n=10)\")\n",
    "run_trials(e, a, 90)\n",
    "a.visualizeV(\"V table (n=100)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
