{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning, Part 1: Agents, States, and Rewards  \n",
    "\n",
    "Here we will show the simplest form of RL: An agent going through a simple environment with a determinstic succession of states.\n",
    "\n",
    "## The environment\n",
    "\n",
    "The environment represents a prototypical conditing paradigm. It is made of three states, which represent an initial  cue, a delay, and a reward. This captures a simple experiment such as the case in which a primate is given reward (juice) after a blue light is presented.\n",
    "\n",
    "An environment is defined by two functions, the state transition matrix $P(s,a,s')$ and a reward transition matrix $R(s,a,s')$. In this simple case, the agent does not act---it is simply observing the environment. So, we can define $P(s,s')$ and $R(s,s')$ without any reference to any action $a$. Furthermore, because the environment is deterministic, we can represent both $P(s,s')$ and $R(s,s')$ as _tables_.\n",
    "\n",
    "### Implementation in Python\n",
    "\n",
    "In the code below, the Environment's state and reward transitions are implemented as _dictionaries_, that is, special structures that associate one object (the _key_) with another (the _value_). This is a vary convenient format for a simple environments in which all events follow deterministically. THe `STATE_TRANSITIONS` dictionary links each state with the state that follows. The `REWARD_TRANSITIONS` dictionary links each state with its associated reward.  The null object `None` marks the end of a trial.\n",
    "\n",
    "When the environment undergoes a transition, the agent receives a new state-reward pair, which, in Python, will be represented as a tuple `(new_state, new_reward)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "class Environment:\n",
    "    \"\"\"A simple environment\"\"\"\n",
    "    STATE_TRANSITIONS = {\"cue\" : \"wait\",\n",
    "                         \"wait\" : \"juice\",\n",
    "                         \"juice\" : None}\n",
    "    \n",
    "    REWARD_TRANSITIONS = {\"cue\" : 0,\n",
    "                          \"wait\" : 0,\n",
    "                          \"juice\" : 1,\n",
    "                          None : 0}\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the environment\"\"\"\n",
    "        # An environment class always keeps track of the current state we are in.\n",
    "        # We always begin with the state 'cue' (the beginning of a trial)\n",
    "        self.state = \"cue\"\n",
    "    \n",
    "    \n",
    "    def transition(self):\n",
    "        \"\"\"Transitions, and returns the state-reward pair\"\"\"\n",
    "        state = self.state  # The current state\n",
    "        \n",
    "        new_state = Environment.STATE_TRANSITIONS[state]\n",
    "        new_reward = Environment.REWARD_TRANSITIONS[new_state]\n",
    "        \n",
    "        # Let's update the current state before \n",
    "        self.state = new_state\n",
    "        \n",
    "        # The agent receives the new state and reward. \n",
    "        return (new_state, new_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the environment\n",
    "\n",
    "Let's test the environment works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 1: 'cue'\n",
      "State 2: 'wait', Reward = 0\n",
      "State 3: 'juice', Reward = 1\n",
      "After state 3: 'None'\n"
     ]
    }
   ],
   "source": [
    "e = Environment()\n",
    "print(\"State 1: '%s'\" % (e.state,))\n",
    "\n",
    "# First transition: Cue -> Wait\n",
    "res = e.transition()\n",
    "print(\"State 2: '%s', Reward = %s\" % res)\n",
    "\n",
    "# Second transition: Wait -> Juice\n",
    "res = e.transition()\n",
    "print(\"State 3: '%s', Reward = %s\" % res)\n",
    "\n",
    "# Final transition: Juice -> 'None'\n",
    "res = e.transition()\n",
    "print(\"After state 3: '%s'\" % (e.state,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "Now, let's create a simple agent that just observes states and estimates their values by creating a $V$-table. Because the number of states is so small, the $V$-table can be initialized right away, with all the states being zero. \n",
    "\n",
    "The $V$-Agent learns the value of states using TD-learning. Every time the agent observes a new state, it updates the associated value of the previous state using the TD-learning equation:\n",
    "\n",
    "$V(S_t) = V(S_t) + \\alpha [ r_{t} + \\gamma V(S_{t+1}) - V(S_t)]$\n",
    "\n",
    "### Implementation in Python\n",
    "\n",
    "The agent is an object with two internal parameters, `alpha` and `gamma` (which record to the $\\alpha$ and $\\gamma$ values of the TD-learning equation), and a dictionary that records the $V$-values associated with every state (The $V$ table). The agent has a single method, `learn`, tha implements the TD-learning equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAgent():\n",
    "    \"\"\"An agent that passively observes states\"\"\"\n",
    "    def __init__(self, gamma=0.9, alpha=0.1):\n",
    "        \"Initializes an agent with default parameters and zero values in the V table\"\n",
    "        self.V = {\"cue\" : 0,\n",
    "                  \"wait\" : 0,\n",
    "                  \"juice\" : 0,\n",
    "                  None : 0}\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        \n",
    "    # This method implements TD-learning\n",
    "    #\n",
    "    def learn(self, state1, reward1, state2):\n",
    "        \"\"\"Learns using TD-learning\"\"\"\n",
    "        V1 = self.V[state1]\n",
    "        V2 = self.V[state2]\n",
    "        g = self.gamma\n",
    "        a = self.alpha\n",
    "        \n",
    "        # Calculate RPE and new estimate of V1\n",
    "        rpe = reward1 + g * V2 - V1\n",
    "        V1 = V1 + a * rpe\n",
    "        \n",
    "        # Update the internal value of V1\n",
    "        self.V[state1] = V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions between agent and environment\n",
    "Now, we need to define how to run a trial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(environment, agent):\n",
    "    while environment.state is not None:\n",
    "        state = environment.state\n",
    "        reward = environment.REWARD_TRANSITIONS[state]\n",
    "        \n",
    "        # This is the moment in which we learn!\n",
    "        transition = environment.transition()\n",
    "        new_state = None\n",
    "        if transition is not None:\n",
    "            new_state = transition[0]\n",
    "        \n",
    "        agent.learn(state, reward, new_state)\n",
    "        \n",
    "def run_trials(environment, agent, n):\n",
    "    for j in range(n):\n",
    "        environment.state = \"cue\"\n",
    "        run_trial(environment, agent)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the system\n",
    "\n",
    "To test the system, we need to create a function that visualizes a $V$-table. The following function takes the an agent's dictionary of state-value associations and visualizes them ina bargraph. For simplicity, the state `None` is not shown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple function to visualize the agent's V-table\n",
    "def visualizeV(Vtable, axes, title=\"V table\"):\n",
    "    \"\"\"Visualizes the V-table\"\"\"\n",
    "    states = [\"cue\", \"wait\", \"juice\"]\n",
    "    values = [Vtable[x] for x in states]\n",
    "    #fig, ax = plt.subplots()\n",
    "    axes.axis([-0.5, 2.5, 0, 1.0])\n",
    "    axes.set_xticks([0, 1, 2])\n",
    "    axes.set_xticklabels([str(x) for x in states])\n",
    "    \n",
    "    axes.bar([0, 1, 2], values)\n",
    "    axes.set_title(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can test the agent's learning by visualizing its internal $V$-table at different momements during learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x252 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "states = [\"cue\", \"wait\", \"juice\", None]\n",
    "\n",
    "a = VAgent()\n",
    "e = Environment()\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(7,3.5), sharey=True)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "axs[0].set_ylabel(r\"Value $V$\")\n",
    "axs[1].set_xlabel(\"States\")    \n",
    "\n",
    "run_trials(e, a, 1)\n",
    "visualizeV(a.V, axs[0], r\"$V$-table ($t=1$)\")\n",
    "\n",
    "run_trials(e, a, 9)\n",
    "visualizeV(a.V, axs[1], r\"$V$-table ($t=10$)\")\n",
    "\n",
    "run_trials(e, a, 90)\n",
    "visualizeV(a.V, axs[2], r\"$V$-table ($t=100$)\")\n",
    "\n",
    "plt.savefig(\"vtable.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
