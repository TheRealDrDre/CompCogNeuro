{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning, Part 3: Dealing with Non-Markov environments\n",
    "\n",
    "Here we present a problem analogous to that faced by Redish (2004): The problem of credit assignment in a non-Markov environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non-markov problem\n",
    "##\n",
    "##          A          Win 1.0\n",
    "##        /   \\      /\n",
    "##  start      middle\n",
    "##        \\   /      \\ \n",
    "##          B          Loss -1.0\n",
    "##\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reward function\n",
    "\n",
    "R = {\"start\" :  0,\n",
    "     \"A\" : 0,\n",
    "     \"B\" : 0,\n",
    "     \"middle\" : 0,\n",
    "     \"win\" : 1.0,\n",
    "     \"loss\" : -1.0,\n",
    "     None : 0.0\n",
    "}\n",
    "\n",
    "# State transition probability function\n",
    "#\n",
    "def PSS(history):\n",
    "    s_current = history[-1]\n",
    "    s_next = None\n",
    "    \n",
    "    if s_current is \"start\":\n",
    "        coin = random.uniform(0, 1)\n",
    "        if coin <= 0.5:\n",
    "            s_next = \"A\"\n",
    "        else:\n",
    "            s_next = \"B\"\n",
    "        \n",
    "    elif s_current is \"A\" or s_current is \"B\":\n",
    "        s_next = \"middle\"\n",
    "\n",
    "    elif s_current is \"middle\":\n",
    "        s_previous = history[-2]\n",
    "        if s_previous is \"A\":\n",
    "            s_next = \"win\"\n",
    "        elif s_previous is \"B\":\n",
    "            s_next = \"loss\"\n",
    "        \n",
    "    elif s_current is \"win\" or s_current is \"loss\":\n",
    "        s_next = None\n",
    "\n",
    "    return (s_next, R[s_current])\n",
    "    \n",
    "\n",
    "## The agent just passively experiences a set of states \n",
    "class Agent:\n",
    "    def __init__(self, states = R.keys(), alpha = 0.1, gamma = 0.9):\n",
    "        self.V = {}\n",
    "\n",
    "        # Inits all state values to zero\n",
    "        for s in states:\n",
    "            self.V[s] = 0.0\n",
    "\n",
    "        # sets the two parameters\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def td_learning(self, state1, state2, reward2):\n",
    "        \"\"\"Classic TD learning\"\"\"\n",
    "        a = self.alpha\n",
    "        g = self.gamma\n",
    "        rpe = reward2 + g * self.V[state2] - self.V[state1]\n",
    "        self.V[state1] += a * rpe\n",
    "\n",
    "\n",
    "def plot_v_table(agent, axes, title=\"V-table\"):\n",
    "    \"Plots a V-table as a bargraph\"\n",
    "    states = (\"start\", \"A\", \"B\", \"middle\", \"win\", \"loss\")\n",
    "    values = [agent.V[state] for state in states]\n",
    "        \n",
    "    axes.axis([-0.5, 5.5, -1.0, 1.0])\n",
    "    x = np.arange(len(states))\n",
    "    axes.set_xticks(x)\n",
    "    axes.set_xticklabels(states)\n",
    "    axes.set_ylabel(r\"Value $V$\")\n",
    "    axes.set_xlabel(\"States\")\n",
    "    axes.bar(x, values)\n",
    "    axes.axhline(y=0, xmin=0, xmax=100, color=\"grey\")\n",
    "    axes.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the agent cannot solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_loop(agent, n = 1000):\n",
    "    for i in range(n):\n",
    "        history = [\"start\"]\n",
    "\n",
    "        while PSS(history)[0] is not None:\n",
    "            s_now = history[-1]\n",
    "            s_next, r = PSS(history)\n",
    "            history.append(s_next)\n",
    "            agent.td_learning(s_now, s_next, r)\n",
    "\n",
    "        s_now = history[-1]\n",
    "        s_next, r = PSS(history)\n",
    "        history.append(s_next)\n",
    "        agent.td_learning(s_now, s_next, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = Agent()\n",
    "rl_loop(a, 10000)\n",
    "fig, ax = plt.subplots()\n",
    "plot_v_table(a, ax, r\"$V$-table, with standard TD-learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The solution: Eligibility traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaAgent:\n",
    "    \"\"\"An aget with eligibility traces and TD(lambda) learning\"\"\"\n",
    "    def __init__(self, states = R.keys(), alpha = 0.1,\n",
    "                 gamma = 0.9, lmbda = 0.9):\n",
    "        self.V = {}\n",
    "        self.E = {}\n",
    "        \n",
    "        # Inits all state values to zero\n",
    "        for s in states:\n",
    "            self.E[s] = 0.0\n",
    "            self.V[s] = 0.0\n",
    "\n",
    "        # sets the three parameters\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "    \n",
    "        \n",
    "    def td_learning(self, s_now, s_next, r):\n",
    "        \"\"\"Learns a V-table with TD(lambda)\"\"\"\n",
    "        #print(\"state %s, reward %.2f\" % (s_now, r))\n",
    "        a = self.alpha\n",
    "        g = self.gamma\n",
    "        l = self.lmbda\n",
    "        rpe = r + g * self.V[s_next] - self.V[s_now]\n",
    "        \n",
    "        # First, update the trace vector\n",
    "        for s in self.E.keys():\n",
    "            self.E[s] *= (l * g)\n",
    "            if s == s_now:\n",
    "                 self.E[s] += 1\n",
    "\n",
    "        # Second, update the V table\n",
    "        for s in self.E.keys():\n",
    "            self.V[s] += a * rpe * self.E[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent()\n",
    "rl_loop(a, 10000)\n",
    "\n",
    "la = LambdaAgent()\n",
    "rl_loop(la, 10000)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(7,3))\n",
    "plot_v_table(a, axs[0], r\"$V$-table, with TD-learning\")\n",
    "plot_v_table(la, axs[1], r\"$V$-table, with TD($\\lambda$)\")\n",
    "\n",
    "for ax in axs.flat:\n",
    "        ax.label_outer()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"figures/vtables_nonmarkov.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eligibility Traces in the Maze Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze():\n",
    "    \"\"\"A maze environment\"\"\"\n",
    "\n",
    "    ACTIONS = (\"up\", \"down\", \"left\", \"right\") # List of actions\n",
    "    INITIAL_STATE = (0, 0) # Always starts at the topleft corner\n",
    "    \n",
    "    def __init__(self, fname = \"grid.txt\"):\n",
    "        \"\"\"Inits a maze by loading the grid file\"\"\"\n",
    "        self.grid = np.loadtxt(fname)\n",
    "        self.state = self.INITIAL_STATE\n",
    "        self.end = False\n",
    "\n",
    "\n",
    "    def state_transition(self, state1, action1):\n",
    "        \"Defines the next state gien the \"\n",
    "        x, y = state1\n",
    "        \n",
    "        # If we have reached the cheese, we transition \n",
    "        # to the terminal state\n",
    "        if self.grid[x, y] > 0:\n",
    "            return None\n",
    "        \n",
    "        # Otherwise, we update the position\n",
    "        state2 = copy(state1)\n",
    "        \n",
    "        if action1 in self.ACTIONS:\n",
    "            if action1 == \"up\":\n",
    "                if x > 0:\n",
    "                    state2 = (x - 1, y)\n",
    "            \n",
    "            elif action1 == \"left\":\n",
    "                if y > 0:\n",
    "                    state2 = (x, y - 1)\n",
    "            \n",
    "            elif action1 == \"down\":\n",
    "                if x < (self.grid.shape[0] - 1):\n",
    "                    state2 = (x + 1, y)\n",
    "\n",
    "            elif action1 == \"right\":\n",
    "                if y < (self.grid.shape[1] - 1):\n",
    "                    state2 = (x, y + 1)\n",
    "                    \n",
    "        return state2\n",
    "                    \n",
    "    \n",
    "    def reward_transition(self, state1, action1, state2):\n",
    "        \"\"\"Reward is -1 for bouncing against the walls, and whatever is on the grid otherwise\"\"\"\n",
    "        if state1 == state2:\n",
    "            return -1\n",
    "        elif state2 == None:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.grid[state2[0], state2[1]]\n",
    "        \n",
    "    \n",
    "    # Quick way to combine State transitions and Reward transitions \n",
    "    def transition(self, action1):\n",
    "        \"\"\"Changes the state following an action\"\"\"\n",
    "        state1 = self.state\n",
    "        state2 = self.state_transition(state1, action1)\n",
    "        reward2 = self.reward_transition(state1, action1, state2)\n",
    "        \n",
    "        self.state = state2\n",
    "        return (state2, reward2) # Returns s_t+1, r_t+1\n",
    "\n",
    "    \n",
    "    def print_state(self):\n",
    "        \"Prints a text representation of the maze (with the agent position)\"\n",
    "        bar = \"-\" * ( 4 * self.grid.shape[1] + 1)\n",
    "        for i in range(self.grid.shape[0]):\n",
    "            row = \"|\"\n",
    "            for j in range(self.grid.shape[1]):\n",
    "                cell = \" \"\n",
    "                if i == self.state[0] and j == self.state[1]:\n",
    "                    cell = \"*\"\n",
    "                row += (\" %s |\" % cell)\n",
    "            print(bar)\n",
    "            print(row)\n",
    "        print(bar)\n",
    "\n",
    "        \n",
    "class VlambdaAgent():\n",
    "    \"\"\"An agent that keeps track of the value of states\"\"\"\n",
    "    def __init__(self, actions=Maze.ACTIONS, alpha=0.1, \n",
    "                 gamma=0.9, lmbda = 0.8):\n",
    "        \"\"\"Creates a V-agent\"\"\"\n",
    "        self.V = {}                # Initial dictionary of States (V-table)\n",
    "        self.E = {}                # Initial dictionary of Eligibility traces\n",
    "        \n",
    "        self.alpha = alpha         # Learning rate\n",
    "        self.gamma = gamma         # Temporal discounting\n",
    "        self.lmbda = lmbda         # Lambda decay\n",
    "        \n",
    "        self.actions = actions     # Set of possible actions (provide those of Maze.ACTIONS)\n",
    "\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"Random policy to explore the maze\"\"\"\n",
    "        return random.choice(self.actions)\n",
    "        \n",
    "    \n",
    "    def td_lambda(self, state1, reward1, state2):\n",
    "        \"\"\"Updates the Q-values when given an (s,a) pair, the reward value and a new state\"\"\"\n",
    "        g = self.gamma\n",
    "        a = self.alpha\n",
    "        l = self.lmbda\n",
    "        \n",
    "        v1 = 0.0\n",
    "        \n",
    "        if state1 in self.V.keys():\n",
    "            v1 = self.V[state1]\n",
    "        else:\n",
    "            self.V[state1] = 0.0\n",
    "            self.E[state1] = 0.0\n",
    "        \n",
    "        v2 = 0.0\n",
    "        \n",
    "        if state2 in self.V.keys():\n",
    "            v2 = self.V[state2]\n",
    "        \n",
    "        rpe = reward1 + g * v2 - v1\n",
    "        \n",
    "        # First, update the trace vector\n",
    "        for s in self.E.keys():\n",
    "            if s == state1:\n",
    "                 self.E[s] += 1\n",
    "            else:\n",
    "                self.E[s] *= l\n",
    "\n",
    "        # Second, update the V table\n",
    "        for s in self.E.keys():\n",
    "            self.V[s] += a * rpe * self.E[s]\n",
    "\n",
    "            \n",
    "def plot_v_table(agent, ax=None, title=r\"$V$-table\", **kwargs):\n",
    "    \"Plots the V table of an agent\"\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    data = np.zeros((4,4))            \n",
    "            \n",
    "    for s in agent.V.keys():\n",
    "        x, y = s\n",
    "        data[x, y] = agent.V[s]\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs, cmap=\"viridis\")\n",
    "\n",
    "    # Create colorbar\n",
    "    #cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    #cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    ax.set_xticklabels(range(1,5))\n",
    "    ax.set_yticklabels(range(1,5))\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=False, bottom=True,\n",
    "                   labeltop=False, labelbottom=True)\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    for edge, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"grey\", linestyle='-', linewidth=2)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    ax.set_title(title)\n",
    "    return im\n",
    "\n",
    "\n",
    "def rl_loop(environment, agent):\n",
    "    \"\"\"A trial ends when the agent gets a reward. The history is returned\"\"\"\n",
    "    state1 = environment.state\n",
    "    reward1 = environment.grid[state1[0], state1[1]]\n",
    "    state2 = \"Start\"\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    while state2 != None:\n",
    "        action = agent.policy(state1)\n",
    "        state2, reward2 = environment.transition(action)\n",
    "        history.append((state2, copy(agent.E)))\n",
    "        \n",
    "        # Update the V-values for state1\n",
    "        agent.td_lambda(state1, reward1, state2)\n",
    "        \n",
    "        state1 = state2\n",
    "        reward1 = reward2\n",
    "\n",
    "    return history\n",
    "\n",
    "    \n",
    "def run_trials(environment, agent, n, collect=True):\n",
    "    \"\"\"Runs N trials\"\"\"\n",
    "    history = []\n",
    "    for j in range(n):\n",
    "        h = rl_loop(environment, agent)\n",
    "        history += h\n",
    "        environment.state = Maze.INITIAL_STATE\n",
    "    \n",
    "    return history    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Maze()\n",
    "a = VlambdaAgent(alpha=0.1)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(7,3))\n",
    "\n",
    "# Because the agent moves at random, the following instruction can take a variable amount of time to complete\n",
    "run_trials(m, a, 1)  \n",
    "plot_v_table(a, axs[0], vmin=0, vmax=10)\n",
    "axs[0].set_title(r\"$V$-Table ($t$=1)\")\n",
    "\n",
    "run_trials(m, a, 9)  \n",
    "plot_v_table(a, axs[1], vmin=0, vmax=10)\n",
    "axs[1].set_title(r\"$V$-Table ($t$=10)\")\n",
    "\n",
    "run_trials(m, a, 990)  \n",
    "plot_v_table(a, axs[2], vmin=0, vmax=10)\n",
    "axs[2].set_title(r\"$V$-Table ($t$=1000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eligibility_traces(history, title=\"Traces\"):\n",
    "    \"Plots the time course of eligibility traces over time\"\n",
    "    # Get eligibility traces\n",
    "    E = [x[1] for x in history]\n",
    "    \n",
    "    # Get list of states\n",
    "    res = []\n",
    "    for e in E:\n",
    "        res.extend(list(e.keys()))\n",
    "    res.sort()\n",
    "\n",
    "    # Remove duplicates\n",
    "    S = []\n",
    "    for s in res:\n",
    "        if s not in S:\n",
    "            S.append(s)\n",
    "\n",
    "            \n",
    "    # The X-axis\n",
    "    x = range(1, len(E) + 1)\n",
    "    \n",
    "    # A Y-line for each state\n",
    "    for s in S:\n",
    "        y = []\n",
    "        for e in E:\n",
    "            if s in list(e.keys()):\n",
    "                y.append(e[s])\n",
    "            else:\n",
    "                y.append(0.0)\n",
    "        plt.plot(x, y)\n",
    "    \n",
    "    # Update state coordinates for proper lagend\n",
    "    R = []\n",
    "    for x, y in S:\n",
    "        R.append((x + 1, y + 1))\n",
    "    plt.legend(R)\n",
    "    \n",
    "    plt.xlabel(r\"Time $t$\")\n",
    "    plt.ylabel(\"Eligibility value\")\n",
    "    plt.title(title)\n",
    "\n",
    "def plot_state(maze, ax=None, position = None, agent=True, reward=True, \n",
    "               title=\"Current State\", **kwargs):\n",
    "    \n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    nrows, ncols = maze.grid.shape\n",
    "    data = np.zeros((nrows, ncols))            \n",
    "            \n",
    "    im = ax.imshow(data, **kwargs, cmap=\"Greys\")\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    ax.set_xticklabels(range(1, ncols + 1))\n",
    "    ax.set_yticklabels(range(1, nrows + 1))\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=False, bottom=True,\n",
    "                   labeltop=False, labelbottom=True)\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    for edge, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"grey\", linestyle='-', linewidth=2)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    if reward:\n",
    "        for i in range(ncols):\n",
    "            for j in range(nrows):\n",
    "                r = maze.grid[j, i]\n",
    "                color = None\n",
    "                if r > 0:\n",
    "                    color = \"#EE111144\"\n",
    "                elif r < 0:\n",
    "                    color = \"#1111EE44\"\n",
    "                else:\n",
    "                    color = \"#11111144\"\n",
    "                text = ax.text(i, j, \"%d\" % (r,),\n",
    "                               ha=\"center\", va=\"top\", color=color)\n",
    "    \n",
    "    if agent:\n",
    "        if position is None:\n",
    "            position = maze.state\n",
    "        x, y = position\n",
    "        text = ax.text(y, x, r\"A\", size=\"larger\", weight=\"bold\",\n",
    "                       ha=\"center\", va=\"bottom\", color=\"k\")\n",
    "    \n",
    "    return im\n",
    "                \n",
    "    \n",
    "def plot_history_sequence(maze, history, shape=None, **kwargs):\n",
    "    \"Plots a sequence of moves in the grid as a set 'movie' of maze positions\"\n",
    "    N = len(history)\n",
    "    ncols = 0\n",
    "    nrows = 0\n",
    "    if shape is None:\n",
    "        side = math.ceil(math.sqrt(N))\n",
    "        if side * (side - 1) > N:\n",
    "            ncols = side \n",
    "            nrows = side - 1\n",
    "        else:\n",
    "            ncols = side\n",
    "            nrows = side\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize = (ncols * 2, nrows * 2))\n",
    "    for ax in axs.flatten():\n",
    "        ax.label_outer()\n",
    "    for y in range(ncols):\n",
    "        for x in range(nrows):\n",
    "            ii = x * ncols + y\n",
    "            if ii < N:\n",
    "                plot_state(maze, ax=axs[x, y], position=history[ii], title=r\"$t=%d$\" % (ii + 1,))\n",
    "            else:\n",
    "                axs[x, y].axis('off')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize eligibility traces in a Maze\n",
    "\n",
    "The following plot shows the eligibility traces associated with each maze cell visited by the agent in an initial run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Maze()\n",
    "a = VlambdaAgent(alpha=0.1)\n",
    "h = run_trials(m, a, 1)  \n",
    "plot_eligibility_traces(h, r\"$e(s)$ traces in a trial\")\n",
    "plt.savefig(\"figures/traces.png\")\n",
    "\n",
    "# Print move by move\n",
    "h = [x[0] for x in h]\n",
    "plot_history_sequence(m, h)\n",
    "plt.savefig(\"figures/traces_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eligibility Traces in the Maze Example, with SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QlambdaAgent():\n",
    "    def __init__(self, actions=Maze.ACTIONS, epsilon=0.1, \n",
    "                 alpha=0.1, gamma=0.9, lmbda=0.9, method=\"SARSA\"):\n",
    "        \"\"\"Creates a Q-agent\"\"\"\n",
    "        self.Q = {}    ## Initial dictionary of (s, a) pairs. At the beginning, it's emtpy.\n",
    "        self.E = {}\n",
    "        \n",
    "        self.epsilon = epsilon     # Epsilon for e-greey policy\n",
    "        self.alpha = alpha         # Learning rate\n",
    "        self.gamma = gamma         # Temporal discounting\n",
    "        self.lmbda = lmbda         # decay rate\n",
    "        self.actions = actions     # Set of possible actions (provide those of Maze.ACTIONS)\n",
    "        self.method = method\n",
    "        \n",
    "    def policy(self, state):\n",
    "        \"\"\"Selects an action with a epsilon-greedy policy\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(self.actions)\n",
    "        else:\n",
    "            q = [self.Q[(state, a)] if (state, a) in self.Q.keys() else 0.0 for a in self.actions]\n",
    "            maxQ = max(q)\n",
    "            count = q.count(maxQ)\n",
    "            if count > 1:\n",
    "                best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "                i = random.choice(best)\n",
    "            else:\n",
    "                i = q.index(maxQ)\n",
    "\n",
    "            action = self.actions[i]\n",
    "        return action\n",
    "\n",
    "        \n",
    "    def sarsa_lambda(self, state1, action1, reward1, state2, action2):\n",
    "        \"\"\"Updates the Q-values when given an (s,a) pair, the reward value and a new state\"\"\"\n",
    "        g = self.gamma\n",
    "        a = self.alpha\n",
    "        l = self.lmbda\n",
    "        \n",
    "        q1 = 0.0\n",
    "        q2 = 0.0\n",
    "        \n",
    "        \n",
    "        if (state1, action1) in self.Q.keys():\n",
    "            q1 = self.Q[(state1, action1)]\n",
    "        else:\n",
    "            self.Q[(state1, action1)] = q1\n",
    "        \n",
    "        if (state2, action2) in self.Q.keys():\n",
    "            q2 = self.Q[(state2, action2)]\n",
    "        \n",
    "        rpe = reward1 + g * q2 - q1\n",
    "        \n",
    "        # Update the eligibility trace table\n",
    "        last_trace = (state1, action1)\n",
    "        \n",
    "        if last_trace not in self.E.keys():\n",
    "            self.E[last_trace] = 0 \n",
    "        \n",
    "        for trace in self.E.keys():\n",
    "            self.E[trace] *= l\n",
    "            if trace == last_trace:\n",
    "                 self.E[trace] += 1\n",
    "\n",
    "        # Update the Q table\n",
    "        for trace in self.E.keys():\n",
    "            self.Q[trace] += a * rpe * self.E[trace]\n",
    "        \n",
    "    \n",
    "    def learn(self, state1, action1, reward1, state2, action2):\n",
    "        \"Generic method for learning actions\"\n",
    "        if self.method == \"SARSA\":\n",
    "            self.sarsa_lambda(state1, action1, reward1, state2, action2)\n",
    "        elif self.method == \"Q-learning\":\n",
    "            self.q_learning(state1, action1, reward1, state2)\n",
    "\n",
    "            \n",
    "def rl_loop(environment, agent):\n",
    "    \"\"\"A trial ends when the agent gets a reward. The history is returned\"\"\"\n",
    "    state1  = environment.state\n",
    "    reward1 = environment.grid[state1[0], state1[1]]\n",
    "    \n",
    "    action1 = agent.policy(state1)\n",
    "    \n",
    "    history = []\n",
    "    state2  = \"\"\n",
    "    \n",
    "    while state2 != None:\n",
    "        # Perceive the next step \n",
    "        state2, reward2 = environment.transition(action1)\n",
    "        \n",
    "        # Save the states visited\n",
    "        history.append(state1)\n",
    "        \n",
    "        # Decide the next action and complete the loop\n",
    "        action2 = agent.policy(state2)\n",
    "        \n",
    "        # Update the Q-values for state1, action1\n",
    "        agent.learn(state1, action1, reward1, state2, action2)\n",
    "        \n",
    "        state1 = state2\n",
    "        reward1 = reward2\n",
    "        action1 = action2\n",
    "        \n",
    "    return history\n",
    "\n",
    "\n",
    "def run_trials(environment, agent, n, collect=True):\n",
    "    \"\"\"Runs N trials\"\"\"\n",
    "    history = []\n",
    "    for j in range(n):\n",
    "        h = rl_loop(environment, agent)\n",
    "        history += h\n",
    "        environment.state = Maze.INITIAL_STATE\n",
    "    \n",
    "    return history    \n",
    "\n",
    "\n",
    "def plot_q_table(agent, **kwargs):\n",
    "    \"\"\"Visualizes the Q tables, one per action\"\"\"\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(5,5))\n",
    "    i = 0\n",
    "    for a in agent.actions:\n",
    "        # Create the corresponding state table\n",
    "        data = np.zeros((4,4))\n",
    "        states = [x for x in agent.Q.keys() if x[1] == a]\n",
    "            \n",
    "        for s in states:\n",
    "            x, y = s[0]\n",
    "            data[x, y] = agent.Q[s]\n",
    "        \n",
    "        # Plot the heatmap\n",
    "        im = axs.flat[i].imshow(data, **kwargs, cmap=\"viridis\")\n",
    "\n",
    "        # Create colorbar\n",
    "        #cbar = ax.figure.colorbar(im, ax=ax)\n",
    "        #cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "        # We want to show all ticks...\n",
    "        axs.flat[i].set_xticks(np.arange(data.shape[1]))\n",
    "        axs.flat[i].set_yticks(np.arange(data.shape[0]))\n",
    "        axs.flat[i].set_xticklabels(range(1,5))\n",
    "        axs.flat[i].set_yticklabels(range(1,5))\n",
    "\n",
    "        # Let the horizontal axes labeling appear on top.\n",
    "        axs.flat[i].tick_params(top=False, bottom=True,\n",
    "                                labeltop=False, labelbottom=True)\n",
    "\n",
    "        # Turn spines off and create white grid.\n",
    "        for edge, spine in axs.flat[i].spines.items():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "        axs.flat[i].set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "        axs.flat[i].set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "        axs.flat[i].grid(which=\"minor\", color=\"grey\", linestyle='-', linewidth=2)\n",
    "        axs.flat[i].tick_params(which=\"minor\", bottom=False, left=False)\n",
    "        axs.flat[i].set_title(r\"$Q$-values for '%s'\" % (a,))\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    for ax in axs.flat:\n",
    "        ax.label_outer()\n",
    "\n",
    "    fig.subplots_adjust(right=0.85, hspace=0.2)\n",
    "    cbar_ax = fig.add_axes([0.88, 0.2, 0.03, 0.6])\n",
    "    fig.colorbar(im, cax=cbar_ax)\n",
    "    #fig.tight_layout()\n",
    "            \n",
    "\n",
    "def plot_history(history, ax=None, title=\"History\", \n",
    "                 cbarlabel=\"Probability Visited\", **kwargs):\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    data = np.zeros((4,4))            \n",
    "    for s in history:\n",
    "        x, y = s\n",
    "        data[x, y] += 1\n",
    "    \n",
    "    data /= np.max(data)\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs, cmap=\"Reds\")\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    \n",
    "    ax.set_xticklabels(range(1,5))\n",
    "    ax.set_yticklabels(range(1,5))\n",
    "\n",
    "    ax.tick_params(top=False, bottom=True,\n",
    "                   labeltop=False, labelbottom=True)\n",
    "    \n",
    "    # Turn spines off and create white grid.\n",
    "    for edge, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"grey\", linestyle='-', linewidth=2)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    ax.set_title(title)\n",
    "    return im, cbar\n",
    "\n",
    "\n",
    "def plot_history_sequence(maze, history, shape=None, **kwargs):\n",
    "    \"Plots a sequence of moves in the grid as a set 'movie' of maze positions\"\n",
    "    N = len(history)\n",
    "    ncols = 0\n",
    "    nrows = 0\n",
    "    if shape is None:\n",
    "        side = math.ceil(math.sqrt(N))\n",
    "        if side * (side - 1) > N:\n",
    "            ncols = side \n",
    "            nrows = side - 1\n",
    "        else:\n",
    "            ncols = side\n",
    "            nrows = side\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize = (ncols * 2, nrows * 2))\n",
    "    for ax in axs.flatten():\n",
    "        ax.label_outer()\n",
    "    for y in range(ncols):\n",
    "        for x in range(nrows):\n",
    "            ii = x * ncols + y\n",
    "            if ii < N:\n",
    "                plot_state(maze, ax=axs[x, y], position=history[ii], title=r\"$t=%d$\" % (ii + 1,))\n",
    "            else:\n",
    "                axs[x, y].axis('off')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $Q$-table when learning with eligibility traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Maze()\n",
    "a = QlambdaAgent(epsilon = 0.1)\n",
    "run_trials(m, a, 100)\n",
    "plot_q_table(a, vmin=0, vmax=10)\n",
    "plt.savefig(\"figures/qlambdatable.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
